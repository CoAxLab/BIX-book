{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qg1lirOVGQQ3"
   },
   "source": [
    "# Lab 10 - Curiosity-driven exploration\n",
    "\n",
    "This lab will exlore how the win-stay, lose-switch (WSLS) approach to the exploration-exploitation dilemma boosts curious exploration in our little bacteria friends.\n",
    "\n",
    "Sections:\n",
    "1. Reinforcement as a search strategy.\n",
    "2. Curiosity as a search strategy.\n",
    "3. Comparing all the methods we have learned so far.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQ38p_MAIzYY"
   },
   "source": [
    "\n",
    "## Background\n",
    "In this final lab we take on all the agents that we have studied so far and some new ones too.\n",
    "\n",
    "\n",
    "_The decisions to be made this week are the exact opposite of every other lab_.\n",
    "\n",
    "I am giving you six tuned agents, and three \"levers\" which control the environment. The now familiar scent grid. Your job this week is to see how curiosity and reward learning bias our search.\n",
    "\n",
    "Our target metrics:\n",
    "- It must gather the most total reward, by a clear margin (error baar overlap)\n",
    "- It must not die the most. That is, as long as one other agent dies more often, or all agents die 0 times, we'll call that good enough. (Any experimental trial which does not lead to finding at least a single target (aka reward) means the exploring agent dies. It's a harsh noisy world we live in, after all.)\n",
    "\n",
    "Once again, on final time it's time for _taxic explorations_. We revisit the sniff world (aka _ScentGrid_) with a familiar twist. We look again at what happens when sense information is not just noisy, but suddenly missing altogether. A concrete, cheap to simulate, case of this is turbulent flows.\n",
    "\n",
    "\n",
    "### Our agents, this time\n",
    "We will study six agents. They are,\n",
    "\n",
    "- A diffusion walker (aka rando-taxis) (aka _DiffusionGrid_)\n",
    "- Sniff! (aka chemo-taxis) (aka _GradientDiffusionGrid_)\n",
    "- Air cognition! (aka \"smart\" chemo-taxis) (aka _AccumulatorGradientGrid_)\n",
    "- Info cognition! (aka \"smart\" info-taxis) (aka _AccumulatorInfoGrid_)\n",
    "- RL w/ random softmax search (aka _ActorCriticGrid_)\n",
    "- Curiosity and RL union (aka rewardo- and info-taxis) (aka _WSLSGrid_)\n",
    "\n",
    "The goal is, as I said, to _the change the world_ -- until each agent \"wins\" (defined above).\n",
    "\n",
    "### Our agents, in review\n",
    "\n",
    "**Random search** (rando-taxis): Actions are sampled from an exponential distribution. For the _randotaxis_ agent number of steps means the number of steps or actions the agent takes.\n",
    "\n",
    "**Sniff!** (chemo-taxis): Recall our basic model of E. Coli exploration is as simple as can be.\n",
    "\n",
    "- When the gradient is positive, meaning you are going \"up\" the gradient, the probability of turning is set to _p pos_.\n",
    "- When the gradient is negative, the turning probability is set to _p neg_. (See code below, for an example).\n",
    "- If the agent \"decides\" to turn, the direction it takes is uniform random.\n",
    "- The length of travel before the next turn decision is sampled from an exponential distribution just like the _DiffusionGrid_\n",
    "\n",
    "**Costly cognition** (\"smart\", chemo- and info-taxis): Both _chemo-_ and _infotaxis_ agents will use a DDM-style accumulator to try and make better decisions about the direction of the gradient. These decisions are of course statistical in nature. (We won't be tuning the accumulator parameters in this lab. Assume the parameters I give you, for the DDM, are \"good enough\".)\n",
    "\n",
    "As beforee will assume that the steps are in a sense conserved. For the other two (accumulator) agents a step can mean two things. For accumulator agents a step can be spent sampling/weighing noisy scent evidence in the same location, or it can be spent moving to a new location. _Note_: Even though the info-accumulator is more complex, it can take advantage of missing scent information to drive its behavior. It can also use positive scent hits, of course, too.\n",
    "\n",
    "**RL** (rewardo-taxis): A Q-learning agent with softmax exploration. The RL agent has no shaping function, or intrinsic reward. It does not use the scent, in other words. It learns to value each position on the grid and make its immediate choices based on the value of the four possible actions that it can make (up, down, left, right).\n",
    "\n",
    "**WSLS** (reward- and info-taxis): A agent that alternates between info-taxis and Q-learning. Both are deterministic. Exploration and exploitation without any random search, in other words.\n",
    "\n",
    "_Details_: For this model a memory $M$ is a discrete probability distribution. I define information value $E$ on the norm of the derivative ($\\nabla M), approximated by $\\hat E = || f(x, M) - M ||$, where $||.||$ denotes the norm. (Norms are distances like hypotanooses.)\n",
    "\n",
    "The goal of any info-taxis (aka, curiosity agent) is to maximize $E$, I claim, based on a Bellman-optimal policy $\\pi^*_E$.\n",
    "\n",
    "So armed with $\\hat E$ I write down another (meta) policy $\\pi^{\\pi}$, in terms of a mixed series of values, $\\hat E$ and environmental rewards $R$. This WSLS rule is shown below. The reward (exploit) policy $\\pi_R$ is Q learning, same as for **RL**.\n",
    "\n",
    "$$\n",
    "    \\begin{split}\n",
    "        \\Pi_{\\pi} =\n",
    "        \\begin{cases}\n",
    "            \\pi^*_{\\hat{E}} & : \\hat{E} - \\eta > R + \\rho \\\\\n",
    "            \\pi_R \t& : \\hat{E} - \\eta < R + \\rho \\\\\n",
    "        \\end{cases}\n",
    "    \\end{split}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvqYZeENI6iZ"
   },
   "source": [
    "\n",
    "## Install and import needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44i85teQGQQ4"
   },
   "outputs": [],
   "source": [
    "# Install explorationlib?\n",
    "!pip install --upgrade git+https://github.com/parenthetical-e/explorationlib\n",
    "!pip install --upgrade git+https://github.com/MattChanTK/gym-maze.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BJSDTc5GQQ5"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "import explorationlib\n",
    "from explorationlib.local_gym import ScentGrid\n",
    "\n",
    "from explorationlib.agent import WSLSGrid\n",
    "from explorationlib.agent import CriticGrid\n",
    "from explorationlib.agent import SoftmaxActor\n",
    "from explorationlib.agent import DiffusionGrid\n",
    "from explorationlib.agent import GradientDiffusionGrid\n",
    "from explorationlib.agent import AccumulatorGradientGrid\n",
    "from explorationlib.agent import AccumulatorInfoGrid\n",
    "from explorationlib.agent import ActorCriticGrid\n",
    "\n",
    "from explorationlib.run import experiment\n",
    "from explorationlib.util import select_exp\n",
    "from explorationlib.util import load\n",
    "from explorationlib.util import save\n",
    "\n",
    "from explorationlib.local_gym import uniform_targets\n",
    "from explorationlib.local_gym import constant_values\n",
    "from explorationlib.local_gym import ScentGrid\n",
    "from explorationlib.local_gym import create_grid_scent\n",
    "from explorationlib.local_gym import add_noise\n",
    "from explorationlib.local_gym import create_grid_scent_patches\n",
    "\n",
    "from explorationlib.plot import plot_position2d\n",
    "from explorationlib.plot import plot_length_hist\n",
    "from explorationlib.plot import plot_length\n",
    "from explorationlib.plot import plot_targets2d\n",
    "from explorationlib.plot import plot_scent_grid\n",
    "from explorationlib.plot import plot_targets2d\n",
    "\n",
    "from explorationlib.score import total_reward\n",
    "from explorationlib.score import num_death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y42Ev90dGQQ5"
   },
   "outputs": [],
   "source": [
    "# Pretty plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "%config IPCompleter.greedy=True\n",
    "plt.rcParams[\"axes.facecolor\"] = \"white\"\n",
    "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "plt.rcParams[\"font.size\"] = \"16\"\n",
    "\n",
    "# Dev\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCAhqRkmGQQ5"
   },
   "source": [
    "## Section 1 -  RL as a search strategy\n",
    "\n",
    "### RL\n",
    "\n",
    "To build some intuition, let's plot the the behavoir of our RL agent as it learns where the rewards are in a (fixed) ScentGrid env. The noise level is 2 standard deviaions, all but 10 percent of it deleted.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1m_VnqsJGso"
   },
   "source": [
    "\n",
    "#### Question 1.1\n",
    "Does the fact that the noise level of the scents is 2 standard deviaions, and all but 10 percent of it deleted matter for the RL agent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsY6NINtGQQ6"
   },
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3U-E_oNGQQ6"
   },
   "source": [
    "### Shared params and env\n",
    "\n",
    "Okay, let's get started by setting up our environment. his is the standard scent grid we have used in prior labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KI_znq-aGQQ6"
   },
   "outputs": [],
   "source": [
    "# Noise and delete\n",
    "p_scent = 0.1\n",
    "noise_sigma = 2.0\n",
    "\n",
    "# Shared\n",
    "num_experiments = 100\n",
    "num_steps = 200\n",
    "seed_value = 5838\n",
    "num_targets = 20 # with 80 agents are more competitive!\n",
    "\n",
    "# ! (leave alone)\n",
    "detection_radius = 1\n",
    "cog_mult = 1\n",
    "max_steps = 1\n",
    "min_length = 1\n",
    "target_boundary = (10, 10)\n",
    "\n",
    "# Targets\n",
    "prng = np.random.RandomState(seed_value)\n",
    "targets = uniform_targets(num_targets, target_boundary, prng=prng)\n",
    "values = constant_values(targets, 1)\n",
    "\n",
    "# Scents\n",
    "scents = []\n",
    "for _ in range(len(targets)):\n",
    "    coord, scent = create_grid_scent_patches(\n",
    "        target_boundary, p=1.0, amplitude=1, sigma=2)\n",
    "    scents.append(scent)\n",
    "\n",
    "# Env\n",
    "env = ScentGrid(mode=None)\n",
    "env.seed(seed_value)\n",
    "env.add_scents(targets, values, coord, scents, noise_sigma=noise_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwyty4hYGQQ7"
   },
   "source": [
    "### Getting to know you, RL\n",
    "\n",
    "For this demo we will set up two agents:\n",
    "\n",
    "- Rando: random walker\n",
    "- RL: An agent that uses reinforcement learning to track a target. It updates the value of each point on the grid based on if it hit a target there before. This updates the immediate $Q$-value for each decision (up, down, left, right)\n",
    "\n",
    "We are going to give each of our agents 99 tries at the _same_ environment. We want to see how repeated exposure to the same environment will improve performance in our agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Ye23k1hGQQ7"
   },
   "outputs": [],
   "source": [
    "# RL\n",
    "possible_actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
    "critic = CriticGrid(default_value=0.5)\n",
    "actor = SoftmaxActor(num_actions=4, actions=possible_actions, beta=4)\n",
    "rl = ActorCriticGrid(actor, critic, lr=0.1, gamma=0.1)\n",
    "\n",
    "# Rando\n",
    "diff = DiffusionGrid(min_length=min_length, scale=1)\n",
    "diff.seed(seed_value)\n",
    "\n",
    "# !\n",
    "rl_exp = experiment(\n",
    "    f\"RL\",\n",
    "    rl,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")\n",
    "rand_exp = experiment(\n",
    "    f\"rand\",\n",
    "    diff,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QjCw5nrGQQ7"
   },
   "source": [
    "### Rando search\n",
    "Just one example of the movement of our random agent, for comparison with the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uJS5HwwPGQQ7"
   },
   "outputs": [],
   "source": [
    "plot_boundary = (20, 20)\n",
    "\n",
    "# -\n",
    "num_experiment = 99\n",
    "ax = None\n",
    "ax = plot_position2d(\n",
    "    select_exp(rand_exp, num_experiment),\n",
    "    boundary=plot_boundary,\n",
    "    label=f\"Rando\",\n",
    "    color=\"grey\",\n",
    "    alpha=0.6,\n",
    "    ax=ax,\n",
    ")\n",
    "ax = plot_targets2d(\n",
    "    env,\n",
    "    boundary=plot_boundary,\n",
    "    color=\"black\",\n",
    "    alpha=1,\n",
    "    label=\"Targets\",\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DC_Usa7qGQQ7"
   },
   "source": [
    "### Search behavior and learning\n",
    "\n",
    "\n",
    "We will now look at the performance of our RL agent at three time points (N=0, early; N=50, middle; N=99, late)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YR4exoU2GQQ7"
   },
   "outputs": [],
   "source": [
    "plot_boundary = (20, 20)\n",
    "\n",
    "# -\n",
    "num_experiment = 0\n",
    "ax = None\n",
    "ax = plot_position2d(\n",
    "    select_exp(rl_exp, num_experiment),\n",
    "    boundary=plot_boundary,\n",
    "    label=f\"N={num_experiment}\",\n",
    "    color=\"orange\",\n",
    "    alpha=0.3,\n",
    "    ax=ax,\n",
    ")\n",
    "num_experiment = 50\n",
    "ax = plot_position2d(\n",
    "    select_exp(rl_exp, num_experiment),\n",
    "    boundary=plot_boundary,\n",
    "    label=f\"N={num_experiment}\",\n",
    "    color=\"orange\",\n",
    "    alpha=0.5,\n",
    "    ax=ax,\n",
    ")\n",
    "num_experiment = 99\n",
    "ax = plot_position2d(\n",
    "    select_exp(rl_exp, num_experiment),\n",
    "    boundary=plot_boundary,\n",
    "    label=f\"N={num_experiment}\",\n",
    "    color=\"orange\",\n",
    "    alpha=1,\n",
    "    ax=ax,\n",
    ")\n",
    "ax = plot_targets2d(\n",
    "    env,\n",
    "    boundary=plot_boundary,\n",
    "    color=\"black\",\n",
    "    alpha=1,\n",
    "    label=\"Targets\",\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNDAAWC8GQQ7"
   },
   "source": [
    "### Reward value, in time\n",
    "\n",
    "Now let's look at the value ($Q$-value in this case) of the optimal value (i.e., $max(Q(a))$) across time for each of these three stages of learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9t91QHmZGQQ7"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 3))\n",
    "plt.plot(rl_exp[0][\"agent_reward_value\"], label=\"N=0\", color=\"orange\", alpha=0.2)\n",
    "plt.plot(rl_exp[50][\"agent_reward_value\"], label=\"N=50\", color=\"orange\", alpha=0.5)\n",
    "plt.plot(rl_exp[99][\"agent_reward_value\"], label=\"N=99\", color=\"orange\", alpha=1)\n",
    "plt.ylabel(\"Value $V(x)$\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "974-6GprLCG8"
   },
   "source": [
    "\n",
    "#### Question 1.2\n",
    "\n",
    "What do you see in this behavior of the RL agent over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pEH4Su3LG6-"
   },
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWXZ-OV6LOd9"
   },
   "source": [
    "### Looking across simulations\n",
    "\n",
    "Now let's plot our metrics and see how the two agents did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HbbyAxdJGQQ8"
   },
   "source": [
    "### Death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fgSHAbqWGQQ8"
   },
   "outputs": [],
   "source": [
    "# Results\n",
    "results = [rand_exp, rl_exp]\n",
    "names = [\"Rando\", \"RL\"]\n",
    "colors = [\"grey\", \"orange\"]\n",
    "\n",
    "# Score by eff\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    scores.append(num_death(res))\n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(4, 3))\n",
    "plt.bar(names, m, yerr=sd, color=\"black\", alpha=0.6)\n",
    "plt.ylabel(\"Deaths\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROgWycGOGQQ8"
   },
   "source": [
    "### Total reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3_1JQ2LGQQ8"
   },
   "outputs": [],
   "source": [
    "# Results\n",
    "results = [rand_exp, rl_exp]\n",
    "names = [\"Rando\", \"RL\"]\n",
    "colors = [\"grey\", \"orange\"]\n",
    "\n",
    "# Score by eff\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    r = total_reward(res)\n",
    "    scores.append(r)\n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.6)\n",
    "plt.ylabel(\"Total reward\")\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "\n",
    "# Dists\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    plt.hist(s, label=name, color=c, alpha=0.5, bins=np.linspace(0, np.max(scores), 50))\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cp1nid3OLiRs"
   },
   "source": [
    "\n",
    "#### Question 1.3\n",
    "\n",
    "How does the performance of the RL agent compare, across all performance metrics, to the random agent? Is this a fair comparison? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvRyWRHbLxzE"
   },
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJFqXIatGQQ8"
   },
   "source": [
    "## Section 2 -  WSLS as a search strategy\n",
    "\n",
    "### WSLS\n",
    "\n",
    "Now let's include our curious agent. Remember, this agent tracks the value of information as well as the value of rewards. It is basically two agents acting as one: the RL agent and an info-taxis agent.\n",
    "\n",
    "The WSLS agnet makes a choice to go after whatever action has the highest value ($R$ or $E$), swtiching back and forth between seeking rewards (i.e., targets) and information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2Dh4ekiGQQ8"
   },
   "outputs": [],
   "source": [
    "# WSLS\n",
    "possible_actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
    "num_action = len(possible_actions)\n",
    "initial_bins = np.linspace(0, 1, 10)\n",
    "\n",
    "critic_R = CriticGrid(default_value=0.0)\n",
    "critic_E = CriticGrid(default_value=np.log(num_action))\n",
    "actor_R = SoftmaxActor(num_actions=4, actions=possible_actions, beta=20)\n",
    "actor_E = SoftmaxActor(num_actions=4, actions=possible_actions, beta=20)\n",
    "\n",
    "wsls = WSLSGrid(\n",
    "    actor_E,\n",
    "    critic_E,\n",
    "    actor_R,\n",
    "    critic_R,\n",
    "    initial_bins,\n",
    "    lr=0.1,\n",
    "    gamma=0.1,\n",
    "    boredom=0.0\n",
    ")\n",
    "\n",
    "# Rando\n",
    "diff = DiffusionGrid(min_length=min_length, scale=1)\n",
    "diff.seed(seed_value)\n",
    "\n",
    "# !\n",
    "wsls_exp = experiment(\n",
    "    f\"wsls\",\n",
    "    wsls,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")\n",
    "rand_exp = experiment(\n",
    "    f\"rand\",\n",
    "    diff,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQ_5HTnFMWDt"
   },
   "source": [
    "Just like before, let's look at our random agent's behavior for later comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccEWQeQYGQQ9"
   },
   "outputs": [],
   "source": [
    "plot_boundary = (20, 20)\n",
    "\n",
    "# -\n",
    "num_experiment = 99\n",
    "ax = None\n",
    "ax = plot_position2d(\n",
    "    select_exp(rand_exp, num_experiment),\n",
    "    boundary=plot_boundary,\n",
    "    label=f\"Rando\",\n",
    "    color=\"grey\",\n",
    "    alpha=0.6,\n",
    "    ax=ax,\n",
    ")\n",
    "ax = plot_targets2d(\n",
    "    env,\n",
    "    boundary=plot_boundary,\n",
    "    color=\"black\",\n",
    "    alpha=1,\n",
    "    label=\"Targets\",\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBsgDGpCMa_T"
   },
   "source": [
    "### Reward AND Information value across learning\n",
    "\n",
    "Just like before, let's see how the behavior of our WSLS agent changes across repeated exposures to the same environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vDY7wPGHGQQ9"
   },
   "outputs": [],
   "source": [
    "plot_boundary = (20, 20)\n",
    "\n",
    "# -\n",
    "num_experiment = 0\n",
    "ax = None\n",
    "ax = plot_position2d(\n",
    "    select_exp(wsls_exp, num_experiment),\n",
    "    boundary=plot_boundary,\n",
    "    label=f\"N={num_experiment}\",\n",
    "    color=\"orangered\",\n",
    "    alpha=0.3,\n",
    "    ax=ax,\n",
    ")\n",
    "num_experiment = 50\n",
    "ax = plot_position2d(\n",
    "    select_exp(wsls_exp, num_experiment),\n",
    "    boundary=plot_boundary,\n",
    "    label=f\"N={num_experiment}\",\n",
    "    color=\"orangered\",\n",
    "    alpha=0.5,\n",
    "    ax=ax,\n",
    ")\n",
    "num_experiment = 99\n",
    "ax = plot_position2d(\n",
    "    select_exp(wsls_exp, num_experiment),\n",
    "    boundary=plot_boundary,\n",
    "    label=f\"N={num_experiment}\",\n",
    "    color=\"orangered\",\n",
    "    alpha=1,\n",
    "    ax=ax,\n",
    ")\n",
    "ax = plot_targets2d(\n",
    "    env,\n",
    "    boundary=plot_boundary,\n",
    "    color=\"black\",\n",
    "    alpha=1,\n",
    "    label=\"Targets\",\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5Yc88cuM23j"
   },
   "source": [
    "\n",
    "#### Question 2.1\n",
    "\n",
    "Based off of the behavior of our WSLS agent, how well do you think it will perform after repeated runs (compared to the Rando agent)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hr1FVKEANFeb"
   },
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lOlT94xGQQ9"
   },
   "source": [
    "### Value over time\n",
    "\n",
    "Again let's look at the optional Value driving the action ($R$ or $E$) of our agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OF6AL5KmGQQ9"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 3))\n",
    "plt.plot(wsls_exp[0][\"agent_reward_value\"], label=\"N=0\", color=\"orangered\", alpha=0.2)\n",
    "plt.plot(wsls_exp[50][\"agent_reward_value\"], label=\"N=50\", color=\"orangered\", alpha=0.5)\n",
    "plt.plot(wsls_exp[99][\"agent_reward_value\"], label=\"N=99\", color=\"orangered\", alpha=1)\n",
    "plt.ylabel(\"Value $V(x)$\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuMgHX1tGQQ9"
   },
   "source": [
    "### Death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqQtx5zWGQQ9"
   },
   "outputs": [],
   "source": [
    "# Results\n",
    "results = [rand_exp, wsls_exp]\n",
    "names = [\"Rando\", \"WSLS\"]\n",
    "colors = [\"grey\", \"orangered\"]\n",
    "\n",
    "# Score by eff\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    scores.append(num_death(res))\n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.9)\n",
    "plt.ylabel(\"Deaths\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlJVYZ60GQQ9"
   },
   "source": [
    "### Total reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3myeU3FEGQQ9"
   },
   "outputs": [],
   "source": [
    "# Results\n",
    "results = [rand_exp, wsls_exp]\n",
    "names = [\"Rando\", \"WSLS\"]\n",
    "colors = [\"grey\", \"orangered\"]\n",
    "\n",
    "# Score by eff\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    r = total_reward(res)\n",
    "    scores.append(r)\n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.6)\n",
    "plt.ylabel(\"Total reward\")\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "\n",
    "# Dists\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    plt.hist(s, label=name, color=c, alpha=0.5, bins=np.linspace(0, np.max(scores), 50))\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6wEJMLXGQQ-"
   },
   "source": [
    "#### Question 2.2\n",
    "\n",
    "Based on the performance you have seen above, is it better to be curious and greedy, or greedy and noisy?\n",
    "Compare the RL agent to the WSLS agent (and Rando for control)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i7BUDes2NxNZ"
   },
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfabeQRSN3XB"
   },
   "source": [
    "## Now let's compare the agents directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82SLR8MfGQQ-"
   },
   "source": [
    "### Death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLJ5ZknzGQQ-"
   },
   "outputs": [],
   "source": [
    "# Results\n",
    "results = [rand_exp, rl_exp, wsls_exp]\n",
    "names = [\"Rando\", \"RL\", \"WSLS\"]\n",
    "colors = [\"grey\", \"orange\", \"orangered\"]\n",
    "\n",
    "# Score by eff\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    scores.append(num_death(res))\n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(4, 3))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.9)\n",
    "plt.ylabel(\"Deaths\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEG_YkoeGQQ-"
   },
   "source": [
    "### Total reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBZ6qbwTGQQ-"
   },
   "outputs": [],
   "source": [
    "# Results\n",
    "results = [rand_exp, rl_exp, wsls_exp]\n",
    "names = [\"Rando\", \"RL\", \"WSLS\"]\n",
    "colors = [\"grey\", \"orange\", \"orangered\"]\n",
    "\n",
    "# Score by eff\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    r = total_reward(res)\n",
    "    scores.append(r)\n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.6)\n",
    "plt.ylabel(\"Total reward\")\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "\n",
    "# Dists\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    plt.hist(s, label=name, color=c, alpha=0.5, bins=np.linspace(0, np.max(scores), 50))\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsBB491ZGQQ-"
   },
   "source": [
    "#### Question 2.3\n",
    "The WSLS approach should have generated more total reward. It may also have had a few (< 10) deaths. (If it did not, try running the WSLS cells again).\n",
    "\n",
    "Likewise, if you study WSLS search behavior and value learning time courses, you'll see it \"settles down\" to one rewarding spot and can stay there.\n",
    "\n",
    "In other words, WSLS is a method with very high inductive bias.\n",
    "\n",
    "Based on the results in this lab so far, and lecture on managing the exploration-exploitation dilemma, how could you change the env so that the exploration bias behind WSLS (deterministic learning maximization) fails, but the random search of RL does not.\n",
    "\n",
    "_Note:_ It is helpful to consider the total reward distribution plots carefully. The middle and the bottom range, especially. (Try rerunning?)\n",
    "\n",
    "_Note_: Everything is on the table. Your counter-example can be whatever you want, well as long as it is physically possible. Be imaginative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3J5UBE7pGQQ_"
   },
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AyZhBXR4GQQ_"
   },
   "source": [
    "## Section 3 - The battle royal of our bacteria agents\n",
    "\n",
    "Now let's run all of our agents on the same world from Sections 1 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3H9x76GGQQ_"
   },
   "source": [
    "### Keep our environment the same\n",
    "\n",
    "Just resetting here in case we tweaked parameters above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-d6zVHq3GQQ_"
   },
   "outputs": [],
   "source": [
    "# Noise and delete\n",
    "p_scent = 0.1\n",
    "noise_sigma = 2.0\n",
    "\n",
    "# Shared\n",
    "num_experiments = 100\n",
    "num_steps = 200\n",
    "seed_value = 5838\n",
    "num_targets = 20 # with 80 agents are more competitive!\n",
    "\n",
    "# ! (leave alone)\n",
    "detection_radius = 1\n",
    "cog_mult = 1\n",
    "max_steps = 1\n",
    "min_length = 1\n",
    "target_boundary = (10, 10)\n",
    "\n",
    "# Targets\n",
    "prng = np.random.RandomState(seed_value)\n",
    "targets = uniform_targets(num_targets, target_boundary, prng=prng)\n",
    "values = constant_values(targets, 1)\n",
    "\n",
    "# Scents\n",
    "scents = []\n",
    "for _ in range(len(targets)):\n",
    "    coord, scent = create_grid_scent_patches(\n",
    "        target_boundary, p=1.0, amplitude=1, sigma=2)\n",
    "    scents.append(scent)\n",
    "\n",
    "# Env\n",
    "env = ScentGrid(mode=None)\n",
    "env.seed(seed_value)\n",
    "env.add_scents(targets, values, coord, scents, noise_sigma=noise_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4kNnvvtGQQ_"
   },
   "source": [
    "### Run 'em all!\n",
    "\n",
    "Now we can run all 6 of our agent types in the environment, the same way, and see how well they did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ml8BeQdtGQQ_"
   },
   "outputs": [],
   "source": [
    "# Agents\n",
    "\n",
    "# rando\n",
    "diff = DiffusionGrid(min_length=min_length, scale=1)\n",
    "diff.seed(seed_value)\n",
    "\n",
    "# sniff\n",
    "sniff = GradientDiffusionGrid(\n",
    "    min_length=min_length,\n",
    "    scale=1.0,\n",
    "    p_neg=1,\n",
    "    p_pos=0.0\n",
    ")\n",
    "sniff.seed(seed_value)\n",
    "\n",
    "# smart chemo\n",
    "chemo = AccumulatorGradientGrid(\n",
    "    min_length=min_length,\n",
    "    max_steps=max_steps,\n",
    "    drift_rate=1,\n",
    "    threshold=3,\n",
    "    accumulate_sigma=1\n",
    ")\n",
    "chemo.seed(seed_value)\n",
    "\n",
    "# smart info\n",
    "info = AccumulatorInfoGrid(\n",
    "    min_length=min_length,\n",
    "    max_steps=max_steps,\n",
    "    drift_rate=1,\n",
    "    threshold=3,\n",
    "    accumulate_sigma=1\n",
    ")\n",
    "info.seed(seed_value)\n",
    "\n",
    "# RL\n",
    "critic = CriticGrid(default_value=0.5)\n",
    "actor = SoftmaxActor(num_actions=4, actions=possible_actions, beta=4)\n",
    "rl = ActorCriticGrid(actor, critic, lr=0.1, gamma=0.1)\n",
    "\n",
    "# WSLS\n",
    "possible_actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
    "num_action = len(possible_actions)\n",
    "initial_bins = np.linspace(0, 1, 10)\n",
    "\n",
    "critic_R = CriticGrid(default_value=0.5)\n",
    "critic_E = CriticGrid(default_value=np.log(num_action))\n",
    "actor_R = SoftmaxActor(num_actions=4, actions=possible_actions, beta=20)\n",
    "actor_E = SoftmaxActor(num_actions=4, actions=possible_actions, beta=20)\n",
    "\n",
    "wsls = WSLSGrid(\n",
    "    actor_E,\n",
    "    critic_E,\n",
    "    actor_R,\n",
    "    critic_R,\n",
    "    initial_bins,\n",
    "    lr=0.1,\n",
    "    gamma=0.1,\n",
    "    boredom=0.0\n",
    ")\n",
    "\n",
    "# !\n",
    "rand_exp = experiment(\n",
    "    f\"rand\",\n",
    "    diff,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")\n",
    "sniff_exp = experiment(\n",
    "    f\"sniff\",\n",
    "    sniff,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")\n",
    "chemo_exp = experiment(\n",
    "    f\"chemo\",\n",
    "    chemo,\n",
    "    env,\n",
    "    num_steps=num_steps * cog_mult,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")\n",
    "info_exp = experiment(\n",
    "    f\"info\",\n",
    "    info,\n",
    "    env,\n",
    "    num_steps=num_steps * cog_mult,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")\n",
    "rl_exp = experiment(\n",
    "    f\"rl\",\n",
    "    rl,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")\n",
    "wsls_exp = experiment(\n",
    "    f\"wsls\",\n",
    "    wsls,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgP1FVjkGQQ_"
   },
   "source": [
    "### Search behavior\n",
    "\n",
    "Let's just take a look at the movements of each agent at the last run of the repeated tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBvglSq7GQQ_"
   },
   "outputs": [],
   "source": [
    "plot_boundary = (20, 20)\n",
    "num_experiment = 99\n",
    "\n",
    "# Results\n",
    "results = [sniff_exp, chemo_exp, info_exp, rand_exp, rl_exp, wsls_exp]\n",
    "names = [\"Sniff\", \"Chemo\", \"Info\", \"Rando\", \"RL\", \"WSLS\"]\n",
    "colors = [\"purple\", \"blue\", \"green\", \"grey\", \"orange\", \"orangered\"]\n",
    "\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    ax = None\n",
    "    ax = plot_position2d(\n",
    "        select_exp(res, num_experiment),\n",
    "        boundary=plot_boundary,\n",
    "        label=f\"{name}\",\n",
    "        color=color,\n",
    "        alpha=0.6,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax = plot_targets2d(\n",
    "        env,\n",
    "        boundary=plot_boundary,\n",
    "        color=\"black\",\n",
    "        alpha=1,\n",
    "        label=\"Targets\",\n",
    "        ax=ax,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ku9eEDklO5kB"
   },
   "source": [
    "#### Question 3.1\n",
    "\n",
    "What do you see in the different agents' behavior? Who moved more and who moved less?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDc9FQUAPAAj"
   },
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0iczHEyPLVR"
   },
   "source": [
    "## Now let's run one final evaluation to test them all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogGW-iODGQQ_"
   },
   "source": [
    "### Death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOoLzkOEGQRA"
   },
   "outputs": [],
   "source": [
    "# Results\n",
    "results = [rand_exp, sniff_exp, chemo_exp, info_exp,  rl_exp, wsls_exp]\n",
    "names = [\"Rando\", \"Sniff\", \"Chemo\", \"Info\",  \"RL\", \"WSLS\"]\n",
    "colors = [\"grey\", \"purple\", \"blue\", \"green\", \"orange\", \"orangered\"]\n",
    "\n",
    "# Score by eff\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    scores.append(num_death(res))\n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.6)\n",
    "plt.ylabel(\"Deaths\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hi7d077PGQRA"
   },
   "source": [
    "### Total reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AnQif0nzGQRA"
   },
   "outputs": [],
   "source": [
    "# Results\n",
    "results = [rand_exp, sniff_exp, chemo_exp, info_exp,  rl_exp, wsls_exp]\n",
    "names = [\"Rando\", \"Sniff\", \"Chemo\", \"Info\",\"RL\", \"WSLS\"]\n",
    "colors = [\"grey\",\"purple\", \"blue\", \"green\", \"orange\", \"orangered\"]\n",
    "\n",
    "# Score\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    r = total_reward(res)\n",
    "    scores.append(r)\n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.6)\n",
    "plt.ylabel(\"Total reward\")\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "\n",
    "# Dists\n",
    "# fig = plt.figure(figsize=(7, 5))\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    fig = plt.figure(figsize=(7, 3))\n",
    "    plt.hist(s, label=name, color=c, alpha=0.4, bins=np.linspace(0, np.max(scores), 50))\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Rkvi5NXGQRB"
   },
   "source": [
    "#### Question 3.2\n",
    "\n",
    "\n",
    "In the same environment, we scaled up th ecomplexity of the decisions of our little bacteria friends. What pattern did you see across agents? Who did the best, who did worse, and who was most variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Muucj-N4GQRB"
   },
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1vEk2pJGQRB"
   },
   "source": [
    "#### Question 3.3\n",
    "\n",
    "Remember from the prior labs that sometimes the Rando agent outperformed agents with more directed decisions in their search. What changes in the environment would provide an edge for the simpler (and more random) agents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rWnps9rUGQRB"
   },
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}