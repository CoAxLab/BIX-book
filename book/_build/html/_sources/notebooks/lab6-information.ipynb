{"cells":[{"cell_type":"markdown","metadata":{"id":"mhT-lilScl3G"},"source":["# Lab 6 - Information theory\n","\n","This lab has a few goals designed to get you comfortable with working with python and playing with the basiscs of information theory.\n","\n","Sections:\n","1. Entropy\n","2. Mutual information\n","3. KL divergence"]},{"cell_type":"markdown","metadata":{"id":"hKkGUWhIhFds"},"source":["## Background\n","\n","For this lab we won't be working with our main library, _explorationlib_. We'll use a few standard libraries to both get more comfortable with python and to refresh the concepts of information theory discussed in class."]},{"cell_type":"markdown","metadata":{"id":"fYdbAhg2tyIU"},"source":["### **What is entropy?**\n","\n","- According to Shannon's definition, the entropy $H(X)$ of a discrete random variable is: $H(X)= \\sum_{x \\in X} p(x) \\log _2 p(x)^{-1}$.\n","- Here we are assuming that the discrete variable is binary, hence the use of $\\log _2$\n","- A key aspect of the definition entropy is the 'surprise' termn, $p(x)^{-1}$. The more surprising an outcome is, the more valuable that bit of information is. We will return to this later in class."]},{"cell_type":"markdown","metadata":{"id":"Utwl97pgGtMj"},"source":["### **Entropy between variables**\n","\n","- The _joint entropy_ between two discrete random variables, $X$ and $Y$, is just an expansion of the regular concept of entropy: $H(X, Y)= \\sum_{x \\in X} p(x,y) \\log _2 p(x,y)^{-1}$.\n","- Note that in here, $p(x,y)$ is the joint probability distribution between the variables.\n","- Expanding out from the rules of conditional probabilities (thank you Mr. Bayes), the joint entropy can be expanded as the product of the entropy of the primary variable and the conditional entropy of the two variables: $H(X,Y) = H(X) + H(Y|X)$. _Pay attention to the ordering of terms, it gets important later_."]},{"cell_type":"markdown","metadata":{"id":"mMWAKQ_PGtMj"},"source":["### **Mutual information between variables**.\n","\n","- Sticking with the idea of joint entropy, the _conditional entropy_ $H(X|Y)$ reflects the residual entropy of $X$ after you have knowledge about $Y$. This is expressed as: $H(X|Y)=H(X)-I(X;Y)$.\n","- We call this second term, $I(X;Y)$ the _mutual information_ between $X$ and $Y$. It is the information provided by Y about X. If you were in statistics and working with continuous variables, this would be the correlation (or covariance).\n","- We can rewrite the equation above as $I(X;Y) = H(X) - H(X|Y) = \\sum_{x \\in X, y \\in Y} p(x,y) \\log _2 \\frac{p(x,y)}{p(x)p(y)}$"]},{"cell_type":"markdown","metadata":{"id":"nsaYc-X4GtMj"},"source":["### **Kullback-Leibler (KL) divergence**.\n","\n","- Often referred to as relative entropy, this is a measure of how one probability distribution diverges from a second, reference probability distribution. It quantifies the amount of information lost when approximating one distribution with another.\n","\n","- Consider $P(X)$ and $Q(X)$ to be two probability distribtions of a random variable. In our example, spike trains from two neurons.\n","\n","- The KL divergence of $P$ and $Q$, $D_{KL}(P||Q)$, is defined as\n","\n","$$D_{KL}(P||Q)=\\Sigma_{x \\in X} p(x)\\log_2 (\\frac{p(x)}{q(x)})$$\n","\n","- Essentially $D_{KL}(P||Q)$ is a measure of the distance between two distributions"]},{"cell_type":"markdown","metadata":{"id":"cKiFhdTxk_n3"},"source":["## Section 0 - Setup\n","\n","For today's lab we'll start simple. All we need is _numpy_ and _matplotlib_ (for plotting)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7TTuWHQEcF1O"},"outputs":[],"source":["# Import the general libraries we will be using\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"pu48IqFjlfeN"},"source":["## Section 1 - Entropy\n","\n","- For this section we will simulate two cells, a presynaptic and postsynaptic cell. The data will consist of spikes. We will vary the degree of dependency between the neurons and look at their relative entropies.\n","- One way to achieve this is to use a Poisson process to simulate the neurons. Here, the firing of each neuron at each time step is modeled as a binary event that occurs with some probability. The influence of the first neuron on the second is represented by increasing the firing probability of the second neuron when the first one fires."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PmMf7zZ2YVd5"},"outputs":[],"source":["def simulate_neuron_spikes(n_samples, base_firing_prob, influence, noise_level, neuron1=None):\n","    \"\"\"\n","    Simulate the spiking of a neuron.\n","\n","    If neuron1 is not None, then this neuron is influenced by neuron1.\n","    \"\"\"\n","    spikes = np.zeros(n_samples)\n","\n","    for i in range(n_samples):\n","        firing_prob = base_firing_prob\n","\n","        if neuron1 is not None and neuron1[i] == 1:\n","            firing_prob += influence\n","\n","        firing_prob += noise_level * np.random.randn()\n","\n","        if np.random.uniform(low=0.0, high=1.0) < firing_prob:\n","            spikes[i] = 1\n","\n","    return spikes\n","\n"]},{"cell_type":"code","source":["# Time and rates\n","total_time_sec = 5\n","sampling_rate_hz = 1000\n","n_samples = total_time_sec * sampling_rate_hz\n","\n","# Probabilities\n","base_firing_prob = 0.02\n","influence = 0.03\n","noise_level = 0.01\n","\n","# Simulate the two neurons\n","neuron1 = simulate_neuron_spikes(n_samples, base_firing_prob, 0, noise_level)\n","neuron2 = simulate_neuron_spikes(n_samples, base_firing_prob, influence, noise_level, neuron1)\n","\n","print(\"Neuron 1 spike train:\", neuron1)\n","print(\"Neuron 2 spike train:\", neuron2)"],"metadata":{"id":"mOL0rrxDdswD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oe57oi3oGtMl"},"source":["What we have created are two binary vectors, _neuron1_ and _neuron2_, that represent the spike trains of two neurons over 5 seconds at a 1 kHz sampling rate.\n","\n","Each time step is a binary event where 1 represents a spike and 0 represents no spike. The base firing probability is 2%, but this is modified by two factors: the influence of the first neuron on the second, and some independent noise.\n","\n","The _influence_ parameter controls how much the first neuron affects the second: whenever the first neuron fires, the firing probability of the second neuron is increased by the influence factor. The _noise level_ parameter controls the amount of independent noise in the firing probabilities. This noise is modeled as Gaussian noise and is independent for each neuron and each time step."]},{"cell_type":"markdown","metadata":{"id":"3TUxsKoQbtm_"},"source":["### Visualizing our two neurons\n","\n","We can use a raster plot to visualize the spike times of our neurons. For this we will use matplotlib.\n","\n","In a raster plot, each row corresponds to a different repetition of the experiment (a different neuron in this case), and the x-axis represents time. Each small vertical line (marker) represents a spike."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"377nSrZKYXlA"},"outputs":[],"source":["def plot_spikes(neuron1, neuron2, sampling_rate_hz):\n","    \"\"\"\n","    Plot the spikes of the two neurons as a raster plot.\n","    \"\"\"\n","    time_points = np.arange(len(neuron1)) / sampling_rate_hz\n","\n","    fig, ax = plt.subplots(2, 1, figsize=(10, 5), sharex=True, sharey=True)\n","\n","    # Neuron 1\n","    ax[0].eventplot(time_points[neuron1 == 1], color='black')\n","    ax[0].set_title(\"Neuron 1\")\n","    ax[0].set_ylabel(\"Spikes\")\n","\n","    # Neuron 2\n","    ax[1].eventplot(time_points[neuron2 == 1], color='black')\n","    ax[1].set_title(\"Neuron 2\")\n","    ax[1].set_ylabel(\"Spikes\")\n","    ax[1].set_xlabel(\"Time (sec)\")\n","\n","    plt.tight_layout()\n","    plt.show()\n"]},{"cell_type":"code","source":["# Run the plotting function\n","plot_spikes(neuron1, neuron2, sampling_rate_hz)"],"metadata":{"id":"VhD8xH9Pdy8u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UXArbLsoGtMm"},"source":["Let's now calculate the entropy of each neuron. For this we will use a simple function that follows the equations for entropy in the Background and the reading."]},{"cell_type":"code","source":["def calculate_entropy(neuron):\n","    \"\"\"\n","    Calculate the entropy of the spike train of a neuron.\n","    \"\"\"\n","    # Count the number of 0s and 1s in the array\n","    unique, counts = np.unique(neuron, return_counts=True)\n","    probs = counts / len(neuron)\n","\n","    # Calculate the entropy\n","    entropy = -np.sum(probs * np.log2(probs))\n","    return entropy"],"metadata":{"id":"yFgMW_r5XnrG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4xvLSWWpGtMm"},"source":["Now let's see how our neurons are doing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3qC9EJ6yGtMm"},"outputs":[],"source":["# Calculate and print the entropies\n","entropy_neuron1 = calculate_entropy(neuron1)\n","entropy_neuron2 = calculate_entropy(neuron2)\n","\n","print(f\"Entropy of Neuron 1: {entropy_neuron1}\")\n","print(f\"Entropy of Neuron 2: {entropy_neuron2}\")\n"]},{"cell_type":"markdown","metadata":{"id":"9Kq7fOrdGtMn"},"source":["---\n","### Question 1.1\n","\n","Increase the baseline firing rate parameter, *base_firing_prob*, by a factor of 10 to 0.2. What happens to the entropies of the neurons? Explain why this effect may or may not occur."]},{"cell_type":"markdown","source":["__Answer:__\n","\n","_(insert response here)_"],"metadata":{"id":"nddZbbE_V4C5"}},{"cell_type":"markdown","metadata":{"id":"eDw0X1MOGtMn"},"source":["---\n","### Question 1.2\n","\n","Put the baseline firing rate parameter back to 0.02. Increase the influence parameter, from neuron 1 to neuron 2, by a factor of 10 to 0.3. What happens to the entropies of the neurons? Explain why this effect may or may not occur."]},{"cell_type":"markdown","source":["__Answer:__\n","\n","_(insert response here)_"],"metadata":{"id":"K7WmXDx_V7dE"}},{"cell_type":"markdown","metadata":{"id":"EJtso6kplths"},"source":["## Section 2 - Mutual information\n","\n","We have our two happy little neurons firing. We've played with the parameters to see how it impacts their relative entropy. Now let's go back to the original parameter settings and see about estimating the mutual information between our neurons.\n","\n","Recall that to estimate the mutual information, we need to estimate the conditional entropy for $X$ on $Y$. For binary variables things are a bit easier because $H(X|Y) = H(Y) - H(X,Y)$. Which means we only need to get both independnet entropies and the joint entropy. This leaves us with this simple and beautiful equation: $I(X; Y) = H(X) + H(Y) - H(X, Y)$.\n","\n","We can easily write a function to do this. First we start with esimating the joint entropy of the two variables.\n","\n","$$ H(X,Y) = -\\Sigma_{x,y}P(X=x,Y=y)\\log_2 P(X=x, Y=y) $$"]},{"cell_type":"code","source":["def calculate_joint_entropy(neuron1, neuron2):\n","    # Calculate joint probability distribution\n","    joint_prob, _, _ = np.histogram2d(neuron1, neuron2, bins=2, range=[[0, 1], [0, 1]])\n","    joint_prob /= len(neuron1)\n","\n","    # Calculate joint entropy\n","    joint_entropy = -np.sum(joint_prob * np.log2(joint_prob + np.finfo(float).eps))\n","\n","    return joint_entropy"],"metadata":{"id":"TwTqtSVua0aZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's take a moment to take a look at this central estimate of the joint firing probabilities. We'll pull this out of the function and probe here."],"metadata":{"id":"JETB3G15peAm"}},{"cell_type":"code","source":["joint_prob, _, _ = np.histogram2d(neuron1, neuron2, bins=2, range=[[0, 1], [0, 1]])\n","joint_prob /= len(neuron1)\n","print(f\"{joint_prob}\")"],"metadata":{"id":"DlEtiGKspS9g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Most of the time, both neurons are 0 (upper left value). A small percentage of the time one neuron fires when the other does not (off diagonal entries). Finally, a small percentage of the time both neurons fire together (lower right)."],"metadata":{"id":"CsQvLc0-przW"}},{"cell_type":"markdown","source":["Now we can calculate the full mutual information by subtracting this joint entropy from the relative entropy of the two variables."],"metadata":{"id":"chKNWUFbfght"}},{"cell_type":"code","source":["def calculate_mutual_information(neuron1, neuron2):\n","    \"\"\"\n","    Calculate the mutual information between the spike trains of two neurons.\n","    \"\"\"\n","    entropy_neuron1 = calculate_entropy(neuron1)\n","    entropy_neuron2 = calculate_entropy(neuron2)\n","    joint_entropy = calculate_joint_entropy(neuron1, neuron2)\n","\n","    # Use the formula for mutual information\n","    mutual_information = entropy_neuron1 + entropy_neuron2 - joint_entropy\n","\n","    return mutual_information\n"],"metadata":{"id":"6SGp6JVOYH8F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lAVMmvD0GtMo"},"source":["So let's see what the mutal information is when we re-simulate our neurons back at the original parameter settings."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8nSxWZzeGtMo"},"outputs":[],"source":["# Time and rates\n","total_time_sec = 5\n","sampling_rate_hz = 1000\n","n_samples = total_time_sec * sampling_rate_hz\n","\n","# Probabilities\n","base_firing_prob = 0.02\n","influence = 0.03\n","noise_level = 0.01\n","\n","# Simulate the two neurons\n","neuron1 = simulate_neuron_spikes(n_samples, base_firing_prob, 0, noise_level)\n","neuron2 = simulate_neuron_spikes(n_samples, base_firing_prob, influence, noise_level, neuron1)\n","\n","# Calculate and print the mutual information\n","mutual_information = calculate_mutual_information(neuron2, neuron1)\n","print(f\"Mutual Information: {mutual_information}\")"]},{"cell_type":"markdown","metadata":{"id":"e88i6epuGtMo"},"source":["This reflects the mutual dependence between the two neurons. You can probably guess what questions we will ask about this."]},{"cell_type":"markdown","metadata":{"id":"pFvD0qrih8Io"},"source":["---\n","### Question 2.1\n","\n","Increase the baseline firing rate parameter, *base_firing_prob*, by a factor of 10 to 0.2. What happens to the mutual information of the neurons? Explain why this effect may or may not occur."]},{"cell_type":"markdown","source":["__Answer:__\n","\n","_(insert response here)_"],"metadata":{"id":"ftmgdv3nV-S8"}},{"cell_type":"markdown","metadata":{"id":"duITA8hciZBL"},"source":["---\n","### Question 2.2\n","\n","Put the baseline firing rate parameter back to 0.02. Increase the influence parameter, from neuron 1 to neuron 2, by a factor of 10 to 0.3. What happens to the mutual information of the neurons? Explain why this effect may or may not occur."]},{"cell_type":"markdown","source":["__Answer:__\n","\n","_(insert response here)_"],"metadata":{"id":"mZxbFtMwWAgv"}},{"cell_type":"markdown","metadata":{"id":"hfUSjU1RGtMp"},"source":["---\n","### Question 2.3\n","\n","Take the last set of simulations that you ran (with the influence set at 0.3) and switch the order of the neurons in the calculation of mutual information. What does or does not happen to the mutual information estimate? Why?"]},{"cell_type":"markdown","source":["__Answer:__\n","\n","_(insert response here)_"],"metadata":{"id":"c6jU5ZrkWDSw"}},{"cell_type":"markdown","source":["## Section 3 - KL divergence\n","\n","We can now move on to our information measure of interest, the KL divergence, $D_{KL}(P||Q)$. Let's start by setting up our neurons so that the first, presynaptic neuron has a strong influence on the spiking probability of the second, postsynaptic neuron.\n","\n","For this we will make a strong synaptic connection by changing the *influence* parameter to 0.99, which means that when neuron1 spikes, neuron2's firing probability increases by 99%."],"metadata":{"id":"bugRAeWQf3pl"}},{"cell_type":"code","source":["# Time and rates\n","total_time_sec = 5\n","sampling_rate_hz = 1000\n","n_samples = total_time_sec * sampling_rate_hz\n","\n","# Probabilities\n","base_firing_prob = 0.02\n","influence = 0.99\n","noise_level = 0.01\n","\n","# Simulate the two neurons\n","neuron1 = simulate_neuron_spikes(n_samples, base_firing_prob, 0, noise_level)\n","neuron2 = simulate_neuron_spikes(n_samples, base_firing_prob, influence, noise_level, neuron1)\n","\n","# Plot\n","plot_spikes(neuron1, neuron2, sampling_rate_hz)"],"metadata":{"id":"nQWtb5P-h6Gv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's write a simple function to calculate the KL divergence."],"metadata":{"id":"-sJgjq4tiMx1"}},{"cell_type":"code","source":["def calculate_kl_divergence(neuron1, neuron2):\n","    # Calculate probability distributions\n","    unique, counts = np.unique(neuron1, return_counts=True)\n","    p1 = counts / len(neuron1)\n","\n","    unique, counts = np.unique(neuron2, return_counts=True)\n","    p2 = counts / len(neuron2)\n","\n","    # Avoid division by zero and log(0) by adding a small constant\n","    epsilon = np.finfo(float).eps\n","    p1 = np.clip(p1, epsilon, 1)\n","    p2 = np.clip(p2, epsilon, 1)\n","\n","    # Calculate KL divergence\n","    kl_div = np.sum(p1 * np.log2(p1 / p2))\n","\n","    return kl_div"],"metadata":{"id":"U3_N4928h_Dw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Using the spiking data above, we get the following"],"metadata":{"id":"JTQWaumDiYPP"}},{"cell_type":"code","source":["kl_div_value = calculate_kl_divergence(neuron1, neuron2)\n","print(f\"KL Divergence: {kl_div_value}\")\n"],"metadata":{"id":"LCm80_7kiXP7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Just to prove to you that KL divergence measures similarity in distributions, let's make two completely independent neurons and look at their KL divergence score."],"metadata":{"id":"7K10D5VbnxSh"}},{"cell_type":"code","source":["# Simulate the two neurons\n","neuron3 = simulate_neuron_spikes(n_samples, base_firing_prob, 0, noise_level)\n","\n","kl_div_value = calculate_kl_divergence(neuron1, neuron3)\n","print(f\"KL Divergence: {kl_div_value}\")"],"metadata":{"id":"NWLHa-51iiBf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","### Question 3.1\n","\n","What does it mean that the KL diverence is higher for our connected neurons (neuron1, neuron2) than our unconnected neurons (neuron1, neuron3)"],"metadata":{"id":"4SlKwm9OonWJ"}},{"cell_type":"markdown","source":["__Answer:__\n","\n","_(insert response here)_"],"metadata":{"id":"Mk43SvWaWGvf"}},{"cell_type":"markdown","source":["---\n","### Question 3.2\n","\n","Go back to our connected neurons (neuron1 & neuron2) and change the *influence* parameter back to 0.03. How does the KL divergence change? Explain why this effect may or may not occur."],"metadata":{"id":"Me6hOdsVoUdE"}},{"cell_type":"markdown","source":["__Answer:__\n","\n","_(insert response here)_"],"metadata":{"id":"oe6qWmb6WJiT"}},{"cell_type":"markdown","source":["## Section 4 - Putting it together\n","\n","Now let's simulate a set of experiments were we systematically increase the *influence* parameter and see how our three measures change."],"metadata":{"id":"suJ4AvrDp5Rh"}},{"cell_type":"code","source":["# Time and rates\n","total_time_sec = 5\n","sampling_rate_hz = 1000\n","n_samples = total_time_sec * sampling_rate_hz\n","\n","# Probabilities\n","base_firing_prob = 0.02\n","noise_level = 0.01\n","\n","# Influence parameters\n","influences = [0.01, 0.1, 0.5, 0.75, 0.99]\n","\n","# For plotting\n","names = influences\n","\n","e1 = []\n","e2 = []\n","mi = []\n","kl = []\n","\n","for influence in influences:\n","  neuron1 = simulate_neuron_spikes(n_samples, base_firing_prob, 0, noise_level)\n","  neuron2 = simulate_neuron_spikes(n_samples, base_firing_prob, influence, noise_level, neuron1)\n","\n","  e1.append(calculate_entropy(neuron1))\n","  e2.append(calculate_entropy(neuron2))\n","  mi.append(calculate_mutual_information(neuron2, neuron1))\n","  kl.append(calculate_kl_divergence(neuron2, neuron1))\n"],"metadata":{"id":"FKexQ1m4qEkD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["First let's look at the relative entropies of the two neurons."],"metadata":{"id":"3j_8z-FAtj87"}},{"cell_type":"code","source":["# Now let's plot the entropies first\n","fig = plt.figure(figsize=(5, 5))\n","plt.bar([str(n) for n in names], e1, color=\"black\", alpha=0.6, label=\"neuron1\")\n","plt.bar([str(n) for n in names], e2, color=\"grey\", alpha=0.6, label=\"neuron2\")\n","plt.ylabel(\"Entropy\")\n","plt.xlabel(\"Influence\")\n","plt.tight_layout()\n","plt.legend()"],"metadata":{"id":"JwjY9JtssAPM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","### Question 4.1\n","\n","What do you notice about the relative entropies of the two neurons?"],"metadata":{"id":"viAyCwgxtDHd"}},{"cell_type":"markdown","source":["__Answer:__\n","\n","_(insert response here)_"],"metadata":{"id":"EEoiQJXuWOQo"}},{"cell_type":"markdown","source":["Next let's look at the mutual information."],"metadata":{"id":"0Ts2iDIYtfzl"}},{"cell_type":"code","source":["# Now let's plot the mutual information\n","fig = plt.figure(figsize=(5, 5))\n","plt.bar([str(n) for n in names], mi, color=\"black\")\n","plt.ylabel(\"Mutual information\")\n","plt.xlabel(\"Influence\")\n","plt.tight_layout()\n","plt.legend()"],"metadata":{"id":"N_gr69RKtpOc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","### Question 4.2\n","\n","What does the mutual information do as you increase the presynaptic influence on the postsynaptic cell? Why does this occur?"],"metadata":{"id":"QFj5vLONuDRu"}},{"cell_type":"markdown","source":["Finally, let's look at the KL divergence."],"metadata":{"id":"NZZvQ30cuPap"}},{"cell_type":"code","source":["# Finally KL divergence\n","fig = plt.figure(figsize=(5, 5))\n","plt.bar([str(n) for n in names], kl, color=\"black\")\n","plt.ylabel(\"KL Divergence\")\n","plt.xlabel(\"Influence\")\n","plt.tight_layout()\n","plt.legend()"],"metadata":{"id":"peeNjSkQuSAR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","### Question 4.3\n","\n","What does the KL divergence do as you increase the presynaptic influence on the postsynaptic cell? Why does this occur?"],"metadata":{"id":"ksAjcXBUudQS"}},{"cell_type":"markdown","source":["__Answer:__\n","\n","_(insert response here)_"],"metadata":{"id":"1SVP_OJ_WT-j"}},{"cell_type":"markdown","source":["---\n","### Question 4.4\n","\n","Put all three results together and in simple English, explain what is happening the properties of both neurons as you increase the influence parameter."],"metadata":{"id":"F2rLI50zujXw"}},{"cell_type":"markdown","source":["__Answer:__\n","\n","_(insert response here)_"],"metadata":{"id":"pbnwqP-bWVyR"}}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"nbformat":4,"nbformat_minor":0}