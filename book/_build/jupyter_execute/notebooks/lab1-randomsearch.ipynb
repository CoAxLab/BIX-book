{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BT1m8zbPtDVg"
   },
   "source": [
    "# Lab 1 - Random exploration\n",
    "\n",
    "This section has two goals.\n",
    "\n",
    "1. To get you familiar with the python library we will be using for most of the semester to simulate and explore exploration.\n",
    "2. Explore the utility of random exploration as an exploration strategy.\n",
    "\n",
    "\n",
    "Google Colab is a simple free way to run python code. See the _python_ chapter for more on it. Most of the chapters and assignments in this book have a button to open in Colab (see the rocket button at the top)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovQzFbr4l0lk"
   },
   "source": [
    "## Background\n",
    "\n",
    "The _explorationlib_ library has three primary parts for running simulations. Here we will outline the three and give a brief explanation for how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmS4VumSl0ll"
   },
   "source": [
    "### **Agents**\n",
    "\n",
    "_Agents_ are functions that have a specific set of 1) inputs, 2) computations, and 3) outputs. The input-outut form is defined by the environment (see below). For example, in the bacterial foraging environments the inputs are scent signals reflecting \"food\" and the ouputs are direction of movement along a 2-dimensional grid. For the bandit environments, the inputs are feedback signals (e.g., rewards) and the outputs are $n$ actions, where $n$ is the number of possible choice options. Agents have an internal _state_ object that is updated according to various algorithms that we will play with in class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qr2IugTGl0ll"
   },
   "source": [
    "### **Environments**\n",
    "\n",
    "_Environments_ define the contingencies that agents act on. Here we use the _gym_ library for most of our simulations. There are two classes of environments we will work with: foraging grids and bandits. These will be explained in detail later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWZphPbVl0ll"
   },
   "source": [
    "### **Experiment**\n",
    "\n",
    "The _experiment_ function takes an agent, an environment, and a set of relevant parameters and executes the simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6vM7u1jtrV5"
   },
   "source": [
    "## Section 0 - Setup\n",
    "\n",
    "Use the button to open this assignment in a colab. Once it is open, if it is open, run all the cells. Read each cell, then run it, that is. This simple test of the colab is also a good but basic introduction to _explorationlib_, which is the basis for all the experiments and assignments you will see.\n",
    "\n",
    "If there are no errors, celebrate and consider _this_ assignment complete.\n",
    "\n",
    "_Note:_ I assume that you, reader, are familiar with python programming already. If you are not, see the _Introduction to python_ assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejckkDAztrV5"
   },
   "source": [
    "### Install _explorationlib_\n",
    "Colab comes with many of the libraries we will need. It does not come with _explorationlib_. It's a module we will be using, and was written to support this book. Let's Install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8vJBfWqKug8"
   },
   "outputs": [],
   "source": [
    "# For local debuggging purposes\n",
    "# Mount Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# # Install files from Google Drive\n",
    "# !pip install --upgrade /content/drive/My\\ Drive/Code/explorationlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_e6BaGcLLBN"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade git+https://github.com/coaxlab/explorationlib\n",
    "!pip install --upgrade git+https://github.com/MattChanTK/gym-maze.git\n",
    "!pip install celluloid # for the gifs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1wPtf6ttrV5"
   },
   "source": [
    "### Import some modules\n",
    "Let us begin by importing some modules from the standard library. These are just some general purpose tools we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZLIk1zMRtDVh"
   },
   "outputs": [],
   "source": [
    "import shutil # For working with files\n",
    "import glob # To help search for pathnames\n",
    "import os # For working with the local operating system\n",
    "import copy # We will be using deep copy a lot\n",
    "import sys # For flexibility working with different OSs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Lbzoy9utrV6"
   },
   "source": [
    "Next we can import modules that are common to scientific programming in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tRo4a_8btrV6"
   },
   "outputs": [],
   "source": [
    "import numpy as np # General purpose utility for math operations\n",
    "import pandas as pd # For working with data frames\n",
    "import seaborn as sns # A great plotting utility\n",
    "import matplotlib.pyplot as plt # Another good plotting utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmngBATktrV6"
   },
   "source": [
    "Now let's grab a bunch of functions from _explorationlib_ to play with. If our install using _pip_ above worked out, this next cell should run without error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dIMHYRWVtrV6"
   },
   "outputs": [],
   "source": [
    "# All the explorers we will play with are called\n",
    "# \"agents\"; a bit of computer science jargon\n",
    "from explorationlib import agent\n",
    "\n",
    "# The environments we will simulate live in a \"gym\"\n",
    "from explorationlib import local_gym as gym\n",
    "\n",
    "# Computational experiments are run with 'experiment'\n",
    "from explorationlib.run import experiment\n",
    "\n",
    "# Here are some tools to select, save, and load\n",
    "# data from computational experiments\n",
    "from explorationlib.util import select_exp\n",
    "from explorationlib.util import load\n",
    "from explorationlib.util import save\n",
    "\n",
    "# A bunch of tools for plotting and for\n",
    "# movie making\n",
    "from explorationlib.plot import plot_position2d\n",
    "from explorationlib.plot import plot_length_hist\n",
    "from explorationlib.plot import plot_length\n",
    "from explorationlib.plot import plot_angle_hist\n",
    "from explorationlib.plot import plot_targets2d\n",
    "#from explorationlib.plot import render_2d\n",
    "from explorationlib.plot import show_gif\n",
    "\n",
    "# A couple metrics for scoring how well, or poorly,\n",
    "# an exploration experiment went.\n",
    "from explorationlib.score import search_efficiency\n",
    "from explorationlib.score import total_reward\n",
    "#from explorationlib.score import average_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIhkeP6wtrV7"
   },
   "source": [
    "Before we go too far, let's do some work to make the plots look nicer.  We don't _have_ to do this, but having clear data visualization helps us better understand our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HuGfJ_MPtrV7"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "plt.rcParams[\"axes.facecolor\"] = \"white\"\n",
    "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "plt.rcParams[\"font.size\"] = \"16\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbqeVdlXtrV7"
   },
   "source": [
    "Finally, let's set up some better autocomplete and development tools in our notebooks. Again, optional. If this errors out, you might consider skipping it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gCSE8g2il0lq"
   },
   "outputs": [],
   "source": [
    "# config IPCompleter.greedy=True\n",
    "# load_ext autoreload\n",
    "# autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQFnYiJZtrV8"
   },
   "source": [
    "### Make a space for the data\n",
    "Make a folder called \"data\" to keep experimental data in. We will use it for all our examples.\n",
    "\n",
    "**WARNING** When running in a colab, anything you save to \"data/\" will be lost as soon as you shutdown the colab. This is ok for now. We will cover ways to save your data permanently later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7rKAcfgl0lq"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWDG-lBItrV8"
   },
   "source": [
    "## Section 1 - Welcome to our virtual world\n",
    "\n",
    "We are now ready to make a little virtual world and little creatures in it that we can use as our test subjects.\n",
    "\n",
    "For today we will work just with random search strategies. In particular, we want to show how random movements in a simple foraging context is an effective, if not ideal, strategy for simple organisms.\n",
    "\n",
    "### Environment\n",
    "\n",
    "The context that our agent will explore in is a 2-dimensional flat world environment. The environment is enclosed by walls on all sides, preventing our agent from escaping.\n",
    "\n",
    "Throughout the space there are food sources sprinkled evenly and randomly throughout. Specifically we will distribute the food according to a uniform distribution, which means that every point in space within our environment has an equal likelihood of having a food pellet present. Every time the agent hits a food target they get 1 point.\n",
    "\n",
    "The _goal_ of the agent is to find at least one food source within the time alloted. More is obviously better, but all our agent needs is one little bite to survive.\n",
    "\n",
    "### Agents\n",
    "\n",
    "For most of this class we will be working with artifical agents that I call _valentinos_. Each valentino is a simulated agent that forages the little 2-dimensional virtual world. We have complete control over the valentino's behavior and will expand their capabilities as the class progresses.\n",
    "\n",
    "We will start simple today with an agent that uses Brownian motion to explore and find food. It is the simplest valentino possible. It doesn't use sensory signals or other environmental information to move about. It just randomly diffuses throughout the environment. We'll name this valentino _Rando_. Rando makes two decisions at every moment (i.e., time point). First, there is the decision of how far to move. We call this the _step size_ and will use the parameter $l_i$ to refer to the length of the step taken at the $i^{th}$ moment. This will be sampled according to an exponential distribution with a scale parameter $\\gamma$ (with $\\gamma=0.1$ for now) with a lot of short jumps and a few larger jumps.\n",
    "\n",
    "$$l_i \\sim \\exp(\\gamma)$$\n",
    "\n",
    "Let's see what this looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pABSU2TQEfCK"
   },
   "outputs": [],
   "source": [
    "# Set the scale parameter\n",
    "scale_param = 0.1\n",
    "\n",
    "# Generate exponential distribution data\n",
    "data = np.random.exponential(scale=scale_param, size=1000)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.hist(data, bins=30, density=True, alpha=0.6, color='b', edgecolor='black')\n",
    "\n",
    "# Plot the theoretical distribution for comparison\n",
    "x = np.linspace(0, np.max(data), 1000)\n",
    "y = (1/scale_param) * np.exp(-x/scale_param)\n",
    "plt.plot(x, y, 'r-', lw=2)\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Exponential Distribution (scale=0.1)')\n",
    "plt.xlabel('Step size')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfsRlulLEfCK"
   },
   "source": [
    "\n",
    "Along with step length, the agent also must decide the direction of movement. We will use the parameter $\\theta$ to represent the angle of movement in polar coordinates. At each moment $i$ the movement direction $\\theta_i$ will be sampled according to a uniform distribution ranging from 0 to $2\\pi$.\n",
    "\n",
    "$$\\theta_i \\sim U(0, 2*\\pi)$$\n",
    "\n",
    "This means that at any given moment, Rando can turn in any direction in a $360^{\\circ}$ circle. We can visualie this distribution of angles pretty easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wkInKSMcKQ8s"
   },
   "outputs": [],
   "source": [
    "n_points=1000\n",
    "n_bins=30\n",
    "\n",
    "# Generate uniform distribution of angles from 0 to 2*pi\n",
    "angles = np.random.uniform(0, 2*np.pi, n_points)\n",
    "\n",
    "# Create the polar histogram\n",
    "plt.figure(figsize=(4, 4))\n",
    "ax = plt.subplot(111, projection='polar')\n",
    "\n",
    "# Plot the histogram\n",
    "ax.hist(angles, bins=n_bins, density=True, color='b', alpha=0.75, edgecolor='black')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Histogram of angles (0 to 2*pi)', va='bottom')\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6l5xnIaEfCK"
   },
   "source": [
    "### Task\n",
    "\n",
    "We want to run a simple experiment using _explorationlib_. We will simulate Rando's behavior as they wander the environment seeking \"food\" resources that are detectable by scents (though our toy animal has no sensory abilities yet).\n",
    "\n",
    "1. Instantiate a single Rando agent, in our 2-d foraging environment.\n",
    "2. Run an experiment, for 500 steps.\n",
    "3. Plot the environment and the agent's path in it.\n",
    "4. Score the agent, and reward (targets), and its efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVQXD-rZtrV8"
   },
   "source": [
    "#### Instantiate\n",
    "\n",
    "We can get things started by specifying all of the relevant parameters and objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53E5uUIxtrV8"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "exp_name = \"data/explorationlib_rando.pkl\"   # where we can store the data\n",
    "num_experiments = 1             # we only want to run one experiment\n",
    "num_steps = 500                 # how many exploration steps in space?\n",
    "\n",
    "scale = .5             # The diffusion constant\n",
    "boundary = (10, 10)   # a 2d world, 10 by 10\n",
    "mode = \"stopping\"     # stop when we hit a wall\n",
    "num_targets = 100     # how many targets in the arena?\n",
    "\n",
    "# Setup targets. Targets are an abstraction for the\n",
    "# thing we are exploring to find. For now, all targets\n",
    "# are just the number 1 placed randomly about.\n",
    "targets = gym.uniform_targets(num_targets, boundary)\n",
    "values = gym.constant_values(targets, 1)\n",
    "\n",
    "# Setup agent and env\n",
    "env = gym.Bounded(boundary=boundary, mode=mode)\n",
    "env.add_targets(targets, values)\n",
    "rando = agent.Diffusion2d(scale=scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KFqwyvttrV8"
   },
   "source": [
    "#### Run\n",
    "\n",
    "We use the _experiment_ function to execute the simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYbsPnW2trV8"
   },
   "source": [
    "(If you've been playing with this notebook for a while, then clear the old output first.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krvd-84jtrV8"
   },
   "outputs": [],
   "source": [
    "# Cleanup old versions\n",
    "for path in glob.glob(f\"{exp_name}\"):\n",
    "    os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hN-r-XNMtrV9"
   },
   "outputs": [],
   "source": [
    "# Run!\n",
    "exp_data = experiment(\n",
    "    f\"Rando\",\n",
    "    rando,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    seed=5858,\n",
    "    dump=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vuj3pIgZtrV9"
   },
   "source": [
    "#### Plot\n",
    "\n",
    "_explorationlib_ has a set of built in visualization functions that we loaded at the beginning. We'll use a few here to see how our agents are doing.\n",
    "\n",
    "First let's now see our environment. The plot below will show the foraging space for Rando and each dot indicates the location of a food source. Notice the uniform scattering of food across the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmgXxNXgtrV9"
   },
   "outputs": [],
   "source": [
    "# Plot the 2d env, and the targets (black dots)\n",
    "plot_targets2d(env, boundary=boundary, title=\"Foraging Env.\", figsize=(3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wjhj-wxBMenT"
   },
   "source": [
    "Now let's take a look at Rando's behavior. We can do this by plotting the positions that the agent (red line) took during this one example run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGaAstFxtrV9"
   },
   "outputs": [],
   "source": [
    "# View size\n",
    "plot_boundary = (20, 20)\n",
    "\n",
    "# Agent\n",
    "ax = None\n",
    "ax = plot_position2d(\n",
    "    select_exp(exp_data,0),\n",
    "    boundary=plot_boundary,\n",
    "    label=f\"Rando\",\n",
    "    title=f\"Brownian Exp.\",\n",
    "    color=\"red\",\n",
    "    alpha=0.6,\n",
    "    ax=None,\n",
    ")\n",
    "\n",
    "ax = plot_targets2d(\n",
    "    env,\n",
    "    boundary=boundary,\n",
    "    color=\"black\",\n",
    "    alpha=1,\n",
    "    label=\"Targets\",\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CEJnzIGM0zc"
   },
   "source": [
    "Notice that there are quite a few parameters you can play with to change the size of the plot, colors, and so on. See `explorationlib.plot` for all the options\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PGAbFhMW74N"
   },
   "source": [
    "Finally, let's see how our distribution of step lengths looks. Here we will just plot the step at each jump (or turn) and the histogram of step lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdteA4lFUtLL"
   },
   "outputs": [],
   "source": [
    "# View size\n",
    "plot_boundary = (20, 20)\n",
    "\n",
    "# Agent\n",
    "ax=None\n",
    "ax=plot_length(select_exp(exp_data,0),color='blue',figsize=(4,3))\n",
    "ax=plot_length_hist(select_exp(exp_data,0),color='blue', loglog=False, figsize=(3.72,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgjhUpT-XWJ0"
   },
   "source": [
    "It doesn't look quite Normally distributed, as Brownian motion would expect. But likely that's because we only have about 500 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahV-N9MetrV9"
   },
   "source": [
    "#### Score\n",
    "\n",
    "Now we can quantify Rando's behavior in this simple experiment.\n",
    "\n",
    "First we will look at _total_reward_, which counts how many times Rando ran into a food target. Recall that every time the agent hits a food source, they get 1 point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJBc02Es0h-r"
   },
   "outputs": [],
   "source": [
    "# Total rewards (targets) collected\n",
    "total_reward(exp_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3wwn3CtQgJ9"
   },
   "source": [
    "So our agent hit their food this many times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3adWnOTEfCL"
   },
   "source": [
    "Next we will look at the efficiency of the search, which is just the number of movements taken by the agent over the number of food targets collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q0fj9QcP0mj5"
   },
   "outputs": [],
   "source": [
    "# How efficient (movements/target) was the search\n",
    "search_efficiency(exp_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGXt7v56QqTv"
   },
   "source": [
    "This means that our Rando agent took around 5 steps to hit a new food source.\n",
    "\n",
    "We want this score to be as low as possible. Right now this number doesn't mean much because we have nothing to compare Rando's behavior to. We will come back to it in later labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVSpi1crtrV9"
   },
   "source": [
    "#### Play\n",
    "\n",
    "Go back to the top of this section, change the _random seed number_ in the cell where we ran the experiment and run it again. Each time you do you should generate a new exploration pattern. You'll get different rewards and efficiency each time as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUZWm0hOXmxi"
   },
   "source": [
    "### Question 1.1\n",
    "\n",
    "Rerun the simulation above but now have the agent run for 10,000 steps. What happens to the histogram (or distribution) of step lengths?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4pHoH3MX3D2"
   },
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbtEcBK2trV9"
   },
   "source": [
    "### Question 1.2\n",
    "\n",
    "Repeat the experiment 10 times with different random seeds. Keep track of the two outcome measures, _total reward_ and _search efficiency_. How much variation do you see from run to run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjUsuFA9l0lt"
   },
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPoIOezVEfCM"
   },
   "source": [
    "### Question 1.3\n",
    "\n",
    "Now set the random seed number back to 52317 and rerun the original simulation. Then change the number of steps in each simulation (the _num_steps_ parameter) to 1000. How do the two outcome measures (_total reward_ and _search efficiency_) change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHgM9OQeEfCM"
   },
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1OuYUJYeXtOTwLuBEF5goOhSMa4gGrOiB",
     "timestamp": 1663858766061
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}