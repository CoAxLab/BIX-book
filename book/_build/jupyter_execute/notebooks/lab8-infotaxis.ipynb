{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHIeLwWce4tw"
   },
   "source": [
    "# Lab 8 - Infotaxis\n",
    "\n",
    "Here we will explore how our little artificial organisms do two things:\n",
    "\n",
    "1. Learn a *concept* of information as a signal.\n",
    "2. How this concept of information facilitates learning in noisy environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNr4qUoAfTlH"
   },
   "source": [
    "## Background\n",
    "\n",
    "In this lab we return to _taxic explorations_. Recall that in Lab 3 we look at what happens when sense information is not just noisy, but partially observed. In otherwords, when there is distortion in the channel of information.\n",
    "\n",
    "Here we will compare the simple chemotaxic framework, where agents simply follow the gradient of the scent, to an _infotaxic_ framework where agents follow the _information_ carried in the signal instead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pU7nMEBcth6"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvWP2GmYgBhZ"
   },
   "source": [
    "## E Coli, again?\n",
    "Recall our basic model of E. Coli exploration is as simple as can be.\n",
    "\n",
    "- When the gradient is positive, meaning you are going \"up\" the gradient, the probability of turning is set to _p pos_.\n",
    "- When the gradient is negative, the turning probability is set to _p neg_. (See code below, for an example).\n",
    "- If the agent \"decides\" to turn, the direction it takes is uniform random.\n",
    "- The length of travel before the next turn decision is sampled from an exponential distribution just like the _DiffusionDiscrete_\n",
    "\n",
    "### Information agents\n",
    "We will study three agents. One who does _chemotaxis_. One who does a kind of _infotaxis_. One that does random search (aka Diffusion). For fun, let's call this one a _randotaxis_ agent. This last rando-agent is really a control. A reference point.\n",
    "\n",
    "In a sense the _chemotaxis_ agent only tries to answer question Q2 (above). While _infotaxis_ only tries to answer Q1. They are extreme strategies, in other words. The bigger question we will ask, in a very limited setting, is which extreme method is better _generally_?\n",
    "\n",
    "\n",
    "### Costly cognition\n",
    "Both _chemo-_ and _infotaxis_ agents will use a DDM-style accumulator to try and make better decisions about the direction of the gradient. These decisions are of course statistical in nature. (We won't be tuning the accumulator parameters in this lab. Assume the parameters I give you, for the DDM, are \"good enough\".)\n",
    "\n",
    "For the _randotaxis_ agent number of steps means the number of steps or actions the agent takes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pw41OUVdgxfG"
   },
   "source": [
    "## Reviewing the definition of _chemotaxis_:\n",
    "Our _chemotaxis_ agent (_AccumulatorGradientGrid_) tries to directly estimate the gradient $\\nabla$ in scent by comparing the level of scent at the last grid position it occupied to the current scent level ($o$). By last grid position here we mean the last grid position when it moved last.\n",
    "\n",
    "$$\\nabla \\approx o_t - o_{t-1}$$\n",
    "\n",
    "Because an accumulator is present, our chemo- sequentially tries to estimate this gradient by sampling the new current location, until the threshold is met.\n",
    "\n",
    " Chemo-accumulators have what we can think of as two cognitive or behavioral steps:\n",
    "\n",
    "1. Use an accumulator to (stabely) estimate the chemo gradient\n",
    "2. Use the gradient to make turning decisions\n",
    "\n",
    "## A definition of _infotaxis_:\n",
    "Compared to chemo- definition the definition of infotaxis is a little more involved. It has what we can think of as five cognitive or behavioral steps:\n",
    "\n",
    "1. Use an accumulator to (stabely) estimate if there is a scent or not. AKA hits and misses.\n",
    "2. Build a probability model of hits/misses (at every point)\n",
    "3. Measure information gained when probability model changes\n",
    "4. Measure the gradient of information gains\n",
    "5. Use the gradient to make turning decisions\n",
    "\n",
    "_Note_: Even though the info-accumulator is more complex, it can take advantage of missing scent information to drive its behavior. It can also use positive scent hits, of course, too.\n",
    "\n",
    "If you want to look at exactly how this agent works, check out line 1242 in the _agents.py_ file: /usr/local/lib/python3.7/dist-packages/explorationlib/agent.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GbUZLcmhC87"
   },
   "source": [
    "## Section - Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAVJYYdhhKSL"
   },
   "source": [
    "First let's set things up for the two parts of the lab. You've done this before, so we don't need to specify each installation and module step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GaM7UFV6yKE1"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade git+https://github.com/coaxlab/explorationlib\n",
    "!pip install --upgrade git+https://github.com/MattChanTK/gym-maze.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zI0CIEceyQx4"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "import explorationlib\n",
    "from explorationlib.local_gym import ScentGrid\n",
    "\n",
    "from explorationlib.agent import DiffusionGrid\n",
    "from explorationlib.agent import AccumulatorGradientGrid\n",
    "from explorationlib.agent import AccumulatorInfoGrid\n",
    "\n",
    "from explorationlib.run import experiment\n",
    "from explorationlib.util import select_exp\n",
    "from explorationlib.util import load\n",
    "from explorationlib.util import save\n",
    "\n",
    "from explorationlib.local_gym import uniform_targets\n",
    "from explorationlib.local_gym import constant_values\n",
    "from explorationlib.local_gym import ScentGrid\n",
    "from explorationlib.local_gym import create_grid_scent\n",
    "from explorationlib.local_gym import add_noise\n",
    "from explorationlib.local_gym import create_grid_scent_patches\n",
    "\n",
    "from explorationlib.plot import plot_position2d\n",
    "from explorationlib.plot import plot_length_hist\n",
    "from explorationlib.plot import plot_length\n",
    "from explorationlib.plot import plot_targets2d\n",
    "from explorationlib.plot import plot_scent_grid\n",
    "from explorationlib.plot import plot_targets2d\n",
    "\n",
    "from explorationlib.score import total_reward\n",
    "from explorationlib.score import num_death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQbjVvs6p1Zz"
   },
   "outputs": [],
   "source": [
    "# Pretty plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "%config IPCompleter.greedy=True\n",
    "plt.rcParams[\"axes.facecolor\"] = \"white\"\n",
    "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "plt.rcParams[\"font.size\"] = \"16\"\n",
    "\n",
    "# Dev\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnKyN9oM0GTm"
   },
   "source": [
    "## Section 1 - Chemotaxis vs. infotaxis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpB4JhugmNJE"
   },
   "source": [
    "In this section we take on accumulating evidence as a policy for decision making. Our venue is still chemotaxis, but now our sensors are noisy. The presence of this uncertainty makes decisions--of the kind common to decision theory--a necessity.\n",
    "\n",
    "Let's see just how helpful the concept of information for chemotaxic search can be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "StauX74y0QjT"
   },
   "outputs": [],
   "source": [
    "# Noise and missing scents\n",
    "p_scent = 0.1\n",
    "noise_sigma = 1\n",
    "\n",
    "# Shared\n",
    "num_experiments = 100\n",
    "num_steps = 400\n",
    "seed_value = 5838\n",
    "\n",
    "# ! (leave alone)\n",
    "detection_radius = 1\n",
    "max_steps = 1\n",
    "min_length = 1\n",
    "num_targets = 50\n",
    "target_boundary = (10, 10)\n",
    "\n",
    "# Targets\n",
    "prng = np.random.RandomState(seed_value)\n",
    "targets = uniform_targets(num_targets, target_boundary, prng=prng)\n",
    "values = constant_values(targets, 1)\n",
    "\n",
    "# Scents\n",
    "scents = []\n",
    "for _ in range(len(targets)):\n",
    "    coord, scent = create_grid_scent_patches(\n",
    "        target_boundary, p=1.0, amplitude=1, sigma=2)\n",
    "    scents.append(scent)\n",
    "\n",
    "# Env\n",
    "env = ScentGrid(mode=None)\n",
    "env.seed(seed_value)\n",
    "env.add_scents(targets, values, coord, scents, noise_sigma=noise_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ip-6AMyMmzjk"
   },
   "source": [
    "Again we are working a scent grid environment where each target emits noisy chemical signals (scents) according to our definitions above.\n",
    "\n",
    "Here's an example of our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4xKn5Oog3HB8"
   },
   "outputs": [],
   "source": [
    "plot_boundary = (10, 10)\n",
    "num_experiment = 0\n",
    "ax = None\n",
    "ax = plot_targets2d(\n",
    "    env,\n",
    "    boundary=plot_boundary,\n",
    "    color=\"black\",\n",
    "    alpha=1,\n",
    "    label=\"Targets\",\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4SRoMBksnAML"
   },
   "source": [
    "We will use 3 agents in these sims:\n",
    "\n",
    "- Rando: Uses random Brownian motion search.\n",
    "- Chemo: Uses only the detected scent gradient to make a decision.\n",
    "- Info: Estimates how much *information* is encoded in the scent signal to make a decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvKupKt10V5s"
   },
   "outputs": [],
   "source": [
    "# Agents\n",
    "\n",
    "# Random search agent\n",
    "diff = DiffusionGrid(min_length=min_length, scale=1)\n",
    "diff.seed(seed_value)\n",
    "\n",
    "drift_rate = 1\n",
    "threshold = 3\n",
    "\n",
    "# Chemotaxis agent\n",
    "chemo = AccumulatorGradientGrid(\n",
    "    min_length=min_length,\n",
    "    max_steps=max_steps,\n",
    "    drift_rate=drift_rate,\n",
    "    threshold=threshold,\n",
    "    accumulate_sigma=1\n",
    ")\n",
    "chemo.seed(seed_value)\n",
    "\n",
    "\n",
    "# Infotaxis agent\n",
    "info = AccumulatorInfoGrid(\n",
    "    min_length=min_length,\n",
    "    max_steps=max_steps,\n",
    "    drift_rate=drift_rate,\n",
    "    threshold=threshold,\n",
    "    accumulate_sigma=1\n",
    ")\n",
    "\n",
    "info.seed(seed_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awCEAAyGngBm"
   },
   "source": [
    "Now let's run the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVTGtSPT0fN_"
   },
   "outputs": [],
   "source": [
    "# Experiments\n",
    "rand_exp = experiment(\n",
    "    f\"rand\",\n",
    "    diff,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")\n",
    "chemo_exp = experiment(\n",
    "    f\"chemo\",\n",
    "    chemo,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")\n",
    "info_exp = experiment(\n",
    "    f\"info\",\n",
    "    info,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNn5xm5nno3t"
   },
   "source": [
    "Let's plot an example experiment. Here I'm choosing the second run for each agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-1x_tV4C0jMS"
   },
   "outputs": [],
   "source": [
    "plot_boundary = (10, 10)\n",
    "\n",
    "# -\n",
    "num_experiment = 2\n",
    "ax = None\n",
    "ax = plot_position2d(\n",
    "    select_exp(chemo_exp, num_experiment),\n",
    "    boundary=plot_boundary,\n",
    "    label=\"Chemo\",\n",
    "    color=\"blue\",\n",
    "    alpha=0.6,\n",
    "    ax=ax,\n",
    ")\n",
    "ax = plot_position2d(\n",
    "    select_exp(info_exp, num_experiment),\n",
    "    boundary=plot_boundary,\n",
    "    label=\"Info\",\n",
    "    color=\"green\",\n",
    "    alpha=0.6,\n",
    "    ax=ax,\n",
    ")\n",
    "ax = plot_position2d(\n",
    "    select_exp(rand_exp, num_experiment),\n",
    "    boundary=plot_boundary,\n",
    "    label=\"Rando\",\n",
    "    color=\"grey\",\n",
    "    alpha=0.8,\n",
    "    ax=ax,\n",
    ")\n",
    "ax = plot_targets2d(\n",
    "    env,\n",
    "    boundary=plot_boundary,\n",
    "    color=\"black\",\n",
    "    alpha=1,\n",
    "    label=\"Targets\",\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sw96O-9pnzTb"
   },
   "source": [
    "Hard to distinguish their individual behaviors, but our agents seem to be exploring.\n",
    "\n",
    "Now let's evaluate some metrics of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ICBnDBbep1Z7"
   },
   "outputs": [],
   "source": [
    "# Results\n",
    "results = [rand_exp, info_exp, chemo_exp]\n",
    "names = [\"Rando\", \"Info\", \"Chemo\"]\n",
    "colors = [\"blue\", \"green\", \"grey\"]\n",
    "\n",
    "# Score by eff\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    l = 0.0\n",
    "    for r in res:\n",
    "        l += r[\"agent_total_l\"][-1]\n",
    "    scores.append(l)\n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(4, 3))\n",
    "plt.bar(names, m, yerr=sd, color=\"black\", alpha=0.6)\n",
    "plt.ylabel(\"Total distance\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPaHH65P2EP8"
   },
   "outputs": [],
   "source": [
    "# Results\n",
    "results = [rand_exp, info_exp, chemo_exp]\n",
    "names = [\"Rando\", \"Info\", \"Chemo\"]\n",
    "colors = [\"blue\", \"green\", \"grey\"]\n",
    "\n",
    "# Score by eff\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    scores.append(num_death(res))\n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(4, 3))\n",
    "plt.bar(names, m, yerr=sd, color=\"black\", alpha=0.6)\n",
    "plt.ylabel(\"Deaths\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Kl9LjUL2IMK"
   },
   "outputs": [],
   "source": [
    "# Results\n",
    "results = [rand_exp, info_exp, chemo_exp]\n",
    "names = [\"Rando\", \"Info\", \"Chemo\"]\n",
    "colors = [\"blue\", \"green\", \"grey\"]\n",
    "\n",
    "# Score by eff\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    r = total_reward(res)\n",
    "    scores.append(r)\n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(5, 4))\n",
    "plt.bar(names, m, yerr=sd, color=\"black\", alpha=0.6)\n",
    "plt.ylabel(\"Total reward\")\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "\n",
    "# Dists\n",
    "fig = plt.figure(figsize=(5, 4))\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    plt.hist(s, label=name, color=c, alpha=0.5, bins=np.linspace(0, np.max(scores), 50))\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1-KWIgnn_Rp"
   },
   "source": [
    "### Question 1.1\n",
    "\n",
    "How does each of our agents perform across the performance measures we have chosen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yqI8WJQjoMsi"
   },
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Jyz-sNYoTe6"
   },
   "source": [
    "### Question 1.2\n",
    "\n",
    "Is having a concept of information (i.e., the Info agent)helpful in these sorts of noisy environments? Why or why not based on how the agents performed? Compare to both Rando and Chemo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0CwjAoYojm4"
   },
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Do4kPlRpBtV"
   },
   "source": [
    "## Section 2 - Robustness of information searching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTTuDJx9pOfI"
   },
   "source": [
    "In this final section we will see how the distortion in the channel driven by missing information influences the efficiency of our Info agent.\n",
    "\n",
    "Here we will test a range of *p_scent* values. Essentially we will be turning *down* the distortion as *p_scent* increases. For these experiments we will hold the *noise_sigma* constant at 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TkSIg5Wt-WOo"
   },
   "outputs": [],
   "source": [
    "# Our parameters\n",
    "p_scents = [0.05, 0.25, .50, .75, .95]\n",
    "\n",
    "# For plotting\n",
    "colors = [\"darkgreen\", \"seagreen\", \"cadetblue\", \"steelblue\", \"mediumpurple\"]\n",
    "names = p_scents # list(range(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfRStyhvqCnS"
   },
   "source": [
    "Let's run these experiments. All of the parameters for the agent and environment (aside from *p_scent*) are specified below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "uJm1Y-21_Fn_"
   },
   "outputs": [],
   "source": [
    "# Define the accumulation parameters\n",
    "drift_rate = 1\n",
    "threshold = 3\n",
    "accumulate_sigma = 1.0\n",
    "\n",
    "# Define non-scent probability values\n",
    "noise_sigma = 1\n",
    "amplitude = 1\n",
    "detection_radius = 1\n",
    "max_steps = 1\n",
    "min_length = 1\n",
    "num_targets = 50\n",
    "target_boundary = (10, 10)\n",
    "\n",
    "# How many experiments to run\n",
    "num_experiments = 100\n",
    "\n",
    "# Infotaxis agent\n",
    "info = AccumulatorInfoGrid(\n",
    "    min_length=min_length,\n",
    "    max_steps=max_steps,\n",
    "    drift_rate=drift_rate,\n",
    "    threshold=threshold,\n",
    "    accumulate_sigma=1\n",
    ")\n",
    "info.seed(seed_value)\n",
    "\n",
    "# Run\n",
    "results = []\n",
    "\n",
    "for i, p_scent in zip(names, p_scents):\n",
    "  # Targets\n",
    "  prng = np.random.RandomState(seed_value)\n",
    "  targets = uniform_targets(num_targets, target_boundary, prng=prng)\n",
    "  values = constant_values(targets, 1)\n",
    "\n",
    "  # Scents\n",
    "  scents = []\n",
    "  for _ in range(len(targets)):\n",
    "      coord, scent = create_grid_scent_patches(\n",
    "          target_boundary, p=p_scent, amplitude=amplitude, sigma=noise_sigma)\n",
    "      scents.append(scent)\n",
    "\n",
    "  # Env\n",
    "  env = ScentGrid(mode=None)\n",
    "  env.seed(seed_value)\n",
    "  env.add_scents(targets, values, coord, scents, noise_sigma=noise_sigma)\n",
    "\n",
    "  exp = experiment(\n",
    "    f\"info\",\n",
    "    info,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    "  )\n",
    "\n",
    "  results.append(exp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EFjD_f3qRkk"
   },
   "source": [
    "Now let us take a look at the performance of our agent across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "4U5dm66iBDk7"
   },
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for result in results:\n",
    "    l = 0.0\n",
    "    for r in result:\n",
    "        l += r[\"agent_total_l\"][-1]\n",
    "    scores.append(l)\n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for s in scores:\n",
    "    m.append(np.mean(s))\n",
    "\n",
    "# -\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.bar([str(n) for n in names], m, color=\"black\", alpha=0.6)\n",
    "plt.ylabel(\"Total distance\")\n",
    "plt.xlabel(\"p_scent\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "mqNtWAC6EIRg"
   },
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for result in results:\n",
    "    scores.append(num_death(result))\n",
    "\n",
    "# -\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.bar([str(n) for n in names], scores, color=\"black\", alpha=0.6)\n",
    "plt.ylabel(\"Deaths\")\n",
    "plt.xlabel(\"p_scent\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "PkhiHHGcEIuN"
   },
   "outputs": [],
   "source": [
    "# Max Score\n",
    "scores = []\n",
    "for result in results:\n",
    "    r = total_reward(result)\n",
    "    scores.append(r)\n",
    "\n",
    "# Tabulate\n",
    "m = []\n",
    "for s in scores:\n",
    "    m.append(np.max(s))\n",
    "\n",
    "# -\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.bar([str(n) for n in names], m, color=\"black\", alpha=0.6)\n",
    "plt.ylabel(\"Best agent's score\")\n",
    "plt.xlabel(\"p_scent\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "EIhxp7iREQwX"
   },
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for result in results:\n",
    "    r = total_reward(result)\n",
    "    scores.append(r)\n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for s in scores:\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "plt.bar([str(n) for n in names], m, yerr=sd, color=\"black\", alpha=0.6)\n",
    "plt.ylabel(\"Avg. score\")\n",
    "plt.xlabel(\"p_scent\")\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "\n",
    "# Dists of means\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "for (i, s, c) in zip(names, scores, colors):\n",
    "    plt.hist(s, label=i, color=c, alpha=0.5, bins=list(range(1,50,1)))\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xlabel(\"p_scent\")\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GiCg65RaqaIx"
   },
   "source": [
    "### Question 2.1\n",
    "\n",
    "How does increasing *p_scent* impact our Info agent's performance? Explain why this particular pattern emerges in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "1bbJGSzvquGm"
   },
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fiRzusWqxzh"
   },
   "source": [
    "### Question 2.2\n",
    "\n",
    "Re-run the simulations from this section, but now change the drift-rate from 1.0 to 0.75. How and why does this influence the agent's behavior (compared to the higher drift-rate)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "j2amk4bYqyOj"
   },
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkEcIec9swC5"
   },
   "source": [
    "### Question 2.3\n",
    "\n",
    "Now set the drift-rate back to 1.0 and reduce the boundary height from 3.0 to 1.5. Re-run the simulations in again. How and why does this influence the agent's behavior (compared to the higher boundary height)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "pGsRHKx3tYnz"
   },
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}