{"cells":[{"cell_type":"markdown","metadata":{"id":"mhT-lilScl3G"},"source":["# Lab 8 - Patch foraging\n","\n","This lab has 4 main components designed to give provide an theoretical and experiental/interactive understanding of foraging in patchy environments.\n","\n","Sections:\n","1. Consider foraging in \"patchy\" environments, and build a simulated patchy environment.\n","1. Predict and evaluate the behavior of agents with different exploration strategies in the patchy environment.\n","1. Predict and evaluate how agents' behavior & performance will change as characteristics of the patchy environment change.\n"]},{"cell_type":"markdown","metadata":{"id":"hKkGUWhIhFds"},"source":["## Background"]},{"cell_type":"markdown","metadata":{"id":"fYdbAhg2tyIU"},"source":["### Real environments are \"patchy\".\n","- So far, we've been playing around with simulated environments where food (\"targets\") are spread uniformly over space.\n","- In reality, environments are often \"patchy\". For example, berries or flowers may be concentrated on isolated bushes, which are spaced apart from one another\n","\n","### Patchy environments define the foraging problem.\n","- The problem of foraging requires that an organism navigate through an environment to find food, balancing the metabolic costs of movement with the energy provided by collected morsels.\n","- In an environment where targets are dispersed uniformly, foraging is rather simple - moving around from via some form of random walk, collecting targets, perhaps with behavior imformed by scent cues.\n","- A patchy environment complicates things - with no food in between patches, between-patch movement can become dangerous at long transit times. Patchy environments thus demand a foraging strategy that is sensitive to the sparsely-dense stucture of food availability.\n","\n","### We can think of optimal foraging in terms of a simplified model.\n","- Charnov (1976) proposed a model of optimal foraging.\n","- Charnov's model formulates foraging space as divided into sections (patches) of different types, and the inter-patch space between them. See the illustration of this from his paper below (and consider how this simple model matches the real environment above so much better than uniformly distributed targets!).\n","![Charnov patch model illustration screenshot](https://raw.githubusercontent.com/CoAxLab/BiologicallyIntelligentExploration/main/Labs/Charnov_patches_screenshot.png)\n","- When we think about foraging in this way, it becomes clear that there is an important distinction between time spent *in* and *between* patches.\n","- Critically, off-patch time and on-patch time (for different patch types) determines energy intake rate for organisms.\n","- In Charnov's model, within-patch energy is depleted as a forager consumes the energy within that patch. Thus there is a point at which an optimal forager should leave a patch in aims of finding another. This is where the formulas in his marginal value come into play.\n","\n","Next up, let's get set up to run some patch environment simulations!"]},{"cell_type":"markdown","metadata":{"id":"cKiFhdTxk_n3"},"source":["## Section 0 - Setup\n","\n","The setup is the same as prior labs."]},{"cell_type":"markdown","metadata":{"id":"y15pYjBArTuw"},"source":["Install our code libraries as usual"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KNjgieoaW_xL"},"outputs":[],"source":["!pip install --upgrade git+https://github.com/coaxlab/explorationlib\n","!pip install --upgrade git+https://github.com/MattChanTK/gym-maze.git\n","!pip install celluloid # for the gifs"]},{"cell_type":"markdown","metadata":{"id":"ymQNT9dlrvnL"},"source":["Import specific modules from the libraries we loaded. We'll use these modules to create and plot enviornments, run experiments with different exploration agents in these environments, visualize their behaviors, and evaluate their performance according to various metrics."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7TTuWHQEcF1O"},"outputs":[],"source":["# Import misc\n","import shutil\n","import glob\n","import os\n","import copy\n","import sys\n","\n","# Vis - 1\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Exp\n","from explorationlib.run import experiment\n","from explorationlib.util import select_exp\n","from explorationlib.util import load\n","from explorationlib.util import save\n","\n","# Agents\n","from explorationlib.agent import DiffusionGrid\n","from explorationlib.agent import GradientDiffusionGrid\n","from explorationlib.agent import GradientInfoGrid\n","\n","# Env\n","from explorationlib.local_gym import ScentGrid\n","from explorationlib.local_gym import create_grid_scent\n","from explorationlib.local_gym import create_grid_scent_patches\n","from explorationlib.local_gym import uniform_targets\n","from explorationlib.local_gym import uniform_patch_targets\n","from explorationlib.local_gym import constant_values\n","\n","# Vis - 2\n","from explorationlib.plot import plot_position2d\n","from explorationlib.plot import plot_length_hist\n","from explorationlib.plot import plot_length\n","from explorationlib.plot import plot_targets2d\n","from explorationlib.plot import plot_scent_grid\n","\n","# Score\n","from explorationlib.score import total_reward\n","from explorationlib.score import num_death\n","from explorationlib.score import on_off_patch_time"]},{"cell_type":"markdown","metadata":{"id":"pu48IqFjlfeN"},"source":["## Section 1 - Building a simulated patchy environment\n","\n","- Below is some environment setup code that should look pretty standard compared to the previous labs you've seen in this class.\n","- The key difference is the generation of targets using a new function, `uniform_patch_targets()` - in other labs we've only used `uniform_targets()`.\n","- This new function randomly places circular patches in the environment, and then places targets at random with uniform probability within each patch.\n","- To specify the characteristics of the patchy environment we want to generate, we supply information about how many patches we want (`n_patches`), how many targets we want per patch (`n_per_patch`), and how large we want each patch to be (`radius`).\n","- To be clear, our simulation won't be perfect - this function can create overlapping patches, and our simulations don't take into account the depletion of patches over time.\n","\n","We will create a patch environmet with *7 patches* of *10 targets each*. Make each patch have a *radius of 2 units*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PmMf7zZ2YVd5"},"outputs":[],"source":["# Noise and missing scents\n","p_scent = 0.1\n","noise_sigma = 2\n","\n","# Shared agent parameters\n","num_experiments = 50\n","num_steps = 500\n","seed_value = 52317\n","\n","# Environment parameters\n","detection_radius = 1\n","max_steps = 1\n","min_length = 1\n","n_patches = 8 #         # number of patches\n","n_per_patch = 20 #      # number targets per patch\n","radius = 1 #            # radius of each patch\n","target_boundary = (10, 10)\n","\n","# Generate patches of argets\n","prng = np.random.RandomState(seed_value)\n","targets, patch_locs = uniform_patch_targets(n_patches, target_boundary, radius, n_per_patch, prng=prng)\n","values = constant_values(targets, 1)\n","\n","# Generate scents from targets\n","scents = []\n","for _ in range(len(targets)):\n","    coord, scent = create_grid_scent_patches(\n","        target_boundary, p=1.0, amplitude=1, sigma=2)\n","    scents.append(scent)\n","\n","# Create ScentGrid environment\n","env = ScentGrid(mode=None)\n","env.seed(seed_value)\n","env.add_scents(targets, values, coord, scents, noise_sigma=noise_sigma)"]},{"cell_type":"markdown","metadata":{"id":"3TUxsKoQbtm_"},"source":["### Visualizing this environment\n","\n","Run the code below to visualize the patchy environment that you just built!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"377nSrZKYXlA"},"outputs":[],"source":["plot_boundary = (10, 10)\n","ax = None\n","ax = plot_targets2d(\n","    env,\n","    boundary=plot_boundary,\n","    color=\"black\",\n","    alpha=1,\n","    label=\"Targets\",\n","    ax=ax,\n",")"]},{"cell_type":"markdown","metadata":{"id":"EJtso6kplths"},"source":["## Section 2 - Different exploration strategies in the patchy environment\n","\n","Now we will predict and evaluate the behavior of agents with different exploration strategies in the patcy environment. We will pick up on the three types of agents we have worked with so far.\n","\n","- A random, Brownian random walker\n","- A simple chemotaxis agent\n","- A curious infotaxis agent\n","\n","These are agents that we have used in pervious labs. See those labs to remember how these agents work.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"51ELNt49mN9U"},"outputs":[],"source":["# Agents\n","\n","# Random search agent\n","rand = DiffusionGrid(min_length=min_length)\n","rand.seed(seed_value)\n","\n","# Common parameters\n","min_length = 1\n","p_neg = 0.8\n","p_pos = 0.2\n","\n","# Chemotaxis agent\n","chemo = GradientDiffusionGrid(\n","    min_length=min_length,\n","    p_neg=p_neg,\n","    p_pos=p_pos,\n",")\n","chemo.seed(seed_value)\n","\n","\n","# Infotaxis agent\n","threshold=0.05\n","info = GradientInfoGrid(\n","    min_length=min_length,\n","    p_neg=p_neg,\n","    p_pos=p_pos,\n","    threshold=threshold,\n",")\n","\n","info.seed(seed_value)"]},{"cell_type":"markdown","metadata":{"id":"fPYY1S7njHHs"},"source":["**Note:** you may like to start the next experiments running as you answer these questions."]},{"cell_type":"markdown","metadata":{"id":"pFvD0qrih8Io"},"source":["---\n","### Question 2.1\n","\n","Which agent do you think will spend the most time in patches? Which agent do you think will spend the least time in patches? Why?"]},{"cell_type":"markdown","source":["__Answer:__\n","\n","_(insert response here)_"],"metadata":{"id":"gWnoBcQfWsIG"}},{"cell_type":"markdown","metadata":{"id":"duITA8hciZBL"},"source":["---\n","### Question 2.2\n","\n","Which agent do you think will accumulate the most reward (spend the most time next to targets?). Which do you think will spend the least? Why?"]},{"cell_type":"markdown","source":["__Answer:__\n","\n","_(insert response here)_"],"metadata":{"id":"ZQgq18NLWuqd"}},{"cell_type":"markdown","metadata":{"id":"DzfHsCj3iw-_"},"source":["---\n","### Question 2.3\n","\n","Which agent do you think will have the most deaths? Which do you think will have the least? Why?"]},{"cell_type":"markdown","source":["__Answer:__\n","\n","_(insert response here)_"],"metadata":{"id":"ICJPAxoIWy79"}},{"cell_type":"markdown","metadata":{"id":"a9zl0zcujDIV"},"source":["Run the code below to perform a number of experiments, simulating how these different agents behave in the patch environment that you built."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yx2xNi1LmWSy"},"outputs":[],"source":["# Experiments\n","rand_exp = experiment(\n","    f\"rand\",\n","    rand,\n","    env,\n","    num_steps=num_steps,\n","    num_experiments=num_experiments,\n","    dump=False,\n","    split_state=True,\n","    seed=seed_value\n",")\n","chemo_exp = experiment(\n","    f\"chemo\",\n","    chemo,\n","    env,\n","    num_steps=num_steps,\n","    num_experiments=num_experiments,\n","    dump=False,\n","    split_state=True,\n","    seed=seed_value\n",")\n","info_exp = experiment(\n","    f\"info\",\n","    info,\n","    env,\n","    num_steps=num_steps,\n","    num_experiments=num_experiments,\n","    dump=False,\n","    split_state=True,\n","    seed=seed_value\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"zv0lEbzjjwRp"},"source":["Run the code below to plot behavior of each agent type during one experiment example."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"csb8q_6kmd_N"},"outputs":[],"source":["plot_boundary = (10, 10)\n","\n","# -\n","num_experiment = num_experiments - 1\n","ax = None\n","ax = plot_position2d(\n","    select_exp(chemo_exp, num_experiment),\n","    boundary=plot_boundary,\n","    label=\"Chemo\",\n","    color=\"blue\",\n","    alpha=0.6,\n","    ax=ax,\n",")\n","ax = plot_position2d(\n","    select_exp(rand_exp, num_experiment),\n","    boundary=plot_boundary,\n","    label=\"Rando\",\n","    color=\"red\",\n","    alpha=0.8,\n","    ax=ax,\n",")\n","ax = plot_position2d(\n","    select_exp(info_exp, num_experiment),\n","    boundary=plot_boundary,\n","    label=\"Info\",\n","    color=\"green\",\n","    alpha=0.6,\n","    ax=ax,\n",")\n","ax = plot_targets2d(\n","    env,\n","    boundary=plot_boundary,\n","    color=\"black\",\n","    alpha=1,\n","    label=\"Targets\",\n","    ax=ax,\n",")"]},{"cell_type":"markdown","metadata":{"id":"kHwF9brYj6xt"},"source":["### Question 2.4\n","\n","Describe the behavior of each agent type in the experiment visualization above. Does the behavior match what you expected? Why do you think you see the specific pattern of behavior for each agent?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"duPbtZ-vkaF_"},"outputs":[],"source":["# Write your answer here as a comment. Explain yourself."]},{"cell_type":"markdown","metadata":{"id":"8S1yLZ6ck0n2"},"source":["### Quantify time on-patches for each agent type\n","\n","The code below makes use of a new scoring function, `on_off_patch_time()`. This function takes experiment results data and analyzes it to see how many time steps were spent on vs. off patches. Run the code block below to measure and plot the proportion of total time steps each agent spends on patches. See if the results match your predictions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j6R7wg_OmjwW"},"outputs":[],"source":["# Results\n","results = [rand_exp, chemo_exp, info_exp]\n","names = [\"Rando\", \"Chemo\",\"Info\"]\n","colors = [\"red\", \"blue\",\"green\"]\n","\n","# Score by on_patch_time #eff\n","scores = []\n","for name, res, color in zip(names, results, colors):\n","    #scores.append(num_death(res))\n","    on_patch_steps, off_patch_steps = on_off_patch_time(res, num_experiments, patch_locs, radius)\n","    scores.append(np.divide(on_patch_steps,(np.array(on_patch_steps) + off_patch_steps)))\n","\n","# Tabulate\n","m, sd = [], []\n","for (name, s, c) in zip(names, scores, colors):\n","    m.append(np.mean(s))\n","    sd.append(np.std(s))\n","\n","# Plot means\n","fig = plt.figure(figsize=(5, 4))\n","plt.bar(names, m, yerr=sd, color=\"black\", alpha=0.6)\n","plt.ylabel(\"Proportion of time steps on patches\")\n","plt.tight_layout()\n","sns.despine()\n","\n","# Dists\n","fig = plt.figure(figsize=(5, 4))\n","for (name, s, c) in zip(names, scores, colors):\n","    plt.hist(s, label=name, color=c, alpha=0.5, bins=np.linspace(0, np.max(scores), 50))\n","    plt.legend()\n","    plt.xlabel(\"proportion of time steps on patches\")\n","    plt.tight_layout()\n","    sns.despine()"]},{"cell_type":"markdown","metadata":{"id":"kehI4hL2n2nx"},"source":["### Quantify total reward for each agent type\n","\n","Check if your predictions were correct."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eca2mXrQmmxJ"},"outputs":[],"source":["# Results\n","results = [rand_exp, chemo_exp, info_exp]\n","names = [\"Rando\", \"Chemo\",\"Info\"]\n","colors = [\"red\", \"blue\",\"green\"]\n","\n","# Score by eff\n","scores = []\n","for name, res, color in zip(names, results, colors):\n","    r = total_reward(res)\n","    scores.append(r)\n","\n","# Tabulate\n","m, sd = [], []\n","for (name, s, c) in zip(names, scores, colors):\n","    m.append(np.mean(s))\n","    sd.append(np.std(s))\n","\n","# Plot means\n","fig = plt.figure(figsize=(5, 4))\n","plt.bar(names, m, yerr=sd, color=\"black\", alpha=0.6)\n","plt.ylabel(\"Total reward\")\n","plt.tight_layout()\n","sns.despine()\n","\n","# Dists\n","fig = plt.figure(figsize=(5, 4))\n","for (name, s, c) in zip(names, scores, colors):\n","    plt.hist(s, label=name, color=c, alpha=0.5, bins=np.linspace(0, np.max(scores), 50))\n","    plt.legend()\n","    plt.xlabel(\"Score\")\n","    plt.tight_layout()\n","    sns.despine()"]},{"cell_type":"markdown","metadata":{"id":"RcPykectoC4e"},"source":["### Quantify deaths for each agent type\n","\n","Check if your predictions were correct."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fa9g7sxUoIDw"},"outputs":[],"source":["# Results\n","results = [rand_exp, chemo_exp, info_exp]\n","names = [\"Rando\", \"Chemo\", \"Info\"]\n","colors = [\"red\", \"blue\", \"green\"]\n","\n","# Score by eff\n","scores = []\n","for name, res, color in zip(names, results, colors):\n","    scores.append(num_death(res))\n","\n","# Tabulate\n","m, sd = [], []\n","for (name, s, c) in zip(names, scores, colors):\n","    m.append(np.mean(s))\n","    sd.append(np.std(s))\n","\n","# Plot means\n","fig = plt.figure(figsize=(4, 3))\n","plt.bar(names, m, yerr=sd, color=\"black\", alpha=0.6)\n","plt.ylabel(\"Deaths\")\n","plt.tight_layout()\n","sns.despine()"]},{"cell_type":"markdown","metadata":{"id":"vwMrfxy5oetS"},"source":["---\n","### Question 2.5\n","\n","Were any of your predictions wrong? If so, what do you think caused the unexpected results?"]},{"cell_type":"markdown","source":["__Answer:__\n","\n","_(insert response here)_"],"metadata":{"id":"WF-3HjoIW3AQ"}},{"cell_type":"markdown","metadata":{"id":"hIv_A-CZ06EI"},"source":["---\n","### Question 2.6\n","\n","Compare time on patch, total rewards, and deaths for the Chemo agent. What does this pattern tell you about the influence of a simple chemotaxis strategy for foraging?"]},{"cell_type":"markdown","source":["__Answer:__\n","\n","_(insert response here)_"],"metadata":{"id":"qi9FWHljW5NQ"}},{"cell_type":"markdown","metadata":{"id":"yZAHaaZKlw69"},"source":["## Section 3 - Exploration strategies in different patchy environments\n","\n","Let's look at a slightly harder foraging problem: sparse patches. We will create a new patch environment, just like our first one, but with only two patches of 20 targets each."]},{"cell_type":"markdown","metadata":{"id":"T85Pr9Ep2NWo"},"source":["---\n","### Question 3.1\n","\n","Which agent do you predict will do the best in terms of on-patch-proportion, total reward, and death."]},{"cell_type":"markdown","source":["__Answer:__\n","\n","_(insert response here)_"],"metadata":{"id":"XZIglRX3W8-o"}},{"cell_type":"markdown","source":["Here's how we'll set up the experiment"],"metadata":{"id":"HWALuRLdrJHw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"K8Jt--YeqLMw"},"outputs":[],"source":["# Noise and missing scents\n","p_scent = 0.1\n","noise_sigma = 2\n","\n","# Shared agent parameters\n","num_experiments = 50\n","num_steps = 500\n","seed_value = 52317               # seed value for random number generator\n","\n","# Environment parameters\n","detection_radius = 1\n","max_steps = 1\n","min_length = 1\n","n_patches = 2 #          # number of patches\n","n_per_patch = 20 #       # number targets per patch\n","radius = 3 #             # radius of each patch\n","target_boundary = (10, 10)\n","\n","# Generate patches of argets\n","prng = np.random.RandomState(seed_value)\n","targets, patch_locs = uniform_patch_targets(n_patches, target_boundary, radius, n_per_patch, prng=prng)\n","\n","values = constant_values(targets, 1)\n","\n","# Generate scents from targets\n","scents = []\n","for _ in range(len(targets)):\n","    coord, scent = create_grid_scent_patches(\n","        target_boundary, p=1.0, amplitude=1, sigma=2)\n","    scents.append(scent)\n","\n","# Create ScentGrid environment\n","env = ScentGrid(mode=None)\n","env.seed(seed_value)\n","env.add_scents(targets, values, coord, scents, noise_sigma=noise_sigma)"]},{"cell_type":"markdown","source":["Now let's see our environment."],"metadata":{"id":"KiCVoLZhrMiw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z9yg-5ifvxLF"},"outputs":[],"source":["plot_boundary = (10, 10)\n","num_experiment = 0\n","ax = None\n","ax = plot_targets2d(\n","    env,\n","    boundary=plot_boundary,\n","    color=\"black\",\n","    alpha=1,\n","    label=\"Targets\",\n","    ax=ax,\n",")"]},{"cell_type":"markdown","source":["Time to test our agents."],"metadata":{"id":"L-E0VJ24rPzi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"B1fvlqrvqWyk"},"outputs":[],"source":["# Experiments\n","rand_exp = experiment(\n","    f\"rand\",\n","    rand,\n","    env,\n","    num_steps=num_steps,\n","    num_experiments=num_experiments,\n","    dump=False,\n","    split_state=True,\n","    seed=seed_value\n",")\n","chemo_exp = experiment(\n","    f\"chemo\",\n","    chemo,\n","    env,\n","    num_steps=num_steps,\n","    num_experiments=num_experiments,\n","    dump=False,\n","    split_state=True,\n","    seed=seed_value\n",")\n","info_exp = experiment(\n","    f\"info\",\n","    info,\n","    env,\n","    num_steps=num_steps,\n","    num_experiments=num_experiments,\n","    dump=False,\n","    split_state=True,\n","    seed=seed_value\n",")"]},{"cell_type":"markdown","source":["First let's take a look at a good demonstration trial. I'm picking one example that will make sense in a little bit."],"metadata":{"id":"ZiY5PxyHrR3a"}},{"cell_type":"code","source":["plot_boundary = (10, 10)\n","\n","# - Picking a good example that is consistent with summary results\n","num_experiment = 4\n","ax = None\n","ax = plot_position2d(\n","    select_exp(chemo_exp, num_experiment),\n","    boundary=plot_boundary,\n","    label=\"Chemo\",\n","    color=\"blue\",\n","    alpha=0.6,\n","    ax=ax,\n",")\n","ax = plot_position2d(\n","    select_exp(rand_exp, num_experiment),\n","    boundary=plot_boundary,\n","    label=\"Rando\",\n","    color=\"red\",\n","    alpha=0.8,\n","    ax=ax,\n",")\n","ax = plot_position2d(\n","    select_exp(info_exp, num_experiment),\n","    boundary=plot_boundary,\n","    label=\"Info\",\n","    color=\"green\",\n","    alpha=0.6,\n","    ax=ax,\n",")\n","ax = plot_targets2d(\n","    env,\n","    boundary=plot_boundary,\n","    color=\"black\",\n","    alpha=1,\n","    label=\"Targets\",\n","    ax=ax,\n",")"],"metadata":{"id":"xWRoBfdnqkGo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Okay, now let's take a look at time on patch."],"metadata":{"id":"MbvIKIekrjDl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hSaXAH1Wt9EN"},"outputs":[],"source":["# Results\n","results = [rand_exp, chemo_exp, info_exp]\n","names = [\"Rando\", \"Chemo\", \"Info\"]\n","colors = [\"red\", \"blue\", \"green\"]\n","\n","# Score by on_patch_time #eff\n","scores = []\n","for name, res, color in zip(names, results, colors):\n","    #scores.append(num_death(res))\n","    on_patch_steps, off_patch_steps = on_off_patch_time(res, num_experiments, patch_locs, radius)\n","    scores.append(np.divide(on_patch_steps,(np.array(on_patch_steps) + off_patch_steps)))\n","\n","# Tabulate\n","m, sd = [], []\n","for (name, s, c) in zip(names, scores, colors):\n","    m.append(np.mean(s))\n","    sd.append(np.std(s))\n","\n","# Plot means\n","fig = plt.figure(figsize=(5, 4))\n","plt.bar(names, m, yerr=sd, color=\"black\", alpha=0.6)\n","plt.ylabel(\"Proportion of time steps on patches\")\n","plt.tight_layout()\n","sns.despine()\n","\n","# Dists\n","fig = plt.figure(figsize=(5, 4))\n","for (name, s, c) in zip(names, scores, colors):\n","    plt.hist(s, label=name, color=c, alpha=0.5, bins=np.linspace(0, np.max(scores), 50))\n","    plt.legend()\n","    plt.xlabel(\"proportion of time steps on patches\")\n","    plt.tight_layout()\n","    sns.despine()"]},{"cell_type":"markdown","source":["Interesting patterns that I have some questions about at the end.\n","\n","Now, let's look at total rewards."],"metadata":{"id":"Tz1mJCDGrsgE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2qwCNGWW1kPm"},"outputs":[],"source":["# Results\n","results = [rand_exp, chemo_exp, info_exp]\n","names = [\"Rando\", \"Chemo\", \"Info\"]\n","colors = [\"red\", \"blue\", \"green\"]\n","\n","# Score by eff\n","scores = []\n","for name, res, color in zip(names, results, colors):\n","    r = total_reward(res)\n","    scores.append(r)\n","\n","# Tabulate\n","m, sd = [], []\n","for (name, s, c) in zip(names, scores, colors):\n","    m.append(np.mean(s))\n","    sd.append(np.std(s))\n","\n","# Plot means\n","fig = plt.figure(figsize=(5, 4))\n","plt.bar(names, m, yerr=sd, color=\"black\", alpha=0.6)\n","plt.ylabel(\"Total reward\")\n","plt.tight_layout()\n","sns.despine()\n","\n","# Dists\n","fig = plt.figure(figsize=(5, 4))\n","for (name, s, c) in zip(names, scores, colors):\n","    plt.hist(s, label=name, color=c, alpha=0.5, bins=np.linspace(0, np.max(scores), 50))\n","    plt.legend()\n","    plt.xlabel(\"Score\")\n","    plt.tight_layout()\n","    sns.despine()"]},{"cell_type":"markdown","source":["Interesting. Look at the difference between our chemotaxis agent and our curious (infotaxis) agent.\n","\n","Now we can look at the contrastive performance measure of deaths."],"metadata":{"id":"vn8bzJwwrzRd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"d1bnWRkFqfMh"},"outputs":[],"source":["# Results\n","results = [rand_exp, chemo_exp, info_exp]\n","names = [\"Rando\", \"Chemo\", \"Info\"]\n","colors = [\"red\", \"blue\", \"green\"]\n","\n","# Score by eff\n","scores = []\n","for name, res, color in zip(names, results, colors):\n","    scores.append(num_death(res))\n","\n","# Tabulate\n","m, sd = [], []\n","for (name, s, c) in zip(names, scores, colors):\n","    m.append(np.mean(s))\n","    sd.append(np.std(s))\n","\n","# Plot means\n","fig = plt.figure(figsize=(4, 3))\n","plt.bar(names, m, yerr=sd, color=\"black\", alpha=0.6)\n","plt.ylabel(\"Deaths\")\n","plt.tight_layout()\n","sns.despine()"]},{"cell_type":"markdown","metadata":{"id":"NDJuNeNV2z40"},"source":["---\n","### Question 3.2\n","\n","Did the results match your predictions? If not, why do you think you saw the results that came up?"]},{"cell_type":"markdown","source":["__Answer:__\n","\n","_(insert response here)_"],"metadata":{"id":"5r7pn4JCXCtH"}},{"cell_type":"markdown","metadata":{"id":"UyU_sOjhzdzV"},"source":["---\n","### Question 3.3\n","\n","Re-run the above simulations in Section 3, but change the seed value for the random number generator. Do this four different times, once each with the following values: 2074, 3074, 4074, 5074.\n","\n","What do you see in the performance of the agents with each new seed value (which specifies different unique environments)?"]},{"cell_type":"markdown","source":["__Answer:__\n","\n","_(insert response here)_"],"metadata":{"id":"jqpK9ahRXFEC"}}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"nbformat":4,"nbformat_minor":0}