{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhT-lilScl3G"
   },
   "source": [
    "# Lab 1- Information theory\n",
    "\n",
    "This lab has a few goals designed to get you comfortable with working with python and playing with the basiscs of information theory.\n",
    "\n",
    "Sections:\n",
    "1. Entropy\n",
    "2. Mutual information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKkGUWhIhFds"
   },
   "source": [
    "## Background\n",
    "\n",
    "We are going to assume a basic familiarity with python. If you need an introduction, see the introduction notebook at the beginning of this book. That should get you enough familiarity to get started.\n",
    "\n",
    "For this lab we won't be working with our main library, _explorationlib_. We'll use a few standard libraries to both get more comfortable with python and to refresh the concepts of information theory discussed in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYdbAhg2tyIU"
   },
   "source": [
    "### **What is entropy again?**\n",
    "\n",
    "- According to Shannon's definition, the entropy $H(X)$ of a discrete random variable is: $H(X)= \\sum_{x \\in X} p(x) \\log _2 p(x)^{-1}$.\n",
    "- Here we are assuming that the discrete variable is binary, hence the use of $\\log _2$\n",
    "- A key aspect of the definition entropy is the 'surprise' termn, $p(x)^{-1}$. The more surprising an outcome is, the more valuable that bit of information is. We will return to this later in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Utwl97pgGtMj"
   },
   "source": [
    "### **Entropy between variables**.\n",
    "\n",
    "- The _joint entropy_ between two discrete random variables, $X$ and $Y$, is just an expansion of the regular concept of entropy: $H(X, Y)= \\sum_{x \\in X} p(x,y) \\log _2 p(x,y)^{-1}$.\n",
    "- Note that in here, $p(x,y)$ is the joint probability distribution between the variables.\n",
    "- Expanding out from the rules of conditional probabilities (thank you Mr. Bayes), the joint entropy can be expanded as the product of the entropy of the primary variable and the conditional entropy of the two variables: $H(X,Y) = H(X) + H(Y|X)$. _Pay attention to the ordering of terms, it gets important later_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMWAKQ_PGtMj"
   },
   "source": [
    "### **Mutual information between variables**.\n",
    "\n",
    "- Sticking with the idea of joint entropy, the _conditional entropy_ $H(X|Y)$ reflects the residual entropy of $X$ after you have knowledge about $Y$. This is expressed as: $H(X|Y)=H(X)-I(X;Y)$.\n",
    "- We call this second term, $I(X;Y)$ the _mutual information_ between $X$ and $Y$. It is the information provided by Y about X. If you were in statistics and working with continuous variables, this would be the correlation (or covariance).\n",
    "- We can rewrite the equation above as $I(X;Y) = H(X) - H(X|Y) = \\sum_{x \\in X, y \\in Y} p(x,y) \\log _2 \\frac{p(x,y)}{p(x)p(y)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsaYc-X4GtMj"
   },
   "source": [
    "### **Transfer entropy between variables**.\n",
    "\n",
    "- In statistics (including information theory), causality follows Humeian logic: cause temporally preceeds effect. Thus, the term 'causality' in a statistical sense really just looks at the asymmetry in cross-correlations between two or more time series.\n",
    "- Information theory relies on _transfer entropy_ $T(X \\rightarrow Y)$ to estimate this statistical relationship.\n",
    "- Trasfer entropy captures how much information the source has transfered to the receiver: $T(X \\rightarrow Y) = I(Y_{f} ; X_p | Y_p) = H(Y_f|Y_p) - H(Y_f|X_p, Y_p)$, which can be rewritten as $T(X \\rightarrow Y) =  \\sum_{y_f \\in Y_f, x_p \\in X_p, y_p \\in Y_p} p(y_f,x_p,y_p) \\log _2 \\frac{p(y_f,x_p|y_p)}{p(y_f|y_p)p(x_p|y_p)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKiFhdTxk_n3"
   },
   "source": [
    "## Section - Setup\n",
    "\n",
    "Use the rocket-shaped button to open this assignment in a colab. Once it is open, if it is open, run all the cells. Read each cell, then run it, that is. It's that simple.\n",
    "\n",
    "We will discuss a different way to work with notebooks for the Exercises, but for now just load in Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymQNT9dlrvnL"
   },
   "source": [
    "For this lab we'll load some standard libraries for working with mutual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1689953957611,
     "user": {
      "displayName": "Timothy Verstynen",
      "userId": "04724296371017749786"
     },
     "user_tz": 240
    },
    "id": "7TTuWHQEcF1O"
   },
   "outputs": [],
   "source": [
    "# Import the general libraries we will be using\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pu48IqFjlfeN"
   },
   "source": [
    "## Section 1 - Entropy\n",
    "\n",
    "- For this section we will simulate two cells, a presynaptic and postsynaptic cell. The data will consist of spikes. We will vary the degree of dependency between the neurons and look at their relative entropies.\n",
    "- One way to achieve this is to use a Poisson process to simulate the neurons. Here, the firing of each neuron at each time step is modeled as a binary event that occurs with some probability. The influence of the first neuron on the second is represented by increasing the firing probability of the second neuron when the first one fires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 276,
     "status": "ok",
     "timestamp": 1689953957886,
     "user": {
      "displayName": "Timothy Verstynen",
      "userId": "04724296371017749786"
     },
     "user_tz": 240
    },
    "id": "PmMf7zZ2YVd5",
    "outputId": "295ebb1a-63bb-42f9-f2ae-7a65ccfc2ac0"
   },
   "outputs": [],
   "source": [
    "# Time and rates\n",
    "total_time_sec = 5\n",
    "sampling_rate_hz = 1000\n",
    "n_samples = total_time_sec * sampling_rate_hz\n",
    "\n",
    "# Probabilities\n",
    "base_firing_prob = 0.02\n",
    "influence = 0.03\n",
    "noise_level = 0.01\n",
    "\n",
    "def simulate_neuron_spikes(n_samples, base_firing_prob, influence, noise_level, neuron1=None):\n",
    "    \"\"\"\n",
    "    Simulate the spiking of a neuron.\n",
    "\n",
    "    If neuron1 is not None, then this neuron is influenced by neuron1.\n",
    "    \"\"\"\n",
    "    spikes = np.zeros(n_samples)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        firing_prob = base_firing_prob\n",
    "\n",
    "        if neuron1 is not None and neuron1[i] == 1:\n",
    "            firing_prob += influence\n",
    "\n",
    "        firing_prob += noise_level * np.random.randn()\n",
    "\n",
    "        if np.random.rand() < firing_prob:\n",
    "            spikes[i] = 1\n",
    "\n",
    "    return spikes\n",
    "\n",
    "# Simulate the two neurons\n",
    "neuron1 = simulate_neuron_spikes(n_samples, base_firing_prob, 0, noise_level)\n",
    "neuron2 = simulate_neuron_spikes(n_samples, base_firing_prob, influence, noise_level, neuron1)\n",
    "\n",
    "print(\"Neuron 1 spike train:\", neuron1)\n",
    "print(\"Neuron 2 spike train:\", neuron2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oe57oi3oGtMl"
   },
   "source": [
    "What we have created are two binary vectors, _neuron1_ and _neuron2_, that represent the spike trains of two neurons over 5 seconds at a 1 kHz sampling rate.\n",
    "\n",
    "Each time step is a binary event where 1 represents a spike and 0 represents no spike. The base firing probability is 2%, but this is modified by two factors: the influence of the first neuron on the second, and some independent noise.\n",
    "\n",
    "The _influence_ parameter controls how much the first neuron affects the second: whenever the first neuron fires, the firing probability of the second neuron is increased by the influence factor. The _noise level_ parameter controls the amount of independent noise in the firing probabilities. This noise is modeled as Gaussian noise and is independent for each neuron and each time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TUxsKoQbtm_"
   },
   "source": [
    "### Visualizing our two neurons\n",
    "\n",
    "We can use a raster plot to visualize the spike times of our neurons. For this we will use matplotlib.\n",
    "\n",
    "In a raster plot, each row corresponds to a different repetition of the experiment (a different neuron in this case), and the x-axis represents time. Each small vertical line (marker) represents a spike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "executionInfo": {
     "elapsed": 854,
     "status": "ok",
     "timestamp": 1689953958739,
     "user": {
      "displayName": "Timothy Verstynen",
      "userId": "04724296371017749786"
     },
     "user_tz": 240
    },
    "id": "377nSrZKYXlA",
    "outputId": "2647a972-7de4-4e5e-86f8-05d4a0256d4d"
   },
   "outputs": [],
   "source": [
    "def plot_spikes(neuron1, neuron2, sampling_rate_hz):\n",
    "    \"\"\"\n",
    "    Plot the spikes of the two neurons as a raster plot.\n",
    "    \"\"\"\n",
    "    time_points = np.arange(len(neuron1)) / sampling_rate_hz\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 5), sharex=True, sharey=True)\n",
    "\n",
    "    # Neuron 1\n",
    "    ax[0].eventplot(time_points[neuron1 == 1], color='black')\n",
    "    ax[0].set_title(\"Neuron 1\")\n",
    "    ax[0].set_ylabel(\"Spikes\")\n",
    "\n",
    "    # Neuron 2\n",
    "    ax[1].eventplot(time_points[neuron2 == 1], color='black')\n",
    "    ax[1].set_title(\"Neuron 2\")\n",
    "    ax[1].set_ylabel(\"Spikes\")\n",
    "    ax[1].set_xlabel(\"Time (sec)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the plotting function\n",
    "plot_spikes(neuron1, neuron2, sampling_rate_hz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXArbLsoGtMm"
   },
   "source": [
    "Let's now calculate the entropy of each neuron. For this we will use a simple function that follows the equations for entropy in the Background and the reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1689953958740,
     "user": {
      "displayName": "Timothy Verstynen",
      "userId": "04724296371017749786"
     },
     "user_tz": 240
    },
    "id": "RFvoOVSUGtMm"
   },
   "outputs": [],
   "source": [
    "def calculate_entropy(neuron):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of the spike train of a neuron.\n",
    "    \"\"\"\n",
    "    # Calculate the probabilities of 0 and 1\n",
    "    p_0 = np.sum(neuron == 0) / len(neuron)\n",
    "    p_1 = np.sum(neuron == 1) / len(neuron)\n",
    "\n",
    "    # Use the formula for entropy of a binary variable\n",
    "    # Note: When p_0 or p_1 is 0, their contribution to the entropy is 0\n",
    "    entropy = 0\n",
    "    if p_0 > 0:\n",
    "        entropy -= p_0 * np.log2(p_0)\n",
    "    if p_1 > 0:\n",
    "        entropy -= p_1 * np.log2(p_1)\n",
    "\n",
    "    return entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xvLSWWpGtMm"
   },
   "source": [
    "Now let's see how our neurons are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1689953958740,
     "user": {
      "displayName": "Timothy Verstynen",
      "userId": "04724296371017749786"
     },
     "user_tz": 240
    },
    "id": "3qC9EJ6yGtMm",
    "outputId": "746b11f1-eb3c-4b87-ab77-fb63cf8dd427"
   },
   "outputs": [],
   "source": [
    "# Calculate and print the entropies\n",
    "entropy_neuron1 = calculate_entropy(neuron1)\n",
    "entropy_neuron2 = calculate_entropy(neuron2)\n",
    "\n",
    "print(f\"Entropy of Neuron 1: {entropy_neuron1}\")\n",
    "print(f\"Entropy of Neuron 2: {entropy_neuron2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Kq7fOrdGtMn"
   },
   "source": [
    "### Question 1.1\n",
    "\n",
    "Increase the baseline firing rate parameter, *base_firing_prob*, by a factor of 10 to 0.2. What happens to the entropies of the neurons? Explain why this effect may or may not occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1689953958740,
     "user": {
      "displayName": "Timothy Verstynen",
      "userId": "04724296371017749786"
     },
     "user_tz": 240
    },
    "id": "1hMT61ogGtMn"
   },
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDw0X1MOGtMn"
   },
   "source": [
    "### Question 1.2\n",
    "\n",
    "Put the baseline firing rate parameter back to 0.02. Increase the influence parameter, from neuron 1 to neuron 2, by a factor of 10 to 0.3. What happens to the entropies of the neurons? Explain why this effect may or may not occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 282,
     "status": "ok",
     "timestamp": 1689953959019,
     "user": {
      "displayName": "Timothy Verstynen",
      "userId": "04724296371017749786"
     },
     "user_tz": 240
    },
    "id": "5__QnwzFGtMn"
   },
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJtso6kplths"
   },
   "source": [
    "## Section 2 - Mutual information\n",
    "\n",
    "We have our two happy little neurons firing. We've played with the parameters to see how it impacts their relative entropy. Now let's go back to the original parameter settings and see about estimating the mutual information between our neurons.\n",
    "\n",
    "Recall that to estimate the mutual information, we need to estimate the conditional entropy for $X$ on $Y$. For binary variables things are a bit easier because $H(X|Y) = H(Y) - H(X,Y)$. Which means we only need to get both independnet entropies and the joint entropy. This leaves us with this simple and beautiful equation: $I(X; Y) = H(X) + H(Y) - H(X, Y)$.\n",
    "\n",
    "We can easily write a function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1689953959020,
     "user": {
      "displayName": "Timothy Verstynen",
      "userId": "04724296371017749786"
     },
     "user_tz": 240
    },
    "id": "51ELNt49mN9U"
   },
   "outputs": [],
   "source": [
    "def calculate_joint_entropy(neuron1, neuron2):\n",
    "    \"\"\"\n",
    "    Calculate the joint entropy of the spike trains of two neurons.\n",
    "    \"\"\"\n",
    "    # Calculate the probabilities of each combination of 0 and 1\n",
    "    p_00 = np.sum((neuron1 == 0) & (neuron2 == 0)) / len(neuron1)\n",
    "    p_01 = np.sum((neuron1 == 0) & (neuron2 == 1)) / len(neuron1)\n",
    "    p_10 = np.sum((neuron1 == 1) & (neuron2 == 0)) / len(neuron1)\n",
    "    p_11 = np.sum((neuron1 == 1) & (neuron2 == 1)) / len(neuron1)\n",
    "\n",
    "    # Use the formula for entropy of a binary variable\n",
    "    # Note: When p_ij is 0, its contribution to the entropy is 0\n",
    "    joint_entropy = 0\n",
    "    if p_00 > 0:\n",
    "        joint_entropy -= p_00 * np.log2(p_00)\n",
    "    if p_01 > 0:\n",
    "        joint_entropy -= p_01 * np.log2(p_01)\n",
    "    if p_10 > 0:\n",
    "        joint_entropy -= p_10 * np.log2(p_10)\n",
    "    if p_11 > 0:\n",
    "        joint_entropy -= p_11 * np.log2(p_11)\n",
    "\n",
    "    return joint_entropy\n",
    "\n",
    "def calculate_mutual_information(neuron1, neuron2):\n",
    "    \"\"\"\n",
    "    Calculate the mutual information between the spike trains of two neurons.\n",
    "    \"\"\"\n",
    "    entropy_neuron1 = calculate_entropy(neuron1)\n",
    "    entropy_neuron2 = calculate_entropy(neuron2)\n",
    "    joint_entropy = calculate_joint_entropy(neuron1, neuron2)\n",
    "\n",
    "    # Use the formula for mutual information\n",
    "    mutual_information = entropy_neuron1 + entropy_neuron2 - joint_entropy\n",
    "\n",
    "    return mutual_information\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAVMmvD0GtMo"
   },
   "source": [
    "So let's see what the mutal information is when we re-simulate our neurons back at the original parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1689953959020,
     "user": {
      "displayName": "Timothy Verstynen",
      "userId": "04724296371017749786"
     },
     "user_tz": 240
    },
    "id": "8nSxWZzeGtMo",
    "outputId": "895cd0de-8005-413b-f9c8-41c61f463dad"
   },
   "outputs": [],
   "source": [
    "# Time and rates\n",
    "total_time_sec = 5\n",
    "sampling_rate_hz = 1000\n",
    "n_samples = total_time_sec * sampling_rate_hz\n",
    "\n",
    "# Probabilities\n",
    "base_firing_prob = 0.02\n",
    "influence = 0.03\n",
    "noise_level = 0.01\n",
    "\n",
    "# Simulate the two neurons\n",
    "neuron1 = simulate_neuron_spikes(n_samples, base_firing_prob, 0, noise_level)\n",
    "neuron2 = simulate_neuron_spikes(n_samples, base_firing_prob, influence, noise_level, neuron1)\n",
    "\n",
    "\n",
    "# Calculate and print the mutual information\n",
    "mutual_information = calculate_mutual_information(neuron2, neuron1)\n",
    "print(f\"Mutual Information: {mutual_information}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e88i6epuGtMo"
   },
   "source": [
    "This reflects the mutual dependence between the two neurons. You can probably guess what questions we will ask about this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFvD0qrih8Io"
   },
   "source": [
    "### Question 2.1\n",
    "\n",
    "Increase the baseline firing rate parameter, *base_firing_prob*, by a factor of 10 to 0.2. What happens to the mutual information of the neurons? Explain why this effect may or may not occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1689953959020,
     "user": {
      "displayName": "Timothy Verstynen",
      "userId": "04724296371017749786"
     },
     "user_tz": 240
    },
    "id": "Qn76WmvPiOj_"
   },
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duITA8hciZBL"
   },
   "source": [
    "### Question 2.2\n",
    "\n",
    "Put the baseline firing rate parameter back to 0.02. Increase the influence parameter, from neuron 1 to neuron 2, by a factor of 10 to 0.3. What happens to the mutual information of the neurons? Explain why this effect may or may not occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1689953959020,
     "user": {
      "displayName": "Timothy Verstynen",
      "userId": "04724296371017749786"
     },
     "user_tz": 240
    },
    "id": "N4o3G235it2n"
   },
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfUSjU1RGtMp"
   },
   "source": [
    "### Question 2.3\n",
    "\n",
    "Take the last set of simulations that you ran (with the influence set at 0.3) and switch the order of the neurons in the calculation of mutual information. What does or does not happen to the mutual information estimate? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1689953959021,
     "user": {
      "displayName": "Timothy Verstynen",
      "userId": "04724296371017749786"
     },
     "user_tz": 240
    },
    "id": "ZiUjJ2k5GtMp"
   },
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
