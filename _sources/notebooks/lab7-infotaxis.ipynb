{"cells":[{"cell_type":"markdown","metadata":{"id":"sHIeLwWce4tw"},"source":["# Lab 7 - Infotaxis\n","(aka- pure curiosity-driven search)\n","\n","Here we will explore how our little artificial organisms do two things:\n","\n","1. Learn a *concept* of information as a signal. This reflects the curiosity about a signal, as opposed to its magnitude.\n","2. How this concept of information facilitates learning in noisy environments."]},{"cell_type":"markdown","metadata":{"id":"KNr4qUoAfTlH"},"source":["## Background\n","\n","In this lab we return to _taxic explorations_. Recall that in Lab 4 (evidence accumulation) we look at what happens when sense information is not just noisy, but partially observed. In otherwords, when there is distortion in the channel of information.\n","\n","Here we will compare the simple chemotaxic framework, where agents simply follow the gradient of the scent, to an _infotaxic_ framework where agents follow the _information_ carried in the signal instead.\n"]},{"cell_type":"markdown","metadata":{"id":"HvWP2GmYgBhZ"},"source":["\n","Recall that our basic model of valentino exploration is as simple as can be.\n","\n","- When the gradient is positive, meaning you are going \"up\" the gradient, the probability of turning is set to _p pos_.\n","- When the gradient is negative, the turning probability is set to _p neg_. (See code below, for an example).\n","- If the agent \"decides\" to turn, the direction it takes is uniform random.\n","- The length of travel before the next turn decision is sampled from an exponential distribution just like the _DiffusionDiscrete_\n","\n","### Information agents\n","We will study three agents. One who does _chemotaxis_. One who does a kind of _infotaxis_. One that does random search (aka Diffusion). For fun, let's call this one a _randotaxis_ agent. This last rando-agent is really a control. A reference point.\n","\n","In a sense the _chemotaxis_ agent only tries to answer question Q2 (above). While _infotaxis_ only tries to answer Q1. They are extreme strategies, in other words. The bigger question we will ask, in a very limited setting, is which extreme method is better _generally_?\n","\n","\n","### Costly cognition\n","Both _chemo-_ and _infotaxis_ agents will use a simple accumulation-of-evidence process to try and make better decisions about the direction of the gradient. These decisions are of course statistical in nature.\n","\n","Our control agent, _randotaxis_, will be another simple Brownian random walk agent.\n"]},{"cell_type":"markdown","metadata":{"id":"pw41OUVdgxfG"},"source":["## Reviewing the definition of _chemotaxis_:\n","Our _chemotaxis_ agent (_GradientDiffusionGrid_) tries to directly estimate the gradient $\\nabla$ in scent by comparing the level of scent at the last grid position it occupied to the current scent level ($o$). By last grid position here we mean the last grid position when it moved last.\n","\n","$$\\nabla \\approx o_t - o_{t-1}$$\n","\n","For today we're going back to the simple, non-accumulator chemotaxic agent that uses the simple gradient to make a decision about running vs. tumbling.\n","\n","## A definition of _infotaxis_:\n","Compared to chemo- definition the definition of infotaxis is a little more involved. It has what we can think of as five cognitive or behavioral steps:\n","\n","1. Use a simple differntial to estimate whether the gradient is increasing or decreasing.\n","\n","2. Build a probability model of hits(increasing)/misses(decreasing) at every point of the grid.\n","3. Measure information gained when probability model changes. This is quantified using the KL divergence that measures how the properties of the signal are changing.\n","4. Measure the gradient of information gains\n","5. Use the gradient to make turning decisions\n","\n","_Note_: Even though the infotaxis is more complex, it can take advantage of missing scent information to drive its behavior. It can also use positive scent hits, of course, too.\n"]},{"cell_type":"markdown","metadata":{"id":"9GbUZLcmhC87"},"source":["## Section 0 - Setup"]},{"cell_type":"markdown","metadata":{"id":"eAVJYYdhhKSL"},"source":["First let's set things up for the two parts of the lab. You've done this before, so we don't need to specify each installation and module step."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GaM7UFV6yKE1"},"outputs":[],"source":["!pip install --upgrade git+https://github.com/coaxlab/explorationlib\n","!pip install --upgrade git+https://github.com/MattChanTK/gym-maze.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zI0CIEceyQx4"},"outputs":[],"source":["import shutil\n","import glob\n","import os\n","\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from copy import deepcopy\n","\n","import explorationlib\n","from explorationlib.local_gym import ScentGrid\n","\n","from explorationlib.agent import DiffusionGrid\n","from explorationlib.agent import GradientDiffusionGrid\n","from explorationlib.agent import GradientInfoGrid\n","\n","from explorationlib.run import experiment\n","from explorationlib.util import select_exp\n","from explorationlib.util import load\n","from explorationlib.util import save\n","\n","from explorationlib.local_gym import uniform_targets\n","from explorationlib.local_gym import constant_values\n","from explorationlib.local_gym import ScentGrid\n","from explorationlib.local_gym import create_grid_scent\n","from explorationlib.local_gym import add_noise\n","from explorationlib.local_gym import create_grid_scent_patches\n","\n","from explorationlib.plot import plot_position2d\n","from explorationlib.plot import plot_length_hist\n","from explorationlib.plot import plot_length\n","from explorationlib.plot import plot_targets2d\n","from explorationlib.plot import plot_scent_grid\n","from explorationlib.plot import plot_targets2d\n","\n","from explorationlib.score import total_reward\n","from explorationlib.score import num_death"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KQbjVvs6p1Zz"},"outputs":[],"source":["# Pretty plots\n","%matplotlib inline\n","%config InlineBackend.figure_format='retina'\n","%config IPCompleter.greedy=True\n","plt.rcParams[\"axes.facecolor\"] = \"white\"\n","plt.rcParams[\"figure.facecolor\"] = \"white\"\n","plt.rcParams[\"font.size\"] = \"16\"\n","\n","# Dev\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"JnKyN9oM0GTm"},"source":["## Section 1 - Chemotaxis vs. curiosity"]},{"cell_type":"markdown","metadata":{"id":"IpB4JhugmNJE"},"source":["In this section we take on accumulating evidence as a policy for decision making. Our venue is still chemotaxis, but now our sensors are noisy. The presence of this uncertainty makes decisions--of the kind common to decision theory--a necessity.\n","\n","Let's see just how helpful the concept of information for chemotaxic search can be."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"StauX74y0QjT"},"outputs":[],"source":["# Noise and missing scents\n","p_scent = 0.5\n","noise_sigma = 1\n","\n","# Shared\n","num_experiments = 100\n","num_steps = 500\n","seed_value = 5838\n","\n","# ! (leave alone)\n","detection_radius = 1\n","max_steps = 1\n","min_length = 1\n","num_targets = 50\n","target_boundary = (10, 10)\n","\n","# Targets\n","prng = np.random.RandomState(seed_value)\n","targets = uniform_targets(num_targets, target_boundary, prng=prng)\n","values = constant_values(targets, 1)\n","\n","# Scents\n","scents = []\n","for _ in range(len(targets)):\n","    coord, scent = create_grid_scent_patches(\n","        target_boundary, p=1.0, amplitude=1, sigma=2)\n","    scents.append(scent)\n","\n","# Env\n","env = ScentGrid(mode=None)\n","env.seed(seed_value)\n","env.add_scents(targets, values, coord, scents, noise_sigma=noise_sigma)"]},{"cell_type":"markdown","metadata":{"id":"ip-6AMyMmzjk"},"source":["Again we are working a scent grid environment where each target emits noisy chemical signals (scents) according to our definitions above.\n","\n","Here's an example of our environment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4xKn5Oog3HB8"},"outputs":[],"source":["plot_boundary = (10, 10)\n","num_experiment = 0\n","ax = None\n","ax = plot_targets2d(\n","    env,\n","    boundary=plot_boundary,\n","    color=\"black\",\n","    alpha=1,\n","    label=\"Targets\",\n","    ax=ax,\n",")"]},{"cell_type":"markdown","metadata":{"id":"4SRoMBksnAML"},"source":["We will use 3 agents in these sims:\n","\n","- Rando: Uses random Brownian motion search.\n","- Chemo: Uses only the detected scent gradient to make a decision.\n","- Info: Estimates how much *information* is encoded in the scent signal to make a decision."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hvKupKt10V5s"},"outputs":[],"source":["# Agents\n","\n","# Random search agent\n","diff = DiffusionGrid(min_length=min_length)\n","diff.seed(seed_value)\n","\n","# Run & tumble information\n","min_length = 1 # Minimum step length on the grid\n","p_neg = 0.8 # Probability of jumping if gradient/information is decreasing\n","p_pos = 0.2 # Probabilty of jumping if gradient/information is increasing\n","\n","# Chemotaxis agent\n","chemo = GradientDiffusionGrid(\n","    min_length=min_length,\n","    p_neg=p_neg,\n","    p_pos=p_pos,\n",")\n","chemo.seed(seed_value)\n","\n","\n","# Infotaxis agent\n","threshold = 0.05 # Threshold for information gain\n","info = GradientInfoGrid(\n","    min_length=min_length,\n","    p_neg=p_neg,\n","    p_pos=p_pos,\n","    threshold=threshold\n",")\n","info.seed(seed_value)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"awCEAAyGngBm"},"source":["Now let's run the experiments."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rVTGtSPT0fN_"},"outputs":[],"source":["# Experiments\n","rand_exp = experiment(\n","    f\"rand\",\n","    diff,\n","    env,\n","    num_steps=num_steps,\n","    num_experiments=num_experiments,\n","    dump=False,\n","    split_state=True,\n","    seed=seed_value\n",")\n","chemo_exp = experiment(\n","    f\"chemo\",\n","    chemo,\n","    env,\n","    num_steps=num_steps,\n","    num_experiments=num_experiments,\n","    dump=False,\n","    split_state=True,\n","    seed=seed_value\n",")\n","info_exp = experiment(\n","    f\"info\",\n","    info,\n","    env,\n","    num_steps=num_steps,\n","    num_experiments=num_experiments,\n","    dump=False,\n","    split_state=True,\n","    seed=seed_value\n",")"]},{"cell_type":"markdown","metadata":{"id":"aNn5xm5nno3t"},"source":["Let's plot an example experiment. Here I'm choosing the sixth run for each agent."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-1x_tV4C0jMS"},"outputs":[],"source":["plot_boundary = (10, 10)\n","\n","# -\n","num_experiment = 5\n","ax = None\n","\n","ax = plot_position2d(\n","    select_exp(rand_exp, num_experiment),\n","    boundary=plot_boundary,\n","    label=\"Rando\",\n","    color=\"red\",\n","    alpha=0.8,\n","    ax=ax,\n",")\n","ax = plot_position2d(\n","    select_exp(chemo_exp, num_experiment),\n","    boundary=plot_boundary,\n","    label=\"Chemo\",\n","    color=\"blue\",\n","    alpha=0.6,\n","    ax=ax,\n",")\n","ax = plot_position2d(\n","    select_exp(info_exp, num_experiment),\n","    boundary=plot_boundary,\n","    label=\"Info\",\n","    color=\"green\",\n","    alpha=0.6,\n","    ax=ax,\n",")\n","ax = plot_targets2d(\n","    env,\n","    boundary=plot_boundary,\n","    color=\"black\",\n","    alpha=1,\n","    label=\"Targets\",\n","    ax=ax,\n",")"]},{"cell_type":"markdown","metadata":{"id":"Sw96O-9pnzTb"},"source":["Hard to distinguish their individual behaviors, but our agents seem to be exploring.\n","\n","Now let's evaluate some metrics of performance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ICBnDBbep1Z7"},"outputs":[],"source":["# Results\n","results = [rand_exp, chemo_exp, info_exp]\n","names = [\"Rando\", \"Chemo\", \"Info\"]\n","colors = [\"red\", \"blue\", \"green\"]\n","\n","# Score by eff\n","scores = []\n","for name, res, color in zip(names, results, colors):\n","    l = 0.0\n","    for r in res:\n","        l += np.sum(r[\"agent_num_step\"])\n","    scores.append(l)\n","\n","# Tabulate\n","m, sd = [], []\n","for (name, s, c) in zip(names, scores, colors):\n","    m.append(np.mean(s))\n","    sd.append(np.std(s))\n","\n","# Plot means\n","fig = plt.figure(figsize=(4, 3))\n","plt.bar(names, m, yerr=sd, color=\"black\", alpha=0.6)\n","plt.ylabel(\"Total run distance\")\n","plt.tight_layout()\n","sns.despine()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yPaHH65P2EP8"},"outputs":[],"source":["# Score by eff\n","scores = []\n","for name, res, color in zip(names, results, colors):\n","    scores.append(num_death(res))\n","\n","# Tabulate\n","m, sd = [], []\n","for (name, s, c) in zip(names, scores, colors):\n","    m.append(np.mean(s))\n","    sd.append(np.std(s))\n","\n","# Plot means\n","fig = plt.figure(figsize=(4, 3))\n","plt.bar(names, m, yerr=sd, color=\"black\", alpha=0.6)\n","plt.ylabel(\"Deaths\")\n","plt.tight_layout()\n","sns.despine()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Kl9LjUL2IMK"},"outputs":[],"source":["# Score by eff\n","scores = []\n","for name, res, color in zip(names, results, colors):\n","    r = total_reward(res)\n","    scores.append(r)\n","\n","# Tabulate\n","m, sd = [], []\n","for (name, s, c) in zip(names, scores, colors):\n","    m.append(np.mean(s))\n","    sd.append(np.std(s))\n","\n","# Plot means\n","fig = plt.figure(figsize=(5, 4))\n","plt.bar(names, m, yerr=sd, color=\"black\", alpha=0.6)\n","plt.ylabel(\"Total reward\")\n","plt.tight_layout()\n","sns.despine()\n","\n","# Dists\n","fig = plt.figure(figsize=(5, 4))\n","for (name, s, c) in zip(names, scores, colors):\n","    plt.hist(s, label=name, color=c, alpha=0.5, bins=np.linspace(0, np.max(scores), 50))\n","    plt.legend()\n","    plt.xlabel(\"Score\")\n","    plt.tight_layout()\n","    sns.despine()"]},{"cell_type":"markdown","metadata":{"id":"n1-KWIgnn_Rp"},"source":["---\n","### Question 1.1\n","\n","How does each of our agents perform across the performance measures we have chosen?"]},{"cell_type":"markdown","source":["__Answer:__\n","\n","_(insert response here)_"],"metadata":{"id":"sgnB1ti6Wckg"}},{"cell_type":"markdown","metadata":{"id":"0Jyz-sNYoTe6"},"source":["---\n","### Question 1.2\n","\n","Is having a concept of information (i.e., the Info agent) helpful in these sorts of noisy environments? Why or why not based on how the agents performed? Compare to both Rando and Chemo."]},{"cell_type":"markdown","source":["__Answer:__\n","\n","_(insert response here)_"],"metadata":{"id":"11z6p3SAWe29"}},{"cell_type":"markdown","metadata":{"id":"7Do4kPlRpBtV"},"source":["## Section 2 - Robustness of information searching"]},{"cell_type":"markdown","metadata":{"id":"PTTuDJx9pOfI"},"source":["You'll notice that the infotaxis agent has an additional parameter over the simple chemotaxis agent, which is it's threshold. This is needed because, unlike the scent gradient, the information gain (i.e., KL-divergence) gradient is bounded at zero. This determines how much information gain is necessary to trigger a shift.\n","\n","Here we will test a range of *threshold* values (within the range of values our infotaxis agent is likely to see) and look at the change in performance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TkSIg5Wt-WOo"},"outputs":[],"source":["# Our parameters\n","thresholds = [0.01, 0.025, .05, .075, .09]\n","\n","# For plotting\n","colors = [\"darkgreen\", \"seagreen\", \"cadetblue\", \"steelblue\", \"mediumpurple\"]\n","names = thresholds"]},{"cell_type":"markdown","metadata":{"id":"IfRStyhvqCnS"},"source":["Let's run these experiments. All of the parameters for the agent and environment (aside from *p_scent*) are specified below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uJm1Y-21_Fn_"},"outputs":[],"source":["# Define non-scent probability values\n","noise_sigma = 1\n","amplitude = 1\n","detection_radius = 1\n","max_steps = 1\n","min_length = 1\n","num_targets = 50\n","target_boundary = (10, 10)\n","\n","# Define targets\n","prng = np.random.RandomState(seed_value)\n","targets = uniform_targets(num_targets, target_boundary, prng=prng)\n","values = constant_values(targets, 1)\n","\n","# Scents\n","scents = []\n","for _ in range(len(targets)):\n","    coord, scent = create_grid_scent_patches(\n","        target_boundary, p=p_scent, amplitude=amplitude, sigma=noise_sigma)\n","    scents.append(scent)\n","\n","# Env\n","env = ScentGrid(mode=None)\n","env.seed(seed_value)\n","env.add_scents(targets, values, coord, scents, noise_sigma=noise_sigma)\n","\n","# How many experiments to run\n","num_experiments = 100\n","\n","# Infotaxis agent\n","min_length = 1\n","p_neg = 0.8\n","p_pos = 0.2\n","\n","# Run\n","results = []\n","\n","for threshold in thresholds:\n","\n","  # Setup our agent with a new threshold\n","  info = GradientInfoGrid(\n","      min_length=min_length,\n","      p_neg=p_neg,\n","      p_pos=p_pos,\n","      threshold=threshold\n","  )\n","  info.seed(seed_value)\n","\n","  # Run the experiment\n","  exp = experiment(\n","    f\"info\",\n","    info,\n","    env,\n","    num_steps=num_steps,\n","    num_experiments=num_experiments,\n","    dump=False,\n","    split_state=True,\n","    seed=seed_value\n","  )\n","  results.append(exp)\n"]},{"cell_type":"markdown","metadata":{"id":"-EFjD_f3qRkk"},"source":["Now let us take a look at the performance. First let's see how the average information gain changes across runs of each experiment (i.e., threshold type). Remember, this is our first agent with a spatial memory and it carries over across runs. So we should see a decreasing trend as the agent learns the environment more."]},{"cell_type":"code","source":["# Score\n","scores = []\n","for result in results:\n","    b = []\n","    for r in result:\n","        b.append(np.mean(r[\"agent_info_gain\"]))\n","\n","    scores.append(b)\n","\n","steps = np.reshape(scores, (len(thresholds), num_experiments))\n","\n","# Create the x values for the plot\n","x = np.arange(len(names))\n","\n","# Plotting each row of the array as a separate line\n","for i in range(len(steps)):\n","    plt.plot(steps[i], marker='o', label=f'{thresholds[i]}')\n","\n","# Add legend\n","plt.legend()\n","\n","plt.ylabel(\"Mean Info Gain (E)\")\n","plt.xlabel(\"Experiment\")\n","plt.tight_layout()\n","sns.despine()"],"metadata":{"id":"K7ToSY2fpjFO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next we will look at the average performance on a variety of measures as we vary the threshold."],"metadata":{"id":"UBxqQLr0Q_hb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4U5dm66iBDk7"},"outputs":[],"source":["# Score\n","scores = []\n","for result in results:\n","    l = 0.0\n","    for r in result:\n","        l += max(r[\"agent_num_turn\"])\n","    l = l / len(result)\n","    scores.append(l)\n","\n","# Tabulate\n","m, sd = [], []\n","for s in scores:\n","    m.append(np.mean(s))\n","\n","# -\n","fig = plt.figure(figsize=(5, 5))\n","plt.bar([str(n) for n in names], m, color=\"black\", alpha=0.6)\n","plt.ylabel(\"Number of turns\")\n","plt.xlabel(\"Threshold\")\n","plt.tight_layout()\n","sns.despine()"]},{"cell_type":"code","source":["# Score\n","scores = []\n","for result in results:\n","    l = 0.0\n","    for r in result:\n","        l += np.sum(r[\"agent_num_step\"])\n","    l = l / len(result)\n","    scores.append(l)\n","\n","# Tabulate\n","m, sd = [], []\n","for s in scores:\n","    m.append(np.mean(s))\n","\n","# -\n","fig = plt.figure(figsize=(5, 5))\n","plt.bar([str(n) for n in names], m, color=\"black\", alpha=0.6)\n","plt.ylabel(\"Total run distance\")\n","plt.xlabel(\"Threshold\")\n","plt.tight_layout()\n","sns.despine()"],"metadata":{"id":"zL2gLXbLKMZn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mqNtWAC6EIRg"},"outputs":[],"source":["# Score\n","scores = []\n","for result in results:\n","    scores.append(num_death(result))\n","\n","# -\n","fig = plt.figure(figsize=(5, 5))\n","plt.bar([str(n) for n in names], scores, color=\"black\", alpha=0.6)\n","plt.ylabel(\"Deaths\")\n","plt.xlabel(\"Threshold\")\n","plt.tight_layout()\n","sns.despine()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PkhiHHGcEIuN"},"outputs":[],"source":["# Max Score\n","scores = []\n","for result in results:\n","    r = total_reward(result)\n","    scores.append(r)\n","\n","# Tabulate\n","m = []\n","for s in scores:\n","    m.append(np.max(s))\n","\n","# -\n","fig = plt.figure(figsize=(5, 5))\n","plt.bar([str(n) for n in names], m, color=\"black\", alpha=0.6)\n","plt.ylabel(\"Best agent's score\")\n","plt.xlabel(\"Threshold\")\n","plt.tight_layout()\n","sns.despine()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EIhxp7iREQwX"},"outputs":[],"source":["# Score\n","scores = []\n","for result in results:\n","    r = total_reward(result)\n","    scores.append(r)\n","\n","# Tabulate\n","m, sd = [], []\n","for s in scores:\n","    m.append(np.mean(s))\n","    sd.append(np.std(s))\n","\n","# Plot means\n","fig = plt.figure(figsize=(6, 3))\n","plt.bar([str(n) for n in names], m, yerr=sd, color=\"black\", alpha=0.6)\n","plt.ylabel(\"Avg. score\")\n","plt.xlabel(\"Threshold\")\n","plt.tight_layout()\n","sns.despine()\n","\n","# Dists of means\n","fig = plt.figure(figsize=(6, 3))\n","for (i, s, c) in zip(names, scores, colors):\n","    plt.hist(s, label=i, color=c, alpha=0.5, bins=list(range(1,50,1)))\n","    plt.legend()\n","    plt.ylabel(\"Frequency\")\n","    plt.xlabel(\"Threshold\")\n","    plt.tight_layout()\n","    sns.despine()"]},{"cell_type":"markdown","metadata":{"id":"GiCg65RaqaIx"},"source":["---\n","### Question 2.1\n","\n","How does increasing the *threshold* impact our Info agent's performance? Explain why this particular pattern emerges in the results."]},{"cell_type":"markdown","source":["__Answer:__\n","\n","_(insert response here)_"],"metadata":{"id":"tnP2yyz4WhsG"}},{"cell_type":"markdown","metadata":{"id":"1fiRzusWqxzh"},"source":["---\n","### Question 2.2\n","\n","Take the results from the two sections together. Compare the infotaxis agent with the random search or chemotaxis agent. What does this tell you about the nature of curious search?"]},{"cell_type":"markdown","source":["__Answer:__\n","\n","_(insert response here)_"],"metadata":{"id":"l0vQdogxWjto"}},{"cell_type":"markdown","source":["\n","---\n","**IMPORTANT** Did you collaborate with anyone on this assignment, or use LLMs like ChatGPT? If so, list their names here.\n","> *Write Name(s) here*"],"metadata":{"id":"BZGFfBysJlEP"}}],"metadata":{"colab":{"provenance":[{"file_id":"1hdCFluV2m1u69HwplWMpPkcKqTDHCyK_","timestamp":1723108213843}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}