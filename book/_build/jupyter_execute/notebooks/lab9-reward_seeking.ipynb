{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUSvmC5RuNU7"
   },
   "source": [
    "# Lab 9 - Reward seeking\n",
    "\n",
    "This lab has 3 main components designed to give you an interactive understanding of core reinforcement learning concepts and the $\\epsilon$-greedy reinforcement learning algorithm.\n",
    "\n",
    "Sections:\n",
    "0. Background on essential reinforcement concepts that we will be engagning with hands-on.\n",
    "1. Investigating random and $\\epsilon$-greedy algorithms in a simple bandit task.\n",
    "2. Seeing how this sort of policy works in our foraging search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRTZMraJwvUh"
   },
   "source": [
    "## Background\n",
    "\n",
    "### The bandit task\n",
    "\n",
    "In this assignment we study exploration in the very abstract $k$-armed bandit task.\n",
    "- In this there are $k$ actions to take.\n",
    "- Each returns a reward $R$, with some probability $p$.\n",
    "- The reward value is either a 1 or a 0.\n",
    "- This means the expected value of each arm is simply probability. Nice and simple right?\n",
    "\n",
    "### Action-value learning\n",
    "\n",
    "Our agents are really learning, at last. Reinforcement Learning (RL), to be precise.\n",
    "\n",
    "The reward value $Q$ update rule for all agents (below) and arm is the same:\n",
    "\n",
    "$ Q \\leftarrow Q + \\alpha * (R - Q) $ [1]\n",
    "\n",
    "Where the learning rate on the reward prediction error ($R-Q$) is denoted as $\\alpha$, so that the equation above looks nice. If you are not familiar with the idea of a learning rate, it is what it sounds like. A parameter that controls how much each value update matters. This is, over time, the rate at which learning happens.\n",
    "\n",
    "$Q$ is trying to approximate the average reward value of each arm.\n",
    "\n",
    "- This kind of difference $(R - Q)$, the reward prediction error (RPE), in Eq [1] is the most typical error signal used for learnin gin RL.\n",
    "- If you're not sure what it means, consider in your head, what would happen to the value update if $Q$ was bigger than the reward $R$ (and overestimate), or if it was smaller.\n",
    "\n",
    "Once you have noodled that a bit, as needed, consider how making $\\alpha$ bigger or smaller might make $Q$ learning faster, or slower, or more or less volatile. (Learning speed and volatility _often_ go together; an annoying matched set.)\n",
    "\n",
    "_Note_: We are not going to really play with $\\alpha$ here. Just giving you some intuition.\n",
    "\n",
    "### Basic exploration strategies\n",
    "\n",
    "Our exploration strategies are a random one, a sequential one, or $\\epsilon$-greedy (aka 'e'-greedy).\n",
    "\n",
    "The $\\epsilon$-greedy method is not the best known solution to trading off exploration with exploitation. Then again, it is widely used to this day. It's a place to start.\n",
    "\n",
    "Our metric is _total reward_. Maximizing that is the goal of all RL, afterall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAQMapBwwx2b"
   },
   "source": [
    "## Section - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "toAbP_djwvUk"
   },
   "outputs": [],
   "source": [
    "# Install explorationlib?\n",
    "!pip install --upgrade git+https://github.com/coaxlab/explorationlib\n",
    "!pip install --upgrade git+https://github.com/MattChanTK/gym-maze.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8n86JdtMwvUl"
   },
   "outputs": [],
   "source": [
    "# Import misc\n",
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Exploration lib\n",
    "import explorationlib\n",
    "\n",
    "# Agents\n",
    "from explorationlib.agent import BanditActorCritic\n",
    "from explorationlib.agent import Critic\n",
    "from explorationlib.agent import EpsilonActor\n",
    "from explorationlib.agent import RandomActor\n",
    "from explorationlib.agent import SequentialActor\n",
    "from explorationlib.agent import BoundedRandomActor\n",
    "from explorationlib.agent import BoundedSequentialActor\n",
    "from explorationlib.agent import WSLSGrid\n",
    "from explorationlib.agent import CriticGrid\n",
    "from explorationlib.agent import SoftmaxActor\n",
    "from explorationlib.agent import ActorCriticGrid\n",
    "from explorationlib.agent import DiffusionGrid\n",
    "\n",
    "# Exp\n",
    "from explorationlib.run import experiment\n",
    "from explorationlib.score import total_reward\n",
    "from explorationlib.score import action_entropy\n",
    "from explorationlib.util import select_exp\n",
    "from explorationlib.util import load\n",
    "from explorationlib.util import save\n",
    "\n",
    "# Vis\n",
    "from explorationlib.plot import plot_bandit\n",
    "from explorationlib.plot import plot_bandit_actions\n",
    "from explorationlib.plot import plot_bandit_critic\n",
    "from explorationlib.plot import plot_bandit_hist\n",
    "from explorationlib.plot import plot_position2d\n",
    "from explorationlib.plot import plot_length_hist\n",
    "from explorationlib.plot import plot_length\n",
    "from explorationlib.plot import plot_targets2d\n",
    "from explorationlib.plot import plot_scent_grid\n",
    "from explorationlib.plot import plot_targets2d\n",
    "\n",
    "# Score\n",
    "from explorationlib.score import total_reward\n",
    "from explorationlib.score import num_death\n",
    "from explorationlib.score import on_off_patch_time\n",
    "\n",
    "# Env\n",
    "from explorationlib.local_gym import BanditUniform4\n",
    "from explorationlib.local_gym import ScentGrid\n",
    "from explorationlib.local_gym import create_grid_scent\n",
    "from explorationlib.local_gym import create_grid_scent_patches\n",
    "from explorationlib.local_gym import uniform_targets\n",
    "from explorationlib.local_gym import uniform_patch_targets\n",
    "from explorationlib.local_gym import constant_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwT-kGsQwvUm"
   },
   "outputs": [],
   "source": [
    "# Pretty plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "%config IPCompleter.greedy=True\n",
    "plt.rcParams[\"axes.facecolor\"] = \"white\"\n",
    "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "plt.rcParams[\"font.size\"] = \"16\"\n",
    "\n",
    "# Dev\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfROgMnJwvUm"
   },
   "source": [
    "## Section 1 - The bandit task\n",
    "\n",
    "In this section we'll study three explorers getting to know one bandit, with four arms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owAFhjOXwvUm"
   },
   "source": [
    "### Creating a bandit task\n",
    "\n",
    "Let's make a 4-armed bandit and then plot its values. (Expected value is the term used in the literature, so we use it here).\n",
    "\n",
    "_Note_: The random seed is fixed. If you change the see and run the cell below, some of the reward probabilities will change. The probability of the best arm, the optimal value arm is fixed however. It is set to 0.35, and located at arm 2. Try it! Rerun the cell below with different seeds, a few times, to get a sense of how the non-optimal arms can vary. When you are done, return to the orginal seed value and re-run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldh9g0gkwvUm"
   },
   "outputs": [],
   "source": [
    "# Shared env params\n",
    "num_experiments = 1\n",
    "seed = 5030 # originally 5030\n",
    "\n",
    "# Create env\n",
    "env = BanditUniform4(p_min=0.1, p_max=0.3, p_best=0.35)\n",
    "env.seed(seed)\n",
    "\n",
    "# Plot env\n",
    "plot_bandit(env, alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjZ_XWsz2b0i"
   },
   "source": [
    "### Question 1.1\n",
    "\n",
    "Given the reward probabilities (expected values) in this bandit task, how \"easy\" or \"difficult\" do you think this task would be to learn from a simple update rule like we showed above? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DR0_mdNC3dxN"
   },
   "outputs": [],
   "source": [
    "# Write your answer here, as a comment. Explain yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffNrJY8rwvUm"
   },
   "source": [
    "### Our three agents\n",
    "\n",
    "A word about the code. Our agents this week work in what gets called an ActorCritic desgin. This breaks reinforcement learning algorithms into two parts: the Actor does action selection, and the Critic estimates the value of each action.\n",
    "\n",
    "Now in normal reinforcement learning, aka not pure exploration, the _Actor_ uses the $Q$ value estimates from the _Critic_ to, in part, make its decisions. Be it explore or exploit. This is indeed the case for how the $\\epsilon$-greedy agent, _EpsilonActor_, works.\n",
    "\n",
    "...But...\n",
    "\n",
    "The other two agents--_SequentialActor_ and _RandomActor_--don't explore with value. The are both _max entropy_ action systems, who don't care about reward value or learning _at all_. The _ActorCritic_ style is reused because it was easy to implement in _explorationlib_. Don't be misled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sOdwj-xwvUn"
   },
   "outputs": [],
   "source": [
    "# Creating the three agents\n",
    "\n",
    "ran = BanditActorCritic(\n",
    "    RandomActor(num_actions=env.num_arms),\n",
    "    Critic(num_inputs=env.num_arms, default_value=0.0)\n",
    ")\n",
    "seq = BanditActorCritic(\n",
    "    SequentialActor(num_actions=env.num_arms),\n",
    "    Critic(num_inputs=env.num_arms, default_value=0.0)\n",
    ")\n",
    "epy = BanditActorCritic(\n",
    "    EpsilonActor(num_actions=env.num_arms, epsilon=0.1),\n",
    "    Critic(num_inputs=env.num_arms, default_value=0.0)\n",
    ")\n",
    "\n",
    "# -\n",
    "agents = [ran, seq, epy]\n",
    "names = [\"random\", \"sequential\", \"ep-greedy\"]\n",
    "colors = [\"blue\", \"green\", \"purple\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TF1NseL9wvUn"
   },
   "source": [
    "Let's run out our three agents on the _env_ (the bandit task we built), and make some plots to visualize what each agent is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gf3a1e_qwvUn"
   },
   "outputs": [],
   "source": [
    "num_steps = 12  # Number of choices each agent gets to make, (around 3 per arm)\n",
    "\n",
    "# !\n",
    "results = []\n",
    "for name, agent in zip(names, agents):\n",
    "    log = experiment(\n",
    "        f\"{name}\",\n",
    "        agent,\n",
    "        env,\n",
    "        num_steps=num_steps,\n",
    "        num_experiments=num_experiments,\n",
    "        dump=False,\n",
    "        split_state=False,\n",
    "    )\n",
    "    results.append(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPy-QdJuwvUn"
   },
   "source": [
    "#### Plot action choices\n",
    "with time (aka steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCHcNGuvwvUn"
   },
   "outputs": [],
   "source": [
    "num_experiment = 0\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    plot_bandit_actions(\n",
    "        select_exp(res, num_experiment),\n",
    "        num_arms=4,\n",
    "        s=4,\n",
    "        title=name,\n",
    "        color=color,\n",
    "        figsize=(2, 1.5)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EjrCSFr4Etr"
   },
   "source": [
    "### Question 1.2\n",
    "\n",
    "Describe the coice behavior of each agent type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PpkTOv6I4OES"
   },
   "outputs": [],
   "source": [
    "# Write your answer here, as a comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q42JrbQg4Qhn"
   },
   "source": [
    "### Question 1.3\n",
    "\n",
    "Re-run the simulations above a few times. What range of choice patterns does epsilon-greedy agent exhibit? What do you think leads to these patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GT5YNP174gz1"
   },
   "outputs": [],
   "source": [
    "# Write your answer here, as a comment. Explain yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVpmoA9ZG_8M"
   },
   "source": [
    "### Question 1.4\n",
    "\n",
    "Now change the 系 value for the 系-greedy agent to $0.01$ and re-run the simulations a few times. How does the behavior of this agent change? What does this tell you about the utility of greediness in this particular form of the bandit task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hcmle6PcHYvS"
   },
   "outputs": [],
   "source": [
    "# Write your answer here, as a comment. Explain yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlvfeC_xwvUo"
   },
   "source": [
    "#### Histograms of action probability (aka arm choice).\n",
    "\n",
    "_Fun fact_: The flatter these plots are, the closer they are to what is called _maximum entropy_ exploration behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ed_pxA31wvUo"
   },
   "outputs": [],
   "source": [
    "num_experiment = 0\n",
    "ax = None\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    ent = np.round(np.mean(action_entropy(res)), 2)\n",
    "    plot_bandit_hist(\n",
    "        select_exp(res, num_experiment),\n",
    "        bins=list(range(0, 5)),\n",
    "        title=f\"{name}\", # (ent {ent})\",\n",
    "        alpha=0.4,\n",
    "        color=color,\n",
    "        figsize=(3, 3),\n",
    "        ax=ax\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOYFZdvQ6ESg"
   },
   "source": [
    "## Section 2 - Investigating the epsilon-greedy algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoBNZ7cOwvUq"
   },
   "source": [
    "#### Meet our dilemma\n",
    "I've been learning Q-value estimates.\n",
    "* Should I explore (keep sampling the options to get more data points to update Q-value estimates)?\n",
    "* Should I exploit (choose the action whose Q value estimate is currently the greatest)?\n",
    "\n",
    "*A simple strategy:*\n",
    "\n",
    "....I'll flip a weight coin,\n",
    "\n",
    "...who's weight has a name. It's $\\epsilon$!\n",
    "\n",
    "The smaller $\\epsilon$ is, the less likely the coin flip comes up \"EXPLORE''. The more likely it comes up on the \"EXPLOIT\" side. If one chooses the exploit side, one is being greedy, right? The bigger $\\epsilon$ the more likely the coin will say \"EXPLORE ''. Etc.\n",
    "\n",
    "Let's play with $\\epsilon$-greedy, on our base case bandit task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEhndJTY7xf5"
   },
   "source": [
    "### Question 2.1\n",
    "\n",
    "We will run three differnt epsilon-greedy agents, each with a different epsilon value (0.05, 0.5, and 0.95). What do you expect each agent's stepwise behavior to \"look like\"? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kItnsV1E85YQ"
   },
   "outputs": [],
   "source": [
    "# Write your answer here, as a comment. Explain yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUfRwZ3UwvUq"
   },
   "outputs": [],
   "source": [
    "num_steps = 4 * 100\n",
    "epsilons = [0.05, 0.5, 0.95]\n",
    "\n",
    "names = [str(epsilon) for epsilon in epsilons]\n",
    "colors = [\"mediumpurple\", \"mediumorchid\", \"mediumvioletred\"]\n",
    "\n",
    "# !\n",
    "results = []\n",
    "for i, (name, epsilon) in enumerate(zip(names, epsilons)):\n",
    "    agent = BanditActorCritic(\n",
    "        EpsilonActor(num_actions=env.num_arms, epsilon=epsilon),\n",
    "        Critic(num_inputs=env.num_arms, default_value=0.0)\n",
    "    )\n",
    "    log = experiment(\n",
    "        f\"ep_{name}\",\n",
    "        agent,\n",
    "        env,\n",
    "        num_steps=num_steps,\n",
    "        num_experiments=100,\n",
    "        dump=False,\n",
    "        split_state=False,\n",
    "    )\n",
    "    results.append(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlbA6-w7wvUq"
   },
   "source": [
    "Example behavior visualizations below. Change _num experiment_ to see more examples (0, 99).\n",
    "\n",
    "_Note_: in every experiment we run in this lab, the optimal value arm is _always_ arm 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INl8PhkIwvUq"
   },
   "outputs": [],
   "source": [
    "num_experiment = 3\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    plot_bandit_actions(\n",
    "        select_exp(res, num_experiment),\n",
    "        max_steps=200,\n",
    "        s=4,\n",
    "        title=\"epsilon = \" + name,\n",
    "        color=color,\n",
    "        figsize=(3, 1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oVDLY4d9AQG"
   },
   "source": [
    "### Question 2.2\n",
    "\n",
    "Did the behavior match what you expected? If not, describe the actual behavior and explain what you think is behind it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ApCfTqQC9zO_"
   },
   "outputs": [],
   "source": [
    "# Write your answer here, as a comment. Explain yourself (if necessary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33LJ1A_donZ9"
   },
   "source": [
    "### Question 2.3\n",
    "\n",
    "Re-run the stepwise choice behavior visualization at different `num_experiment` settings to view what happens in different experiments. Find a case when epsilon=0.05 agent didn't select the best arm (Arm 2) very much. How do you think this happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3HCCSdJ--ZO"
   },
   "outputs": [],
   "source": [
    "# Write your answer here, as a comment. Explain yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DenCGlj_L3z"
   },
   "source": [
    "### Question 2.4\n",
    "\n",
    "We will visualize the average total reward for each agent next. Which agent do you think will collect the most reward and which one the least? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wOu0dMDh_he4"
   },
   "outputs": [],
   "source": [
    "# Write your answer here, as a comment. Explain yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-os4akOs_kYm"
   },
   "source": [
    "### Question 2.5\n",
    "\n",
    "Which agent do you think will exhibit the most variance in total reward across experiments? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O1ZV1DiL_3Dw"
   },
   "outputs": [],
   "source": [
    "# Write your answer here, as a comment. Explain yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOjKKMBHwvUr"
   },
   "source": [
    "#### Visualize total reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Um4uiocCwvUr"
   },
   "outputs": [],
   "source": [
    "# Score\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    r = total_reward(res)\n",
    "    scores.append(r)\n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.8)\n",
    "plt.ylabel(\"Total reward\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9OyVujF__Zi"
   },
   "source": [
    "### Question 2.6\n",
    "\n",
    "Were your predictions in questions 2.4 and 2.5 correct? If not, describe the actual results and explain what you think led to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Y0S_JFO_82E"
   },
   "outputs": [],
   "source": [
    "# Write your answer here, as a comment. Explain yourself (if necessary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hu4nbPHSAcRm"
   },
   "source": [
    "#### Visualizing a histogram of average choice reward across experiments\n",
    "\n",
    "To get average choice reward for each experiment, we divide the total reward for the experiment by the number of steps (choices) taken to collect that total reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p1EpR9H8wvUr"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 3))\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    plt.hist(np.array(s)/num_steps, label=name, color=c, alpha=0.8, bins=np.linspace(0, 0.35, 50))\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Average choice reward\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqDghdsEpMzn"
   },
   "source": [
    "### Question 2.7\n",
    "\n",
    "If instead of 400 steps in each experiment we let each agent run to near infinity, what would the above histogram look like and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8M8LM98-CaMZ"
   },
   "outputs": [],
   "source": [
    "# Write your answer here, as a comment. Explain yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqrNOWdCwvUy"
   },
   "source": [
    "### Question 2.8\n",
    "\n",
    "Based on what you've seen here today, if you were to follow an 系-greedy policy for choices in your own life, what value of 系 would you choose? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZJzkvMmwvUy"
   },
   "outputs": [],
   "source": [
    "# Write your answer here, as a comment. Explain yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhIN6RfLhbd9"
   },
   "source": [
    "## Section 3 - Reward driven search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDCMt4b6hfJY"
   },
   "source": [
    "Now we are going to create an agent in our foraging environment that implements reward driven search using the exact same approaches we have described above. We shall call this process _rewardtaxis_.\n",
    "\n",
    "This is a Q-learning agent that uses softmax exploration policiy, as opposed to an $\\epsilon$-greedy policy. It does not use teh scent, but wanders until it finds a target, notes the value (reward) of each position on the grid and makes it immediate choices based onthe value of the four possible actions (up, down, left, right) from the current position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bFgqTlaigRm"
   },
   "source": [
    "We start by setting up our patchy environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZhCnyhYiag5"
   },
   "outputs": [],
   "source": [
    "# Noise and delete\n",
    "p_scent = 0.1\n",
    "noise_sigma = 2.0\n",
    "\n",
    "# Shared\n",
    "num_experiments = 100\n",
    "num_steps = 500\n",
    "seed_value = 5838\n",
    "num_targets = 20 # with 80 agents are more competitive!\n",
    "\n",
    "# Environment parameters\n",
    "n_patches = 8 #         # number of patches\n",
    "n_per_patch = 20 #      # number targets per patch\n",
    "radius = 1 #            # radius of each patch\n",
    "target_boundary = (10, 10)\n",
    "\n",
    "\n",
    "# ! (leave alone)\n",
    "detection_radius = 1\n",
    "cog_mult = 1\n",
    "max_steps = 1\n",
    "min_length = 1\n",
    "\n",
    "# Targets\n",
    "prng = np.random.RandomState(seed_value)\n",
    "targets, patch_locs = uniform_patch_targets(n_patches, target_boundary, radius, n_per_patch, prng=prng)\n",
    "values = constant_values(targets, 1)\n",
    "\n",
    "# Scents\n",
    "scents = []\n",
    "for _ in range(len(targets)):\n",
    "    coord, scent = create_grid_scent_patches(\n",
    "        target_boundary, p=1.0, amplitude=1, sigma=2)\n",
    "    scents.append(scent)\n",
    "\n",
    "# Env\n",
    "env = ScentGrid(mode=None)\n",
    "env.seed(seed_value)\n",
    "env.add_scents(targets, values, coord, scents, noise_sigma=noise_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WT_tG8dimsA"
   },
   "source": [
    "### Getting to know you, RL\n",
    "\n",
    "For this demo we will set up two agents:\n",
    "\n",
    "- Rando: random walker just like we have been using before.\n",
    "- RL: An agent that uses reinforcement learning (Q-learning) to track a target.\n",
    "\n",
    "We are going to give each of our agents 99 tries at the _same_ environment. We want to see how repeated exposure to the same environment will improve performance in our agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wxvuPGKlizdg"
   },
   "outputs": [],
   "source": [
    "# RL\n",
    "possible_actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
    "critic = CriticGrid(default_value=0.5)\n",
    "actor = SoftmaxActor(num_actions=4, actions=possible_actions, beta=20)\n",
    "rl = ActorCriticGrid(actor, critic, lr=0.1, gamma=0.1)\n",
    "\n",
    "# Rando\n",
    "\n",
    "diff = DiffusionGrid()\n",
    "diff.seed(seed_value)\n",
    "\n",
    "# !\n",
    "rl_exp = experiment(\n",
    "    f\"RL\",\n",
    "    rl,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")\n",
    "rand_exp = experiment(\n",
    "    f\"rand\",\n",
    "    diff,\n",
    "    env,\n",
    "    num_steps=num_steps,\n",
    "    num_experiments=num_experiments,\n",
    "    dump=False,\n",
    "    split_state=True,\n",
    "    seed=seed_value\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrDnoiRijKJp"
   },
   "source": [
    "To start, let's just look at one example of the movement of our random agent, for comparison with the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hz4xbgVSi4K7"
   },
   "outputs": [],
   "source": [
    "plot_boundary = (20, 20)\n",
    "\n",
    "# -\n",
    "num_experiment = 99\n",
    "ax = None\n",
    "ax = plot_position2d(\n",
    "    select_exp(rand_exp, num_experiment),\n",
    "    boundary=plot_boundary,\n",
    "    label=f\"Rando\",\n",
    "    color=\"grey\",\n",
    "    alpha=0.6,\n",
    "    ax=ax,\n",
    ")\n",
    "ax = plot_targets2d(\n",
    "    env,\n",
    "    boundary=plot_boundary,\n",
    "    color=\"black\",\n",
    "    alpha=1,\n",
    "    label=\"Targets\",\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xf-InKwjfz3"
   },
   "source": [
    "So our little friend is doing just fine in this random case.\n",
    "\n",
    "Now let's look at our our RL agent progresses over runs of the experiment. We'll look at at three time points (N=0, early; N=50, middle; N=99, late)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJtu8jiOjd1Z"
   },
   "outputs": [],
   "source": [
    "plot_boundary = (20, 20)\n",
    "\n",
    "# -\n",
    "num_experiment = 0\n",
    "ax = None\n",
    "ax = plot_position2d(\n",
    "    select_exp(rl_exp, num_experiment),\n",
    "    boundary=plot_boundary,\n",
    "    label=f\"N={num_experiment}\",\n",
    "    color=\"orange\",\n",
    "    alpha=0.3,\n",
    "    ax=ax,\n",
    ")\n",
    "num_experiment = 50\n",
    "ax = plot_position2d(\n",
    "    select_exp(rl_exp, num_experiment),\n",
    "    boundary=plot_boundary,\n",
    "    label=f\"N={num_experiment}\",\n",
    "    color=\"orange\",\n",
    "    alpha=0.5,\n",
    "    ax=ax,\n",
    ")\n",
    "num_experiment = 99\n",
    "ax = plot_position2d(\n",
    "    select_exp(rl_exp, num_experiment),\n",
    "    boundary=plot_boundary,\n",
    "    label=f\"N={num_experiment}\",\n",
    "    color=\"orange\",\n",
    "    alpha=1,\n",
    "    ax=ax,\n",
    ")\n",
    "ax = plot_targets2d(\n",
    "    env,\n",
    "    boundary=plot_boundary,\n",
    "    color=\"black\",\n",
    "    alpha=1,\n",
    "    label=\"Targets\",\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HM7xhQ8jypX"
   },
   "source": [
    "Notice how much more structured the search becomes areound the patches with more practice. Not great, but getting there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xX_mB0AxkFEY"
   },
   "source": [
    "### Reward value, in time\n",
    "\n",
    "Now let's look at the value ($Q$-value in this case) of the optimal value (i.e., $max(Q(a))$) across time for each of these three stages of learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQTKLG-TjwGO"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 3))\n",
    "plt.plot(rl_exp[0][\"agent_reward_value\"], label=\"N=0\", color=\"orange\", alpha=0.2)\n",
    "plt.plot(rl_exp[50][\"agent_reward_value\"], label=\"N=50\", color=\"orange\", alpha=0.5)\n",
    "plt.plot(rl_exp[99][\"agent_reward_value\"], label=\"N=99\", color=\"orange\", alpha=1)\n",
    "plt.ylabel(\"Value $V(x)$\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uXktvGskcZ6"
   },
   "source": [
    "\n",
    "#### Question 3.1\n",
    "\n",
    "What do you see in this behavior of the RL agent over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Syfs_FmkV3L"
   },
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLXNdwqqkmvK"
   },
   "source": [
    "### Looking across simulations\n",
    "\n",
    "Now let's plot our metrics and see how the two agents did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-iJZTwwkp0m"
   },
   "source": [
    "### Death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "348RIUdTknRD"
   },
   "outputs": [],
   "source": [
    "# Results\n",
    "results = [rand_exp, rl_exp]\n",
    "names = [\"Rando\", \"RL\"]\n",
    "colors = [\"grey\", \"orange\"]\n",
    "\n",
    "# Score by eff\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    scores.append(num_death(res))\n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(4, 3))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.6)\n",
    "plt.ylabel(\"Deaths\")\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtvNlqGDkuOF"
   },
   "source": [
    "### Total reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oh8dxrAPkrkU"
   },
   "outputs": [],
   "source": [
    "# Results\n",
    "results = [rand_exp, rl_exp]\n",
    "names = [\"Rando\", \"RL\"]\n",
    "colors = [\"grey\", \"orange\"]\n",
    "\n",
    "# Score by eff\n",
    "scores = []\n",
    "for name, res, color in zip(names, results, colors):\n",
    "    r = total_reward(res)\n",
    "    scores.append(r)\n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.6)\n",
    "plt.ylabel(\"Total reward\")\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "\n",
    "# Dists\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    plt.hist(s, label=name, color=c, alpha=0.5, bins=np.linspace(0, np.max(scores), 50))\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQ3FxT-Yk4SJ"
   },
   "source": [
    "#### Question 3.2\n",
    "\n",
    "How does the performance of the RL agent compare, across all performance metrics, to the random agent? Is this a fair comparison? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0ZE2TCIkyI8"
   },
   "outputs": [],
   "source": [
    "# Write your answer here as a comment. Explain yourself."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}