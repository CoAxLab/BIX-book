

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Lab 9 - Reward seeking &#8212; Biologically Intelligent eXploration (BIX)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/lab9-reward_seeking';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Lab 10 - Exploring vs. exploiting" href="lab10-exploration_vs_exploitation.html" />
    <link rel="prev" title="Lab 8 - Patch foraging" href="lab8-patch_foraging.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/levy-flight.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/levy-flight.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Biologically Intelligent eXploration (BIX)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="intro-python.html">Introduction to python</a></li>

<li class="toctree-l1"><a class="reference internal" href="intro-github.html">Introduction to Github and Colab</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Labs</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lab1-randomsearch.html">Lab 1 - Random exploration</a></li>
<li class="toctree-l1"><a class="reference internal" href="lab2-chemotaxis.html">Lab 2 - Simple Chemotaxis</a></li>
<li class="toctree-l1"><a class="reference internal" href="lab3-signal_detection_theory.html">Lab 3 - Signal detection theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="lab4-evidence_accumulation.html">Lab 4 - Evidence Accumulation</a></li>
<li class="toctree-l1"><a class="reference internal" href="lab5-cbgt.html">Lab 5 - CBGT pathways</a></li>
<li class="toctree-l1"><a class="reference internal" href="lab6-information.html">Lab 6 - Information theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="lab7-infotaxis.html">Lab 7 - Infotaxis</a></li>
<li class="toctree-l1"><a class="reference internal" href="lab8-patch_foraging.html">Lab 8 - Patch foraging</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Lab 9 - Reward seeking</a></li>
<li class="toctree-l1"><a class="reference internal" href="lab10-exploration_vs_exploitation.html">Lab 10 - Exploring vs. exploiting</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/CoAxLab/BIX-book/main?urlpath=tree/book/notebooks/lab9-reward_seeking.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/CoAxLab/BIX-book/blob/main/book/notebooks/lab9-reward_seeking.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/notebooks/lab9-reward_seeking.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Lab 9 - Reward seeking</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#background">Background</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-bandit-task">The bandit task</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#action-value-learning">Action-value learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-exploration-strategies">Basic exploration strategies</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-setup">Section - Setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-1-the-bandit-task">Section 1 - The bandit task</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-1-1">Question 1.1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-1-2">Question 1.2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-1-3">Question 1.3</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-1-4">Question 1.4</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-2-investigating-the-epsilon-greedy-algorithm">Section 2 - Investigating the epsilon-greedy algorithm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-2-1">Question 2.1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-2-2">Question 2.2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-2-3">Question 2.3</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-2-4">Question 2.4</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-2-6">Question 2.6</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-2-7">Question 2.7</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-2-8">Question 2.8</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-3-reward-driven-search">Section 3 - Reward driven search</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-3-1">Question 3.1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-3-2">Question 3.2</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="lab-9-reward-seeking">
<h1>Lab 9 - Reward seeking<a class="headerlink" href="#lab-9-reward-seeking" title="Permalink to this heading">#</a></h1>
<p>This lab has 3 main components designed to give you an interactive understanding of core reinforcement learning concepts and the <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy reinforcement learning algorithm.</p>
<p>Sections:
0. Background on essential reinforcement concepts that we will be engagning with hands-on.</p>
<ol class="arabic simple">
<li><p>Investigating random and <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy algorithms in a simple bandit task.</p></li>
<li><p>Seeing how this sort of policy works in our foraging search.</p></li>
</ol>
<section id="background">
<h2>Background<a class="headerlink" href="#background" title="Permalink to this heading">#</a></h2>
<section id="the-bandit-task">
<h3>The bandit task<a class="headerlink" href="#the-bandit-task" title="Permalink to this heading">#</a></h3>
<p>In this assignment we study exploration in the very abstract <span class="math notranslate nohighlight">\(k\)</span>-armed bandit task.</p>
<ul class="simple">
<li><p>In this there are <span class="math notranslate nohighlight">\(k\)</span> actions to take.</p></li>
<li><p>Each returns a reward <span class="math notranslate nohighlight">\(R\)</span>, with some probability <span class="math notranslate nohighlight">\(p\)</span>.</p></li>
<li><p>The reward value is either a 1 or a 0.</p></li>
<li><p>This means the expected value of each arm is simply probability. Nice and simple right?</p></li>
</ul>
</section>
<section id="action-value-learning">
<h3>Action-value learning<a class="headerlink" href="#action-value-learning" title="Permalink to this heading">#</a></h3>
<p>Our agents are really learning, at last. Reinforcement Learning (RL), to be precise.</p>
<p>The reward value <span class="math notranslate nohighlight">\(Q\)</span> update rule for all agents (below) and arm is the same:</p>
<p><span class="math notranslate nohighlight">\( Q \leftarrow Q + \alpha * (R - Q) \)</span> [1]</p>
<p>Where the learning rate on the reward prediction error (<span class="math notranslate nohighlight">\(R-Q\)</span>) is denoted as <span class="math notranslate nohighlight">\(\alpha\)</span>, so that the equation above looks nice. If you are not familiar with the idea of a learning rate, it is what it sounds like. A parameter that controls how much each value update matters. This is, over time, the rate at which learning happens.</p>
<p><span class="math notranslate nohighlight">\(Q\)</span> is trying to approximate the average reward value of each arm.</p>
<ul class="simple">
<li><p>This kind of difference <span class="math notranslate nohighlight">\((R - Q)\)</span>, the reward prediction error (RPE), in Eq [1] is the most typical error signal used for learnin gin RL.</p></li>
<li><p>If you’re not sure what it means, consider in your head, what would happen to the value update if <span class="math notranslate nohighlight">\(Q\)</span> was bigger than the reward <span class="math notranslate nohighlight">\(R\)</span> (and overestimate), or if it was smaller.</p></li>
</ul>
<p>Once you have noodled that a bit, as needed, consider how making <span class="math notranslate nohighlight">\(\alpha\)</span> bigger or smaller might make <span class="math notranslate nohighlight">\(Q\)</span> learning faster, or slower, or more or less volatile. (Learning speed and volatility <em>often</em> go together; an annoying matched set.)</p>
<p><em>Note</em>: We are not going to really play with <span class="math notranslate nohighlight">\(\alpha\)</span> here. Just giving you some intuition.</p>
</section>
<section id="basic-exploration-strategies">
<h3>Basic exploration strategies<a class="headerlink" href="#basic-exploration-strategies" title="Permalink to this heading">#</a></h3>
<p>Our exploration strategies are a random one, a sequential one, or <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy (aka ‘e’-greedy).</p>
<p>The <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy method is not the best known solution to trading off exploration with exploitation. Then again, it is widely used to this day. It’s a place to start.</p>
<p>Our metric is <em>total reward</em>. Maximizing that is the goal of all RL, afterall.</p>
</section>
</section>
<section id="section-setup">
<h2>Section - Setup<a class="headerlink" href="#section-setup" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install explorationlib?</span>
<span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span>git+https://github.com/coaxlab/explorationlib
<span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span>git+https://github.com/MattChanTK/gym-maze.git
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import misc</span>
<span class="kn">import</span> <span class="nn">shutil</span>
<span class="kn">import</span> <span class="nn">glob</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Exploration lib</span>
<span class="kn">import</span> <span class="nn">explorationlib</span>

<span class="c1"># Agents</span>
<span class="kn">from</span> <span class="nn">explorationlib.agent</span> <span class="kn">import</span> <span class="n">BanditActorCritic</span>
<span class="kn">from</span> <span class="nn">explorationlib.agent</span> <span class="kn">import</span> <span class="n">Critic</span>
<span class="kn">from</span> <span class="nn">explorationlib.agent</span> <span class="kn">import</span> <span class="n">EpsilonActor</span>
<span class="kn">from</span> <span class="nn">explorationlib.agent</span> <span class="kn">import</span> <span class="n">RandomActor</span>
<span class="kn">from</span> <span class="nn">explorationlib.agent</span> <span class="kn">import</span> <span class="n">SequentialActor</span>
<span class="kn">from</span> <span class="nn">explorationlib.agent</span> <span class="kn">import</span> <span class="n">BoundedRandomActor</span>
<span class="kn">from</span> <span class="nn">explorationlib.agent</span> <span class="kn">import</span> <span class="n">BoundedSequentialActor</span>
<span class="kn">from</span> <span class="nn">explorationlib.agent</span> <span class="kn">import</span> <span class="n">WSLSGrid</span>
<span class="kn">from</span> <span class="nn">explorationlib.agent</span> <span class="kn">import</span> <span class="n">CriticGrid</span>
<span class="kn">from</span> <span class="nn">explorationlib.agent</span> <span class="kn">import</span> <span class="n">SoftmaxActor</span>
<span class="kn">from</span> <span class="nn">explorationlib.agent</span> <span class="kn">import</span> <span class="n">ActorCriticGrid</span>
<span class="kn">from</span> <span class="nn">explorationlib.agent</span> <span class="kn">import</span> <span class="n">DiffusionGrid</span>

<span class="c1"># Exp</span>
<span class="kn">from</span> <span class="nn">explorationlib.run</span> <span class="kn">import</span> <span class="n">experiment</span>
<span class="kn">from</span> <span class="nn">explorationlib.score</span> <span class="kn">import</span> <span class="n">total_reward</span>
<span class="kn">from</span> <span class="nn">explorationlib.score</span> <span class="kn">import</span> <span class="n">action_entropy</span>
<span class="kn">from</span> <span class="nn">explorationlib.util</span> <span class="kn">import</span> <span class="n">select_exp</span>
<span class="kn">from</span> <span class="nn">explorationlib.util</span> <span class="kn">import</span> <span class="n">load</span>
<span class="kn">from</span> <span class="nn">explorationlib.util</span> <span class="kn">import</span> <span class="n">save</span>

<span class="c1"># Vis</span>
<span class="kn">from</span> <span class="nn">explorationlib.plot</span> <span class="kn">import</span> <span class="n">plot_bandit</span>
<span class="kn">from</span> <span class="nn">explorationlib.plot</span> <span class="kn">import</span> <span class="n">plot_bandit_actions</span>
<span class="kn">from</span> <span class="nn">explorationlib.plot</span> <span class="kn">import</span> <span class="n">plot_bandit_critic</span>
<span class="kn">from</span> <span class="nn">explorationlib.plot</span> <span class="kn">import</span> <span class="n">plot_bandit_hist</span>
<span class="kn">from</span> <span class="nn">explorationlib.plot</span> <span class="kn">import</span> <span class="n">plot_position2d</span>
<span class="kn">from</span> <span class="nn">explorationlib.plot</span> <span class="kn">import</span> <span class="n">plot_length_hist</span>
<span class="kn">from</span> <span class="nn">explorationlib.plot</span> <span class="kn">import</span> <span class="n">plot_length</span>
<span class="kn">from</span> <span class="nn">explorationlib.plot</span> <span class="kn">import</span> <span class="n">plot_targets2d</span>
<span class="kn">from</span> <span class="nn">explorationlib.plot</span> <span class="kn">import</span> <span class="n">plot_scent_grid</span>
<span class="kn">from</span> <span class="nn">explorationlib.plot</span> <span class="kn">import</span> <span class="n">plot_targets2d</span>

<span class="c1"># Score</span>
<span class="kn">from</span> <span class="nn">explorationlib.score</span> <span class="kn">import</span> <span class="n">total_reward</span>
<span class="kn">from</span> <span class="nn">explorationlib.score</span> <span class="kn">import</span> <span class="n">num_death</span>
<span class="kn">from</span> <span class="nn">explorationlib.score</span> <span class="kn">import</span> <span class="n">on_off_patch_time</span>

<span class="c1"># Env</span>
<span class="kn">from</span> <span class="nn">explorationlib.local_gym</span> <span class="kn">import</span> <span class="n">BanditUniform4</span>
<span class="kn">from</span> <span class="nn">explorationlib.local_gym</span> <span class="kn">import</span> <span class="n">ScentGrid</span>
<span class="kn">from</span> <span class="nn">explorationlib.local_gym</span> <span class="kn">import</span> <span class="n">create_grid_scent</span>
<span class="kn">from</span> <span class="nn">explorationlib.local_gym</span> <span class="kn">import</span> <span class="n">create_grid_scent_patches</span>
<span class="kn">from</span> <span class="nn">explorationlib.local_gym</span> <span class="kn">import</span> <span class="n">uniform_targets</span>
<span class="kn">from</span> <span class="nn">explorationlib.local_gym</span> <span class="kn">import</span> <span class="n">uniform_patch_targets</span>
<span class="kn">from</span> <span class="nn">explorationlib.local_gym</span> <span class="kn">import</span> <span class="n">constant_values</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Pretty plots</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format=&#39;retina&#39;
<span class="o">%</span><span class="k">config</span> IPCompleter.greedy=True
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;axes.facecolor&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;white&quot;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;figure.facecolor&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;white&quot;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;font.size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;16&quot;</span>

<span class="c1"># Dev</span>
<span class="o">%</span><span class="k">load_ext</span> autoreload
<span class="o">%</span><span class="k">autoreload</span> 2
</pre></div>
</div>
</div>
</div>
</section>
<section id="section-1-the-bandit-task">
<h2>Section 1 - The bandit task<a class="headerlink" href="#section-1-the-bandit-task" title="Permalink to this heading">#</a></h2>
<p>In this section we’ll study three explorers getting to know one bandit, with four arms.</p>
<p><strong>Creating a bandit task</strong></p>
<p>Let’s make a 4-armed bandit and then plot its values. (Expected value is the term used in the literature, so we use it here).</p>
<p><em>Note</em>: The random seed is fixed. If you change the see and run the cell below, some of the reward probabilities will change. The probability of the best arm, the optimal value arm is fixed however. It is set to 0.35, and located at arm 2. Try it! Rerun the cell below with different seeds, a few times, to get a sense of how the non-optimal arms can vary. When you are done, return to the orginal seed value and re-run.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Shared env params</span>
<span class="n">num_experiments</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">5030</span> <span class="c1"># originally 5030</span>

<span class="c1"># Create env</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">BanditUniform4</span><span class="p">(</span><span class="n">p_min</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">p_max</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">p_best</span><span class="o">=</span><span class="mf">0.35</span><span class="p">)</span>
<span class="n">env</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="c1"># Plot env</span>
<span class="n">plot_bandit</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
<section id="question-1-1">
<h3>Question 1.1<a class="headerlink" href="#question-1-1" title="Permalink to this heading">#</a></h3>
<p>Given the reward probabilities (expected values) in this bandit task, how “easy” or “difficult” do you think this task would be to learn from a simple update rule like we showed above? Why?</p>
<p><strong>Answer:</strong></p>
<p><em>(insert response here)</em></p>
<p><strong>Our three agents</strong></p>
<p>A word about the code. Our agents this week work in what gets called an ActorCritic desgin. This breaks reinforcement learning algorithms into two parts: the Actor does action selection, and the Critic estimates the value of each action.</p>
<p>Now in normal reinforcement learning, aka not pure exploration, the <em>Actor</em> uses the <span class="math notranslate nohighlight">\(Q\)</span> value estimates from the <em>Critic</em> to, in part, make its decisions. Be it explore or exploit. This is indeed the case for how the <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy agent, <em>EpsilonActor</em>, works.</p>
<p>…But…</p>
<p>The other two agents–<em>SequentialActor</em> and <em>RandomActor</em>–don’t explore with value. The are both <em>max entropy</em> action systems, who don’t care about reward value or learning <em>at all</em>. The <em>ActorCritic</em> style is reused because it was easy to implement in <em>explorationlib</em>. Don’t be misled.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Creating the three agents</span>

<span class="n">ran</span> <span class="o">=</span> <span class="n">BanditActorCritic</span><span class="p">(</span>
    <span class="n">RandomActor</span><span class="p">(</span><span class="n">num_actions</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">num_arms</span><span class="p">),</span>
    <span class="n">Critic</span><span class="p">(</span><span class="n">num_inputs</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">num_arms</span><span class="p">,</span> <span class="n">default_value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">seq</span> <span class="o">=</span> <span class="n">BanditActorCritic</span><span class="p">(</span>
    <span class="n">SequentialActor</span><span class="p">(</span><span class="n">num_actions</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">num_arms</span><span class="p">),</span>
    <span class="n">Critic</span><span class="p">(</span><span class="n">num_inputs</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">num_arms</span><span class="p">,</span> <span class="n">default_value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">epy</span> <span class="o">=</span> <span class="n">BanditActorCritic</span><span class="p">(</span>
    <span class="n">EpsilonActor</span><span class="p">(</span><span class="n">num_actions</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">num_arms</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span>
    <span class="n">Critic</span><span class="p">(</span><span class="n">num_inputs</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">num_arms</span><span class="p">,</span> <span class="n">default_value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># -</span>
<span class="n">agents</span> <span class="o">=</span> <span class="p">[</span><span class="n">ran</span><span class="p">,</span> <span class="n">seq</span><span class="p">,</span> <span class="n">epy</span><span class="p">]</span>
<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;random&quot;</span><span class="p">,</span> <span class="s2">&quot;sequential&quot;</span><span class="p">,</span> <span class="s2">&quot;ep-greedy&quot;</span><span class="p">]</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="s2">&quot;green&quot;</span><span class="p">,</span> <span class="s2">&quot;purple&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s run out our three agents on the <em>env</em> (the bandit task we built), and make some plots to visualize what each agent is doing.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_steps</span> <span class="o">=</span> <span class="mi">12</span>  <span class="c1"># Number of choices each agent gets to make, (around 3 per arm)</span>

<span class="c1"># !</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">agent</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">agents</span><span class="p">):</span>
    <span class="n">log</span> <span class="o">=</span> <span class="n">experiment</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="n">agent</span><span class="p">,</span>
        <span class="n">env</span><span class="p">,</span>
        <span class="n">num_steps</span><span class="o">=</span><span class="n">num_steps</span><span class="p">,</span>
        <span class="n">num_experiments</span><span class="o">=</span><span class="n">num_experiments</span><span class="p">,</span>
        <span class="n">dump</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">split_state</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">log</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Plot action choices</strong>
with time (aka steps).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_experiment</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">res</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
    <span class="n">plot_bandit_actions</span><span class="p">(</span>
        <span class="n">select_exp</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">num_experiment</span><span class="p">),</span>
        <span class="n">num_arms</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">s</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span>
        <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">)</span>
        <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="question-1-2">
<h3>Question 1.2<a class="headerlink" href="#question-1-2" title="Permalink to this heading">#</a></h3>
<p>Describe the coice behavior of each agent type.</p>
<p><strong>Answer:</strong></p>
<p><em>(insert response here)</em></p>
</section>
<hr class="docutils" />
<section id="question-1-3">
<h3>Question 1.3<a class="headerlink" href="#question-1-3" title="Permalink to this heading">#</a></h3>
<p>Re-run the simulations above a few times. What range of choice patterns does epsilon-greedy agent exhibit? What do you think leads to these patterns?</p>
<p><strong>Answer:</strong></p>
<p><em>(insert response here)</em></p>
</section>
<hr class="docutils" />
<section id="question-1-4">
<h3>Question 1.4<a class="headerlink" href="#question-1-4" title="Permalink to this heading">#</a></h3>
<p>Now change the ϵ value for the ϵ-greedy agent to <span class="math notranslate nohighlight">\(0.01\)</span> and re-run the simulations a few times. How does the behavior of this agent change? What does this tell you about the utility of greediness in this particular form of the bandit task?</p>
<p><strong>Answer:</strong></p>
<p><em>(insert response here)</em></p>
<p><strong>Histograms of action probability (aka arm choice).</strong></p>
<p><em>Fun fact</em>: The flatter these plots are, the closer they are to what is called <em>maximum entropy</em> exploration behavior.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_experiment</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">ax</span> <span class="o">=</span> <span class="kc">None</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">res</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
    <span class="n">ent</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">action_entropy</span><span class="p">(</span><span class="n">res</span><span class="p">)),</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plot_bandit_hist</span><span class="p">(</span>
        <span class="n">select_exp</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">num_experiment</span><span class="p">),</span>
        <span class="n">bins</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)),</span>
        <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="c1"># (ent {ent})&quot;,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span>
        <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span>
        <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="section-2-investigating-the-epsilon-greedy-algorithm">
<h2>Section 2 - Investigating the epsilon-greedy algorithm<a class="headerlink" href="#section-2-investigating-the-epsilon-greedy-algorithm" title="Permalink to this heading">#</a></h2>
<p><strong>Meet our dilemma</strong>
I’ve been learning Q-value estimates.</p>
<ul class="simple">
<li><p>Should I explore (keep sampling the options to get more data points to update Q-value estimates)?</p></li>
<li><p>Should I exploit (choose the action whose Q value estimate is currently the greatest)?</p></li>
</ul>
<p><em>A simple strategy:</em></p>
<p>….I’ll flip a weight coin,</p>
<p>…who’s weight has a name. It’s <span class="math notranslate nohighlight">\(\epsilon\)</span>!</p>
<p>The smaller <span class="math notranslate nohighlight">\(\epsilon\)</span> is, the less likely the coin flip comes up “EXPLORE’’. The more likely it comes up on the “EXPLOIT” side. If one chooses the exploit side, one is being greedy, right? The bigger <span class="math notranslate nohighlight">\(\epsilon\)</span> the more likely the coin will say “EXPLORE ‘’. Etc.</p>
<p>Let’s play with <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy, on our base case bandit task.</p>
<hr class="docutils" />
<section id="question-2-1">
<h3>Question 2.1<a class="headerlink" href="#question-2-1" title="Permalink to this heading">#</a></h3>
<p>We will run three differnt epsilon-greedy agents, each with a different epsilon value (0.05, 0.5, and 0.95). What do you expect each agent’s stepwise behavior to “look like”? Why?</p>
<p><strong>Answer:</strong></p>
<p><em>(insert response here)</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_steps</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">100</span>
<span class="n">epsilons</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">]</span>

<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">epsilon</span><span class="p">)</span> <span class="k">for</span> <span class="n">epsilon</span> <span class="ow">in</span> <span class="n">epsilons</span><span class="p">]</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;mediumpurple&quot;</span><span class="p">,</span> <span class="s2">&quot;mediumorchid&quot;</span><span class="p">,</span> <span class="s2">&quot;mediumvioletred&quot;</span><span class="p">]</span>

<span class="c1"># !</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">epsilons</span><span class="p">)):</span>
    <span class="n">agent</span> <span class="o">=</span> <span class="n">BanditActorCritic</span><span class="p">(</span>
        <span class="n">EpsilonActor</span><span class="p">(</span><span class="n">num_actions</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">num_arms</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">),</span>
        <span class="n">Critic</span><span class="p">(</span><span class="n">num_inputs</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">num_arms</span><span class="p">,</span> <span class="n">default_value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">log</span> <span class="o">=</span> <span class="n">experiment</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;ep_</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="n">agent</span><span class="p">,</span>
        <span class="n">env</span><span class="p">,</span>
        <span class="n">num_steps</span><span class="o">=</span><span class="n">num_steps</span><span class="p">,</span>
        <span class="n">num_experiments</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">dump</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">split_state</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">log</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Example behavior visualizations below. Change <em>num experiment</em> to see more examples (0, 99).</p>
<p><em>Note</em>: in every experiment we run in this lab, the optimal value arm is <em>always</em> arm 2.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_experiment</span> <span class="o">=</span> <span class="mi">3</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">res</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
    <span class="n">plot_bandit_actions</span><span class="p">(</span>
        <span class="n">select_exp</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">num_experiment</span><span class="p">),</span>
        <span class="n">max_steps</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
        <span class="n">s</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;epsilon = &quot;</span> <span class="o">+</span> <span class="n">name</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span>
        <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="question-2-2">
<h3>Question 2.2<a class="headerlink" href="#question-2-2" title="Permalink to this heading">#</a></h3>
<p>Did the behavior match what you expected? If not, describe the actual behavior and explain what you think is behind it.</p>
<p><strong>Answer:</strong></p>
<p><em>(insert response here)</em></p>
</section>
<hr class="docutils" />
<section id="question-2-3">
<h3>Question 2.3<a class="headerlink" href="#question-2-3" title="Permalink to this heading">#</a></h3>
<p>Re-run the stepwise choice behavior visualization at different <code class="docutils literal notranslate"><span class="pre">num_experiment</span></code> settings to view what happens in different experiments. Find a case when epsilon=0.05 agent didn’t select the best arm (Arm 2) very much. How do you think this happened?</p>
<p><strong>Answer:</strong></p>
<p><em>(insert response here)</em></p>
</section>
<hr class="docutils" />
<section id="question-2-4">
<h3>Question 2.4<a class="headerlink" href="#question-2-4" title="Permalink to this heading">#</a></h3>
<p>We will visualize the average total reward for each agent next. Which agent do you think will collect the most reward and which one the least? Why?</p>
<p><strong>Answer:</strong></p>
<p><em>(insert response here)</em></p>
<p><strong>Visualize total reward</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Score</span>
<span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">res</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">total_reward</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>

<span class="c1"># Tabulate</span>
<span class="n">m</span><span class="p">,</span> <span class="n">sd</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="k">for</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
    <span class="n">m</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
    <span class="n">sd</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>

<span class="c1"># Plot means</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><span class="n">sd</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Total reward&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Epsilon&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="question-2-6">
<h3>Question 2.6<a class="headerlink" href="#question-2-6" title="Permalink to this heading">#</a></h3>
<p>Were your predictions in questions 2.4 and 2.5 correct? If not, describe the actual results and explain what you think led to them.</p>
<p><strong>Answer:</strong></p>
<p><em>(insert response here)</em></p>
<p><strong>Visualizing a histogram of average choice reward across experiments</strong></p>
<p>To get average choice reward for each experiment, we divide the total reward for the experiment by the number of steps (choices) taken to collect that total reward.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">/</span><span class="n">num_steps</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Average choice reward&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Count&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="question-2-7">
<h3>Question 2.7<a class="headerlink" href="#question-2-7" title="Permalink to this heading">#</a></h3>
<p>If instead of 400 steps in each experiment we let each agent run to near infinity, what would the above histogram look like and why?</p>
<p><strong>Answer:</strong></p>
<p><em>(insert response here)</em></p>
</section>
<hr class="docutils" />
<section id="question-2-8">
<h3>Question 2.8<a class="headerlink" href="#question-2-8" title="Permalink to this heading">#</a></h3>
<p>Based on what you’ve seen here today, if you were to follow an ϵ-greedy policy for choices in your own life, what value of ϵ would you choose? Why?</p>
<p><strong>Answer:</strong></p>
<p><em>(insert response here)</em></p>
</section>
</section>
<section id="section-3-reward-driven-search">
<h2>Section 3 - Reward driven search<a class="headerlink" href="#section-3-reward-driven-search" title="Permalink to this heading">#</a></h2>
<p>Now we are going to create an agent in our foraging environment that implements reward driven search using the exact same approaches we have described above. We shall call this process <em>rewardtaxis</em>.</p>
<p>This is a Q-learning agent that uses softmax exploration policiy, as opposed to an <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy policy. It does not use teh scent, but wanders until it finds a target, notes the value (reward) of each position on the grid and makes it immediate choices based onthe value of the four possible actions (up, down, left, right) from the current position.</p>
<p>We start by setting up our patchy environment.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Noise and delete</span>
<span class="n">p_scent</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">noise_sigma</span> <span class="o">=</span> <span class="mf">2.0</span>

<span class="c1"># Shared</span>
<span class="n">num_experiments</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">seed_value</span> <span class="o">=</span> <span class="mi">5838</span>
<span class="n">num_targets</span> <span class="o">=</span> <span class="mi">20</span> <span class="c1"># with 80 agents are more competitive!</span>

<span class="c1"># Environment parameters</span>
<span class="n">n_patches</span> <span class="o">=</span> <span class="mi">8</span> <span class="c1">#         # number of patches</span>
<span class="n">n_per_patch</span> <span class="o">=</span> <span class="mi">20</span> <span class="c1">#      # number targets per patch</span>
<span class="n">radius</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1">#            # radius of each patch</span>
<span class="n">target_boundary</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>


<span class="c1"># ! (leave alone)</span>
<span class="n">detection_radius</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">cog_mult</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">max_steps</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">min_length</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># Targets</span>
<span class="n">prng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">seed_value</span><span class="p">)</span>
<span class="n">targets</span><span class="p">,</span> <span class="n">patch_locs</span> <span class="o">=</span> <span class="n">uniform_patch_targets</span><span class="p">(</span><span class="n">n_patches</span><span class="p">,</span> <span class="n">target_boundary</span><span class="p">,</span> <span class="n">radius</span><span class="p">,</span> <span class="n">n_per_patch</span><span class="p">,</span> <span class="n">prng</span><span class="o">=</span><span class="n">prng</span><span class="p">)</span>
<span class="n">values</span> <span class="o">=</span> <span class="n">constant_values</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Scents</span>
<span class="n">scents</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">targets</span><span class="p">)):</span>
    <span class="n">coord</span><span class="p">,</span> <span class="n">scent</span> <span class="o">=</span> <span class="n">create_grid_scent_patches</span><span class="p">(</span>
        <span class="n">target_boundary</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">amplitude</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">scents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scent</span><span class="p">)</span>

<span class="c1"># Env</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">ScentGrid</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">env</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed_value</span><span class="p">)</span>
<span class="n">env</span><span class="o">.</span><span class="n">add_scents</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">coord</span><span class="p">,</span> <span class="n">scents</span><span class="p">,</span> <span class="n">noise_sigma</span><span class="o">=</span><span class="n">noise_sigma</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Getting to know you, RL</strong></p>
<p>For this demo we will set up two agents:</p>
<ul class="simple">
<li><p>Rando: random walker just like we have been using before.</p></li>
<li><p>RL: An agent that uses reinforcement learning (Q-learning) to track a target.</p></li>
</ul>
<p>We are going to give each of our agents 99 tries at the <em>same</em> environment. We want to see how repeated exposure to the same environment will improve performance in our agents.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># RL</span>
<span class="n">possible_actions</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]</span>
<span class="n">critic</span> <span class="o">=</span> <span class="n">CriticGrid</span><span class="p">(</span><span class="n">default_value</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">actor</span> <span class="o">=</span> <span class="n">SoftmaxActor</span><span class="p">(</span><span class="n">num_actions</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">actions</span><span class="o">=</span><span class="n">possible_actions</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">rl</span> <span class="o">=</span> <span class="n">ActorCriticGrid</span><span class="p">(</span><span class="n">actor</span><span class="p">,</span> <span class="n">critic</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Rando</span>

<span class="n">diff</span> <span class="o">=</span> <span class="n">DiffusionGrid</span><span class="p">()</span>
<span class="n">diff</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed_value</span><span class="p">)</span>

<span class="c1"># !</span>
<span class="n">rl_exp</span> <span class="o">=</span> <span class="n">experiment</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;RL&quot;</span><span class="p">,</span>
    <span class="n">rl</span><span class="p">,</span>
    <span class="n">env</span><span class="p">,</span>
    <span class="n">num_steps</span><span class="o">=</span><span class="n">num_steps</span><span class="p">,</span>
    <span class="n">num_experiments</span><span class="o">=</span><span class="n">num_experiments</span><span class="p">,</span>
    <span class="n">dump</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">split_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="n">seed_value</span>
<span class="p">)</span>
<span class="n">rand_exp</span> <span class="o">=</span> <span class="n">experiment</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;rand&quot;</span><span class="p">,</span>
    <span class="n">diff</span><span class="p">,</span>
    <span class="n">env</span><span class="p">,</span>
    <span class="n">num_steps</span><span class="o">=</span><span class="n">num_steps</span><span class="p">,</span>
    <span class="n">num_experiments</span><span class="o">=</span><span class="n">num_experiments</span><span class="p">,</span>
    <span class="n">dump</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">split_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="n">seed_value</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To start, let’s just look at one example of the movement of our random agent, for comparison with the cells below</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_boundary</span> <span class="o">=</span> <span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>

<span class="c1"># -</span>
<span class="n">num_experiment</span> <span class="o">=</span> <span class="mi">99</span>
<span class="n">ax</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plot_position2d</span><span class="p">(</span>
    <span class="n">select_exp</span><span class="p">(</span><span class="n">rand_exp</span><span class="p">,</span> <span class="n">num_experiment</span><span class="p">),</span>
    <span class="n">boundary</span><span class="o">=</span><span class="n">plot_boundary</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Rando&quot;</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;grey&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plot_targets2d</span><span class="p">(</span>
    <span class="n">env</span><span class="p">,</span>
    <span class="n">boundary</span><span class="o">=</span><span class="n">plot_boundary</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Targets&quot;</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>So our little friend is doing just fine in this random case.</p>
<p>Now let’s look at our our RL agent progresses over runs of the experiment. We’ll look at at three time points (N=0, early; N=50, middle; N=99, late)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_boundary</span> <span class="o">=</span> <span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>

<span class="c1"># -</span>
<span class="n">num_experiment</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">ax</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plot_position2d</span><span class="p">(</span>
    <span class="n">select_exp</span><span class="p">(</span><span class="n">rl_exp</span><span class="p">,</span> <span class="n">num_experiment</span><span class="p">),</span>
    <span class="n">boundary</span><span class="o">=</span><span class="n">plot_boundary</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;N=</span><span class="si">{</span><span class="n">num_experiment</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;orange&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">num_experiment</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plot_position2d</span><span class="p">(</span>
    <span class="n">select_exp</span><span class="p">(</span><span class="n">rl_exp</span><span class="p">,</span> <span class="n">num_experiment</span><span class="p">),</span>
    <span class="n">boundary</span><span class="o">=</span><span class="n">plot_boundary</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;N=</span><span class="si">{</span><span class="n">num_experiment</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;orange&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">num_experiment</span> <span class="o">=</span> <span class="mi">99</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plot_position2d</span><span class="p">(</span>
    <span class="n">select_exp</span><span class="p">(</span><span class="n">rl_exp</span><span class="p">,</span> <span class="n">num_experiment</span><span class="p">),</span>
    <span class="n">boundary</span><span class="o">=</span><span class="n">plot_boundary</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;N=</span><span class="si">{</span><span class="n">num_experiment</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;orange&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plot_targets2d</span><span class="p">(</span>
    <span class="n">env</span><span class="p">,</span>
    <span class="n">boundary</span><span class="o">=</span><span class="n">plot_boundary</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Targets&quot;</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Notice how much more structured the search becomes areound the patches with more practice. Not great, but getting there.</p>
<p><strong>Reward value, in time</strong></p>
<p>Now let’s look at the value (<span class="math notranslate nohighlight">\(Q\)</span>-value in this case) of the optimal value (i.e., <span class="math notranslate nohighlight">\(max(Q(a))\)</span>) across time for each of these three stages of learning.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rl_exp</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;agent_reward_value&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;N=0&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;orange&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rl_exp</span><span class="p">[</span><span class="mi">50</span><span class="p">][</span><span class="s2">&quot;agent_reward_value&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;N=50&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;orange&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rl_exp</span><span class="p">[</span><span class="mi">99</span><span class="p">][</span><span class="s2">&quot;agent_reward_value&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;N=99&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;orange&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Value $V(x)$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Step&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;center left&#39;</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
<section id="question-3-1">
<h3>Question 3.1<a class="headerlink" href="#question-3-1" title="Permalink to this heading">#</a></h3>
<p>What do you see in this behavior of the RL agent over time?</p>
<p><strong>Answer:</strong></p>
<p><em>(insert response here)</em></p>
<p><strong>Looking across simulations</strong></p>
<p>Now let’s plot our metrics and see how the two agents did.</p>
<p><strong>Death</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Results</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="n">rand_exp</span><span class="p">,</span> <span class="n">rl_exp</span><span class="p">]</span>
<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Rando&quot;</span><span class="p">,</span> <span class="s2">&quot;RL&quot;</span><span class="p">]</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;grey&quot;</span><span class="p">,</span> <span class="s2">&quot;orange&quot;</span><span class="p">]</span>

<span class="c1"># Score by eff</span>
<span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">res</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">num_death</span><span class="p">(</span><span class="n">res</span><span class="p">))</span>

<span class="c1"># Tabulate</span>
<span class="n">m</span><span class="p">,</span> <span class="n">sd</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="k">for</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
    <span class="n">m</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
    <span class="n">sd</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>

<span class="c1"># Plot means</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><span class="n">sd</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Deaths&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Total reward</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Results</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="n">rand_exp</span><span class="p">,</span> <span class="n">rl_exp</span><span class="p">]</span>
<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Rando&quot;</span><span class="p">,</span> <span class="s2">&quot;RL&quot;</span><span class="p">]</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;grey&quot;</span><span class="p">,</span> <span class="s2">&quot;orange&quot;</span><span class="p">]</span>

<span class="c1"># Score by eff</span>
<span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">res</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">total_reward</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>

<span class="c1"># Tabulate</span>
<span class="n">m</span><span class="p">,</span> <span class="n">sd</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="k">for</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
    <span class="n">m</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
    <span class="n">sd</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>

<span class="c1"># Plot means</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><span class="n">sd</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Total reward&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>

<span class="c1"># Dists</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="mi">50</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Score&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="question-3-2">
<h3>Question 3.2<a class="headerlink" href="#question-3-2" title="Permalink to this heading">#</a></h3>
<p>How does the performance of the RL agent compare, across all performance metrics, to the random agent? Is this a fair comparison? Why or why not?</p>
<p><strong>Answer:</strong></p>
<p><em>(insert response here)</em></p>
<hr class="docutils" />
<p><strong>IMPORTANT</strong> Did you collaborate with anyone on this assignment, or use LLMs like ChatGPT? If so, list their names here.</p>
<blockquote>
<div><p><em>Write Name(s) here</em></p>
</div></blockquote>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="lab8-patch_foraging.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Lab 8 - Patch foraging</p>
      </div>
    </a>
    <a class="right-next"
       href="lab10-exploration_vs_exploitation.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Lab 10 - Exploring vs. exploiting</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#background">Background</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-bandit-task">The bandit task</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#action-value-learning">Action-value learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-exploration-strategies">Basic exploration strategies</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-setup">Section - Setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-1-the-bandit-task">Section 1 - The bandit task</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-1-1">Question 1.1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-1-2">Question 1.2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-1-3">Question 1.3</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-1-4">Question 1.4</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-2-investigating-the-epsilon-greedy-algorithm">Section 2 - Investigating the epsilon-greedy algorithm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-2-1">Question 2.1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-2-2">Question 2.2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-2-3">Question 2.3</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-2-4">Question 2.4</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-2-6">Question 2.6</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-2-7">Question 2.7</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-2-8">Question 2.8</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-3-reward-driven-search">Section 3 - Reward driven search</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-3-1">Question 3.1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#question-3-2">Question 3.2</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Timothy Verstynen, Erik Peterson, Matthew Clapp, Jack Burgess
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>